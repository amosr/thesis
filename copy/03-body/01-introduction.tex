\chapter{Introduction}
\label{chapter:introduction}

All the data in the world won't tell you anything if you don't look at it.
To learn from the data, we must ask it a question.
We must query the data.
Once we have learnt one thing, we will find ourselves wanting to know more; knowledge is addictive.
To learn many things from the data, we must not just ask it one question; we must interview it.
We must query the data many times.

When our data is too large to fit in memory, it must be read from disk or network.
Reading data from disk takes longer than reading from memory, and reading over the network takes longer still.
For a simple query, we will spend more time reading the data than performing the computation part of the query.
We would like to limit the amount of time spent reading.
We cannot avoid reading the data altogether, but we can limit the number of times we read the same data.
If we perform multiple queries over the same input data separately, each query needs to read the same data, and we end up reading the same data multiple times.
We want to perform these multiple queries together, sharing the data among the queries, and reading the data only once.

Our data can exceed the size of memory; the working set of our query program cannot.
If our query program requires more memory than the computer has, it will not be able to compute the answer we desire.
When we write a query, we need to be sure that it will not run out of memory when we run it today.
With datasets that grow every day, we also need to be sure that the query will not run out of memory tomorrow or the day after.
Unless we want to add new memory to the computer every day, the memory usage should be as close as possible to constant, regardless of the data size.

Streaming lets us transform large datasets in constant memory.
Stream transformers consume data element by element, processing the elements in sequential order.
Stream transformers can only store a limited number of elements at a time.
These restrictions mean that we cannot, for example, sort all the input data, or read elements in random access.
The upside of these restrictions is that if we can write our queries as streaming programs, we can be confident that they will not run out of memory.

% If our input data is stored persistently on disk somewhere, we can perform multiple passes over the input if necessary.
% For example, if we wish to read elements in random access, we may be able to emulate this 
% This means that many streaming
% We would like to keep the number of iterations over the input data to a minimum.

\section{Many queries}

Assume we have many queries, and we have written each one as a separate streaming program.
We wish to execute all the queries together, so we only pay the overhead of reading from disk once.

Suppose we have some historical share prices for a particular company.
For each day, we have the share's open price at the start of the day, and the close price at the end of the day.

\begin{lstlisting}
data DailyPrice = DailyPrice
  { date  :: Date
  , open  :: Double
  , close :: Double }
\end{lstlisting}

We wish to query this data.
For simplicity, let us assume that we have the data in a lazy list, \lstinline/[DailyPrice]/.
We can treat lazy lists as a kind of stream where the next value is computed on demand, as long as we are careful not to reuse the same list multiple times.
When a list is only used once, its values can be computed, used, and collected by the garbage collector.
If we reuse the same list, once its values are computed by one consumer they must be retained in memory until the other consumer uses them.
List combinators provide a convenient way to write our example queries.
% Later we shall see the that reading data as lazy lists can introduce memory problems when the list is used multiple times, but list combinators provide a convenient way to write queries for now.

If we assume that the list of days are distinct, we can count how many days we have data for by folding over the list and incrementing the count for every element.

\begin{lstlisting}
countDays :: [DailyPrice] -> Int
countDays prices = foldl (\count _ -> count + 1) 0 prices
\end{lstlisting}

We can write a slightly more interesting query if we only count the `profit' days where the close price was above the open price.

\begin{lstlisting}
countProfitDays :: [DailyPrice] -> Int
countProfitDays prices =
 let profits = filter (\p -> open p < close p) prices
 in  foldl (\count _ -> count + 1) 0 profits
\end{lstlisting}

These queries are not very useful on their own, but if we put them both together they will tell us a bit more about the company.
If we wanted to compute both of these with regular lists, we could just construct a tuple and call both functions.
This approach will loop over the list twice as well as retaining the whole list in memory, which is unsuitable for streaming.

Instead, we must \emph{fuse} the queries together, combining the two into a single function that computes both.
We put the work of both queries inside a single loop over the list, which stores as loop state the count of all days, and the count of `profit' days.
To count all days we increment the count for each element, essentially applying the original fold for each element.
To count the `profit' days we must perform both the filter and the fold inside the loop, so we only apply the fold if the filter predicate is true.

\begin{lstlisting}
countBoth :: [DailyPrice] -> (Int, Int)
countBoth prices = loop 0 0 prices
 where
  loop sCountDays sCountProfitDays (p:prices')
   = let sCountDays' = sCountDays + 1
         sCountProfitDays'
          | open p < close p
          = sCountProfitDays + 1
          | otherwise
          = sCountProfitDays
     in loop sCountDays' sCountProfitDays' prices
  loop sCountDays sCountProfitDays []
   = (sCountDays, sCountProfitDays)
\end{lstlisting}

This fused query is more efficient than running both queries separately as it only requires a single traversal, but we have lost the composability benefits of using high-level combinators.
We have had to intertwine all the logic of the folds and the filter into a single place.
This program is not too hard to write by hand, but it is more complex than each original query individually.

A more interesting query is to compute the sum of the \emph{gap}.
The gap is the difference between one day's close price and the next day's open price, which tells us how perception of the stock changed overnight while the market was closed.
We can compute this by pairing each day with the day after, finding the difference between each pair, then summing the differences.

\begin{lstlisting}
sumGap :: [DailyPrice] -> Double
sumGap prices =
 let pairs       = zip prices (tail prices)
     differences = map (\(p,pTail) -> close p - open pTail) pairs
 in  sum differences
\end{lstlisting}

We wish to perform all three queries together, but because this query is a bit more complex than the previous ones, let us start by looking at how we can write this query on its own as a single loop.

\begin{lstlisting}
sumGap' :: [DailyPrice] -> Double
sumGap' prices = loop 0 prices
 where
  loop sSum []
   = sSum
  loop sSum (p:prices')
   = let sSum'
          | (pTail:_) <- prices'
          = sSum + (close p - open pTail)
          | otherwise
          = sSum
     in loop sSum' prices'
\end{lstlisting}

% \begin{lstlisting}
% sumGap' :: [DailyPrice] -> Double
% sumGap' prices = loop 0 prices
%  where
%   loop sSum (p:pTail:prices')
%    = let diff  = close p - open pTail
%          sSum' = sSum + diff
%      in loop sSum' (pTail:prices')
%   loop sSum [p]
%    = sSum
%   loop sSum []
%    = sSum
% \end{lstlisting}

The main difference between this loop and the one for @countBoth@ is that this loop looks at the two elements at the start of the list in order to compare two adjacent days.

To perform all three queries, @sumGap@, @countDays@, and @countProfitDays@.

\begin{lstlisting}
sumGapCount :: [DailyPrice] -> (Int, Int, Double)
sumGapCount prices = loop 0 0 0 prices
 where
  loop sCountDays sCountProfitDays sSum []
   = (sCountDays, sCountProfitDays, sSum)
  loop sCountDays sCountProfitDays sSum (p:prices')
   = let sCountDays' = sCountDays + 1
         sCountProfitDays'
          | open p < close p
          = sCountProfitDays + 1
          | otherwise
          = sCountProfitDays
         sSum'
          | (pTail:_) <- prices'
          = sSum + (close p - open pTail)
          | otherwise
          = sSum
     in loop sCountDays' sCountProfitDays' sSum' prices
\end{lstlisting}



% \begin{lstlisting}
% sumGap' :: [DailyPrice] -> Double
% sumGap' prices = init
%  where
%   init []          = 0
%   init (p:prices') = loop 0 p prices'
% 
%   loop sSum pTail []
%    = sSum
%   loop sSum pTail (p:prices')  
%    = let diff   = p - pTail
%          pTail' = p
%          sSum'  = sSum + diff
%      in  loop sSum' pTail' prices'
% \end{lstlisting}


Combining two queries is hard enough; when we want to combine three or more queries, we soon reach the limit of what can be achieved with intuition alone.
As our queries become more complex and more numerous, the compound becomes more complex.
We need a system to help us: a rigorous approach to transforming multiple queries into a single compound query.
This transform is called fusion.

\section{Push streams}

The first part of this thesis looks at fusing queries that use push streams.
Push streams are those where the producer controls the flow of computation, and the flow of values.
The producer pushes values into the consumer, and the consumer must accept with these values.
Producers determine the order in which values are given to consumers.

In push streams, one producer can push to many consumers.
Each query is a consumer, and the input file is the producer.
By pushing the data to all the queries, we are able to perform any number of queries at the same time.
Push streams allow us to take any queries operating over the same input and fuse them together.
We can rely on fusion.

Push streams have limited expressivity, however.
Push streams cannot support most use-cases with multiple input streams, such as appending streams, pairing them together, or merging sorted streams so that the result remains sorted.
When we have multiple input files, the query has a particular order in which it needs to read elements from each file.
In push streams, the producer imposes its order upon the consumer.
The query has no choice in the matter.
This limited expressivity is the price we pay for guaranteed fusion.

Trying to read from multiple input files as push streams is a bit like putting the files together and randomly interspersing the lines.
You can read a line, but you don't know which file it came from until after you read it.

For datasets with a single input file, push streams are a perfect solution.
Push streams can express any streaming computation with a single input file, while guaranteeing that all queries over the same input can be fused together.

\section{Pushing and pulling with Kahn process networks}

When we have multiple input files, we can no longer use push queries.
The consumer must choose which order to read from its input files.
The consumer must pull from its inputs, rather than the inputs pushing.

Pull streams, however, introduce their own problems.
Just as push streams only support one producer, pull streams only support one consumer.
In pull streams, the consumer is in control of the computation.
There can only be one controller and there can only be one consumer.
Having one consumer limits us to one query.
Pull streams are unsuitable for performing multiple queries at the same time.


Pull streams and push streams have one thing in common: there is a single point of control for the computation.
Where the control lies differs: in push it is in the producer; in pull it is in the consumer.
Our problem is that we are limited to a single point of control, while we have several queries.

Having a single point of control is very useful for a machine; after all this is how single-threaded processors execute programs.
But we wish to write better programs.
We wish to help programmers write better programs.
One of the ways to do that is by abstracting over the machine itself, and abstracting over the accident of sequential shared state imperative programs.

A single point of control is not necessary or ideal for streaming programs.
We can write programs with multiple points of control: concurrent programs.
Concurrent programs with shared state are absurdly difficult to write, and even harder to get right.
A program's state space is quadratic in the number of points of control.
This state space means for a programmer to reason about correctness, a programmer must reason about a quadratic number of cases.
If we wish to reduce the burden of streaming and writing compound queries by hand, we cannot increase the burden of reasoning.

Determinism is the key ingredient to make concurrent programming tractable.
Requiring concurrent programs to be deterministic tames the state space explosion.
We no longer have to reason about a quadratic number of states, because we know that whichever order the programs execute in, we will get the right result.

We want a concurrent, deterministic streaming model.
Such a model is called a Kahn Process Network.
Kahn Process Networks are networks of concurrent processes, communicating via channels, with streams of elements flowing along channels.
Determinism is achieved by a few restrictions: all communication between processes is through channels; all reading from channels is blocking; and all channels are owned by a single process.
Kahn Process Networks turn non-deterministic concurrency into a deterministic computation.

% There is no state shared between processes, which means the only way processes can communicate is through channels.
% 
% A process cannot peek to see if a channel has a value before deciding whether to read it.
% Once a process is reading from a channel, it must wait until a value is available.
% Peeking would allow a process to observe the non-determinism in scheduling.
% 
% There is no race between two processes to see which can read from a channel first.
% Kahn Process Networks are just swell.

Kahn Process Networks are a good computational method, but not the best execution strategy.
Sending values across channels is expensive: we must lock the channel, copy the value onto the queue, and unlock it.
By the time the consumer receives the value, it is unlikely to be in cache, and definitely no longer in a register.
To ameliorate the concurrency overhead, implementations tend to chunk together values into arrays before sending them.
This can only reduce the overhead; it can never remove it entirely.
Instead, we wish to compile away the overhead.
We take the processes in a Kahn Process Network, and convert it into a single process that does the job of all the processes together, without any communication.

% In general, not all queries can be compounded together.
% 
% This compounding operation is called horizontal fusion.
% Horizontal in this case refers to how data flow graphs are arranged; nodes operating on the same input data, which can be performed in parallel or sequentially, are typically horizontally adjacent.
% Vertical fusion refers to nodes operating in a pipeline, where each node feeds values to the node below.
% 
% Stream fusion is a rich area of research which has for the most part focussed on vertical fusion.
% 
% The reason for compounding queries is to reduce execution time.
% It is quite important, then, that our compound query does not take longer to execute than the original queries, combined.
% In fact, we would like our compound query to execute in almost as little time as the slowest query on its own.
% What we would like is not always achievable, but it can be helpful to have something to strive for.



