\chapter{Introduction}
\label{introduction}
To learn interesting things from large datasets, we generally want to perform lots of queries.
When each query is simple and our data is big, we might spend more time reading the data than we spend computing the answer.
In this case, we would like to amortise the cost of reading the data by performing multiple queries at the same time.

When querying datasets that do not fit in memory or disk, it can be hard to ensure that our query program's internal state will fit in memory.
One way to transform large datasets in constant memory is to write the query as a \emph{streaming program} (\cref{taxonomy}), by composing stream transformers together.
Composing stream transformers naively can add some performance overhead, which is then usually removed by \emph{fusing} multiple transformers into a single transformer.

This thesis describes low-overhead streaming models for executing multiple queries at a time.
We focus on two streaming models: push streams (\cref{taxonomy/push}), and Kahn process networks (\cref{taxonomy/kpn}, \cref{chapter:process:processes}).

Push streams can execute multiple queries at a time, but these queries can be unwieldy to write as they must be constructed ``back-to-front''.
In \cref{part:icicle} we introduce a query language called Icicle, which allows programmers to write and reason about queries using a more familiar array-based semantics, while retaining the execution strategy of push streams.
The type system of Icicle guarantees that well-typed query programs have the same semantics whether they are executed as array programs or as stream programs, and that all queries over the same input can be executed together in a single pass.
% \TODO{talk about duplicate work in push streams as well, rather than focussing on fairly trivial syntax changes}
% Icicle is a streaming query language, which means that queries can only perform a single pass over the input data.
% The Icicle compiler translates queries written in Icicle to push streams, and fuses multiple queries together to execute multiple queries at a time.
Icicle has been running in production at Ambiata, a machine-learning company, for over two years.
For one particular client, we receive thirty gigabtyes of uncompressed new data every day, over which Icicle computes more than four thousand queries in a single pass.
The input data is kept indefinitely and, once compressed, currently requires five terabytes of storage.
When creating a new machine-learning model, Icicle can provide training data by computing queries over the entire historical data.
% Icicle is also used to provide training data for machine learning models, in which case the history of all input data must be kept.
% The history of all input data is kept for training machine learning models, and takes five terabytes

However, push streams, and by extension Icicle, generally do not support computations with multiple inputs except for two cases: first, non-deterministically merging two streams; and second, in some circumstances appending streams --- as we shall see in \cref{taxonomy/push}.
As an alternative to push streams, Kahn process networks~\citep{kahn1976coroutines} support both multiple inputs and multiple queries, but require dynamic scheduling and inter-process communication, both of which can introduce significant runtime overhead.
In \cref{s:Fusion} we describe \emph{process fusion}, a method for taking multiple processes in a Kahn process network and fusing them together into a single process.
The fused process communicates through local variables rather than costly communication channels.
This fusion method generalises previous work on stream fusion (\cref{related/stream-fusion}) and demonstrates the connection between fusion and the synchronised product operator (\cref{related/synchronised-product}), which is generally used in the context of verifying properties of concurrent processes and model checking, rather than as an optimisation.

% When the input data is small enough to fit in memory, we can write each query as an \emph{array program}.
% Array programs can read from input arrays multiple times, and can produce intermediate and output arrays, which can also be read.
% We can execute array programs by splitting them into multiple \emph{passes}.
% Each pass is executed as a separate streaming program, where the input streams read from arrays and the output streams write to arrays.
% Streaming operations have a local view of the input data, and can only see one element at a time.
% are restricted to a single pass over the input, but if the input data is persistent, for example a file on disk or an array in memory, we can perform multiple passes by executing each pass separately as its own streaming operation.
% We call such streams \emph{rewindable}, as they can be ``rewinded'' back to the start after reading.

When the input data is small enough to fit in memory, we can write each query as an \emph{array program}.
Array programs can read from input arrays multiple times, and can produce intermediate and output arrays which can also be read multiple times.
% Array programs can read from input arrays multiple times, and can produce intermediate and output arrays.
We can execute array programs by splitting them into \emph{passes}, and executing each pass as a separate streaming program, where the input and output streams are backed by manifest arrays.
For array programs that require multiple passes, there are generally many different ways to divide the work among the passes.
We perform \emph{clustering} on the program to determine how many passes to perform, and how to schedule each operation among the different passes.
The choice of clustering affects runtime performance.
To minimise the time spent reading and re-reading the data, we would like to use a clustering which requires the minimum number of passes and intermediate arrays.
Finding this clustering is NP-hard \cite{darte1999complexity}.

In \cref{clustering}, we find the minimal clustering by generating an integer linear program, and using an external solver to find the solution.
The clustering algorithm of \citet{megiddo1998optimal} computes the clustering for an imperative loop nest, rather than a set of array combinators.
Individual loops in the loop nest are assigned to clusters, and all loops in the same cluster are fused together.
In \citet{megiddo1998optimal}, as with many imperative loop fusion systems, loops can only be fused together if their loop bounds are identical.
In such systems, a loop that filters an array cannot be fused with a loop that consumes the filtered array, as they have different loop bounds.
With process fusion, we \emph{can} fuse a filter with its consumer, so a clustering algorithm for imperative loop nests would introduce more passes than necessary.
Our clustering algorithm extends \citet{megiddo1998optimal} to cluster array combinators, and to support size-changing operations such as filter.
% As such, clustering algorithms designed for imperative loop fusion ignore the opportunities for fusing filters together, and may assign more clusters than necessary.
% By working with high-level combinators rather than loop nests, we extend this clustering algorithm to recognise that filters can be fused with their consumer or their producer.

\section{Contributions}

This thesis makes the following contributions:

\begin{description}
\item[Modal types to ensure efficiency and correctness:]
if, due to time or cost constraints, we can only afford one pass over the input data, we need some guarantee that all our queries can be executed together.
The streaming query language Icicle uses modal types to ensure all queries over the same input can be executed together in a single pass, as well as ensuring that the stream query has the same semantics as if it were operating over arrays.
Icicle is described in \cref{part:icicle}.

% Pure compute:
% 1024 machines, 7 minutes per machine (5tb input)
% Compute + scatter + *:
% 100x r3.4xlarge instances to run 1024 query tasks, taking 4.5 hours each

\item[Process fusion:]
a method for fusing stream combinators; the first that supports all three of multiple inputs, multiple queries, and user-defined combinators.
In this streaming model, each combinator in each query is implemented as a sequential process, and together, the combinators of all queries form a concurrent process network.
Processes are then fused together using an extension of synchronised product.
The fusion algorithm is described in \cref{chapter:process:processes}.

\item[Formal proof of correctness of fusion:]
a proof of correctness for process fusion, mechanised in the proof assistant Coq.
The proof states that when two processes are fused together, the fused process computes the same result as the original processes.
The proof is described in \cref{s:Proofs}.

\item[Clustering for array-backed streams:]
a clustering algorithm for array combinators, which supports size-changing operations such as filter.
This algorithm encodes the clustering constraints of a set of array combinators as an integer linear program to be solved externally.
The clustering algorithm is described in \cref{clustering}.

\end{description}

Before delving into these contributions, the next chapter introduces some background on different streaming models, as well as more concretely motivating why we want to execute multiple streaming queries concurrently.

% \subsection{Extension of clustering algorithm to filters}
%   \REF{clustering}:
% When we require multiple passes over the input, there may be multiple ways to divide the work.
% The decision of \emph{clustering} --- how to group the combinators together --- becomes important.
% Choosing a clustering that minimises a particular objective is NP-hard \cite{darte1999complexity}.
% \cite{megiddo1998optimal}'s clustering algorithm converts imperative loop nests to integer linear programs to find the optimal clustering according to some objective function.
% Here, as with many imperative loop fusion systems, loops can only be fused together if their loop bounds are identical.
% In such systems a loop that filters an array cannot be fused with a loop that consumes the filtered array, as they have different loop bounds.
% Many combinator-based fusion systems do support fusing a filter with its consumer.
% As such, clustering algorithms designed for imperative loop fusion ignore the opportunities for fusing filters together, and may assign more clusters than necessary.
% By working with high-level combinators rather than loop nests, we extend this clustering algorithm to recognise that filters can be fused with their consumer or their producer.


% All the data in the world won't tell you anything if you don't look at it.
% To learn from the data, we must ask it a question by querying the data.
% Once we have learnt one thing, we will find ourselves wanting to know more; knowledge is addictive.
% To learn many things from the data, we must not just ask it one question; we must interview it.
% We must query the data many times.
% 
% When our data is too large to fit in memory, we must read it from disk or over the network.
% Reading data from disk takes longer than reading from memory, and reading over the network takes longer still.
% When we query data over disk or network, we can spend more time waiting for data than performing computations with the data.
% For a computer, time spent waiting is time wasted, so we would like to limit the amount of time spent waiting for data.
% We cannot avoid reading the data altogether, but we may be able to limit the number of times we read the same data.
% If we perform multiple queries over the same input data separately, each query needs to read the same data, and we end up reading the same data multiple times.
% Ideally, we would perform multiple queries together, sharing the data among the queries, and reading the data only once.
% 
% Our data can exceed the size of memory; the working set of our query program cannot.
% If our query program requires more memory than the computer has, it will not be able to compute the answer we desire.
% When we write a query, we need to be sure that it will not run out of memory.
% With datasets that grow every day, we also need to be sure that the query will not run out of memory when we execute it tomorrow or the day after.
% Unless we want to keep adding new memory to the computer as the dataset grows, the memory usage should be as close as possible to constant, regardless of the data size.
% 
% One way to transform large datasets in constant memory is to write the query as a \emph{streaming program}\REFTODO{streaming}.
% We can write streaming programs by composing stream transformers together.
% Stream transformers consume data element by element, processing the elements in sequential order, and can only store a limited number of elements at a time.
% Because of these restrictions a stream transformer cannot, for example, sort all the input data, or read elements in random access.
% The upside of these restrictions is that if we can write our queries as streaming programs, we can be confident that they will not run out of memory when we execute them.
% 
% Composing stream transformers together adds some performance overhead, which is usually removed by \emph{fusing} multiple transformers into a single transformer \REFTODO{fusion}.
% Unfortunately, stream systems that support executing multiple queries at the same time do not perform fusion, or are limited in expressivity \REFTODO{background}.
% This thesis aims to address these limitations by proposing a streaming system based on concurrent process networks and fusion method.

% \TODO{Need to be explicit about the difference between `streaming' as a concept and and particular streaming models.} Different stream representations can represent different programs; we can think of a streaming model as the set of supported stream transformers, as well as the rules about how we can connect transformers together.

% If our input data is stored persistently on disk somewhere, we can perform multiple passes over the input if necessary.
% For example, if we wish to read elements in random access, we may be able to emulate this 
% This means that many streaming
% We would like to keep the number of iterations over the input data to a minimum.

