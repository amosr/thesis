\chapter{Introduction}
\label{chapter:introduction}

All the data in the world won't tell you anything if you don't look at it.
To learn from the data, we must ask it a question.
We must query the data.
Once we have learnt one thing, we will find ourselves wanting to know more.
Knowledge is addictive.
To learn many things from the data, we must not just ask it one question; we must interview it.
We must query the data many times.

When our data is too large to fit in memory, it must be read from disk or network.
Reading data from disk takes longer than reading from memory, and reading over the network takes longer still.
For a simple query, we will spend more time reading the data than performing the query part of the computation.
We would like to limit the amount of time spent reading.
We cannot avoid reading the data altogether, but we can limit the number of times we read the same data.
If we perform multiple queries separately, each query needs to read the same data, and we end up reading the same data multiple times.
We want to perform these multiple queries together, sharing the data among the queries, and reading the data only once.

Our data can exceed the size of memory; the working set of our query program cannot.
If our query program requires more memory than the computer has, it will crash before returning the answer.
When we write a query, we need to be sure that it will not run out of memory when we run it today.
With datasets that grow every day, we also need to be sure that the query will not run out of memory tomorrow or the day after.
The memory usage should be constant regardless of the amount of data.

Streaming lets us transform large datasets in constant memory.
Stream transformers consume data element by element, processing the elements in sequential order.
Stream transformers can only store a limited number of elements at a time, and cannot sort or reorder the elements.
These heavy restrictions are worthwhile.
We can write our queries as streaming programs, confident that they will not run out of memory.

\section{Many queries}

We have many queries, and we have written each one as a separate streaming program.
We wish to execute all the queries together, so we only pay once the overhead of reading from disk.

If we have two queries to execute at the same time, we can write a new query that combines them together.
If both queries are simple, the compound query will remain simple.
If one query is simple and the other is complex, the compound query will be a bit more complex.
If both queries are complex, the compound will be tremendously complex.

Combining two queries is hard enough; when we want to combine three or more queries, we soon reach the limit of what can be achieved with intuition alone.
As our queries become more complex and more numerous, the compound becomes exponentially more complex.
We need a system to help us: a rigorous approach to transforming multiple queries into a single compound query.
This transform is called fusion.

\section{Push streams}

The first part of this thesis looks at fusing queries that use push streams.
Push streams are those where the producer controls the flow of computation, and the flow of values.
The producer pushes values into the consumer, and the consumer must accept with these values.
Producers determine the order in which values are given to consumers.

In push streams, one producer can push to many consumers.
Each query is a consumer, and the input file is the producer.
By pushing the data to all the queries, we are able to perform any number of queries at the same time.
Push streams allow us to take any queries operating over the same input and fuse them together.
We can rely on fusion.

Push streams have limited expressivity, however.
Push streams cannot support most use-cases with multiple input streams, such as appending streams, pairing them together, or merging sorted streams so that the result remains sorted.
When we have multiple input files, the query has a particular order in which it needs to read elements from each file.
In push streams, the producer imposes its order upon the consumer.
The query has no choice in the matter.
This limited expressivity is the price we pay for guaranteed fusion.

Trying to read from multiple input files as push streams is a bit like putting the files together and randomly interspersing the lines.
You can read a line, but you don't know which file it came from until after you read it.

For datasets with a single input file, push streams are a perfect solution.
Push streams can express any streaming computation with a single input file, while guaranteeing that all queries over the same input can be fused together.

\section{Pushing and pulling with Kahn process networks}

When we have multiple input files, we can no longer use push queries.
The consumer must choose which order to read from its input files.
The consumer must pull from its inputs, rather than the inputs pushing.

Pull streams, however, introduce their own problems.
Just as push streams only support one producer, pull streams only support one consumer.
In pull streams, the consumer is in control of the computation.
There can only be one controller and there can only be one consumer.
Having one consumer limits us to one query.
Pull streams are unsuitable for performing multiple queries at the same time.


Pull streams and push streams have one thing in common: there is a single point of control for the computation.
Where the control lies differs: in push it is in the producer; in pull it is in the consumer.
Our problem is that we are limited to a single point of control, while we have several queries.

Having a single point of control is very useful for a machine; after all this is how single-threaded processors execute programs.
But we wish to write better programs.
We wish to help programmers write better programs.
One of the ways to do that is by abstracting over the machine itself, and abstracting over the accident of sequential shared state imperative programs.

A single point of control is not necessary or ideal for streaming programs.
We can write programs with multiple points of control: concurrent programs.
Concurrent programs with shared state are absurdly difficult to write, and even harder to get right.
A program's state space is quadratic in the number of points of control.
This state space means for a programmer to reason about correctness, a programmer must reason about a quadratic number of cases.
If we wish to reduce the burden of streaming and writing compound queries by hand, we cannot increase the burden of reasoning.

Determinism is the key ingredient to make concurrent programming tractable.
Requiring concurrent programs to be deterministic tames the state space explosion.
We no longer have to reason about a quadratic number of states, because we know that whichever order the programs execute in, we will get the right result.

We want a concurrent, deterministic streaming model.
Such a model is called a Kahn Process Network.
Kahn Process Networks are networks of concurrent processes, communicating via channels, with streams of elements flowing along channels.
Determinism is achieved by a few restrictions: all communication between processes is through channels; all reading from channels is blocking; and all channels are owned by a single process.
Kahn Process Networks turn non-deterministic concurrency into a deterministic computation.

% There is no state shared between processes, which means the only way processes can communicate is through channels.
% 
% A process cannot peek to see if a channel has a value before deciding whether to read it.
% Once a process is reading from a channel, it must wait until a value is available.
% Peeking would allow a process to observe the non-determinism in scheduling.
% 
% There is no race between two processes to see which can read from a channel first.
% Kahn Process Networks are just swell.

Kahn Process Networks are a good computational method, but not the best execution strategy.
Sending values across channels is expensive: we must lock the channel, copy the value onto the queue, and unlock it.
By the time the consumer receives the value, it is unlikely to be in cache, and definitely no longer in a register.
To ameliorate the concurrency overhead, implementations tend to chunk together values into arrays before sending them.
This can only reduce the overhead; it can never remove it entirely.
Instead, we wish to compile away the overhead.
We take the processes in a Kahn Process Network, and convert it into a single process that does the job of all the processes together, without any communication.

% In general, not all queries can be compounded together.
% 
% This compounding operation is called horizontal fusion.
% Horizontal in this case refers to how data flow graphs are arranged; nodes operating on the same input data, which can be performed in parallel or sequentially, are typically horizontally adjacent.
% Vertical fusion refers to nodes operating in a pipeline, where each node feeds values to the node below.
% 
% Stream fusion is a rich area of research which has for the most part focussed on vertical fusion.
% 
% The reason for compounding queries is to reduce execution time.
% It is quite important, then, that our compound query does not take longer to execute than the original queries, combined.
% In fact, we would like our compound query to execute in almost as little time as the slowest query on its own.
% What we would like is not always achievable, but it can be helpful to have something to strive for.



