\chapter{Introduction}
\label{introduction}
To learn interesting things from large datasets, we generally want to perform lots of queries.
When our query is simple and our data is big, we might spend more time reading the data than we spend computing the answer.
In this case, we would like to amortise the cost of reading the data by performing multiple queries at the same time.

When querying datasets that do not fit in memory or disk, it can be hard to ensure that our query program's internal state will fit in memory.
One way to transform large datasets in constant memory is to write the query as a \emph{streaming program} (\cref{taxonomy}), which we can write by composing stream transformers together.
Composing stream transformers together can add some performance overhead, which is usually removed by \emph{fusing} together multiple transformers into a single transformer.

This thesis describes low-overhead streaming models for executing multiple queries at a time.
We focus on two streaming models: push streams (\cref{taxonomy/push}), and Kahn process networks (\cref{taxonomy/kpn}, \cref{chapter:process:processes}).

Push streams can execute multiple queries at a time, but these queries can be unwieldy to write as they must be constructed ``back-to-front''.
In \cref{part:icicle} we introduce a query language called Icicle, which allows programmers to write and reason about queries using a more familiar array-based semantics, while retaining the execution strategy of push streams.
The type system of Icicle guarantees that well-typed query programs have the same semantics whether they are executed as array programs or as stream programs, and that all queries over the same input can be executed together in a single pass.
% \TODO{talk about duplicate work in push streams as well, rather than focussing on fairly trivial syntax changes}
% Icicle is a streaming query language, which means that queries can only perform a single pass over the input data.
% The Icicle compiler translates queries written in Icicle to push streams, and fuses multiple queries together to execute multiple queries at a time.

However, push streams do not support computations with multiple inputs except for non-deterministically merging two streams, and in some circumstances appending streams, as we shall see in \cref{taxonomy/push}.
As an alternative to push streams, Kahn process networks support both multiple inputs and multiple queries, but require dynamic scheduling and inter-process communication, both of which introduce significant runtime overhead.
In \cref{s:Fusion} we introduce \emph{process fusion}, a method for taking multiple processes in a Kahn process network and fusing them together into a single process.
The fused process communicates through local variables rather than costly communication channels.
This fusion method generalises previous work on stream fusion (\cref{related/stream-fusion}) and demonstrates the connection between fusion and synchronised product of processes (\cref{related/synchronised-product}), which is generally used as a proof technique rather than an optimisation.

Streaming programs are restricted to a single pass over the input, but if the input data is persistent, for example a file on disk or an array in memory, we can perform multiple passes by executing each pass separately as its own streaming operation.
We call such streams \emph{rewindable}, as they can be ``rewinded'' back to the start after reading.
For programs requiring multiple passes over the input, there are generally many different ways to divide the work among the passes.
We perform \emph{clustering} on the program to determine how many passes to perform, and how to schedule each stream operation among the different passes.
The choice of clustering affects runtime performance.
To minimise the time spent reading and re-reading the data, we would like to use a clustering which requires the least number of passes.
Finding this clustering is NP-hard \cite{darte1999complexity}.

In \cref{clustering}, we find the clustering by generating an Integer Linear Program, which can be solved by an external solver.
The clustering algorithm of \citet{megiddo1998optimal} computes the clustering for an imperative loop nest, rather than a set of stream transformers.
Individual loops in the loop nest are assigned to clusters, and all loops in the same cluster are fused together.
Here, as with many imperative loop fusion systems, loops can only be fused together if their loop bounds are identical.
In such systems, a loop that filters an array cannot be fused with a loop that consumes the filtered array, as they have different loop bounds.
With process fusion, we \emph{can} fuse a filter with its consumer, so a clustering algorithm for imperative loop nests would introduce more passes than necessary.
Our clustering algorithm is an extension \citet{megiddo1998optimal} to cluster stream transformers, and to support size-changing operations such as filter.
% As such, clustering algorithms designed for imperative loop fusion ignore the opportunities for fusing filters together, and may assign more clusters than necessary.
% By working with high-level combinators rather than loop nests, we extend this clustering algorithm to recognise that filters can be fused with their consumer or their producer.
\TODO{refine clustering introduction}

\section{Contributions}

This thesis makes the following contributions:

\begin{description}
\item[Modal types to ensure efficiency and correctness:]
if, due to time or cost constraints, we can only afford one pass over the input data, we need some guarantee that all our queries can be executed together.
The streaming query language Icicle uses modal types to ensure all queries over the same input can be executed together in a single pass, as well as ensuring that the stream query has the same semantics as if it were operating over arrays.
At the time of writing in 2018, Icicle is currently running in production in the machine-learning pipeline at Ambiata.
Icicle is described in \cref{part:icicle}.
\TODO{get production stats}

\item[Process fusion:]
a method for fusing stream combinators; the first that supports all three of multiple inputs, multiple queries, and user-defined combinators.
In this streaming model, each combinator in each query is implemented as a sequential process, and together, the combinators of all queries form a concurrent process network.
Processes are then fused together using an extension of synchronised product.
The fusion algorithm is described in \cref{chapter:process:processes}.

\item[Formal proof of correctness of fusion:]
a proof of correctness for the above-mentioned fusion system, mechanised in the proof assistant Coq.
The proof states that when two processes are fused together, the fused process computes the same result as the original processes.
The proof is described in \cref{s:Proofs}.

\item[Clustering for rewindable streams:]
a clustering algorithm for streaming transformers, which supports size-changing operations such as filter.
This algorithm converts a set of streaming transformers to an Integer Linear Program to be solved externally.
The clustering algorithm is described in \cref{clustering}.
\TODO{refine}

\end{description}

Before delving into these contributions, the next chapter introduces some background on different streaming models, as well as more concretely motivating why we want to execute multiple streaming queries concurrently.

% \subsection{Extension of clustering algorithm to filters}
%   \REF{clustering}:
% When we require multiple passes over the input, there may be multiple ways to divide the work.
% The decision of \emph{clustering} --- how to group the combinators together --- becomes important.
% Choosing a clustering that minimises a particular objective is NP-hard \cite{darte1999complexity}.
% \cite{megiddo1998optimal}'s clustering algorithm converts imperative loop nests to integer linear programs to find the optimal clustering according to some objective function.
% Here, as with many imperative loop fusion systems, loops can only be fused together if their loop bounds are identical.
% In such systems a loop that filters an array cannot be fused with a loop that consumes the filtered array, as they have different loop bounds.
% Many combinator-based fusion systems do support fusing a filter with its consumer.
% As such, clustering algorithms designed for imperative loop fusion ignore the opportunities for fusing filters together, and may assign more clusters than necessary.
% By working with high-level combinators rather than loop nests, we extend this clustering algorithm to recognise that filters can be fused with their consumer or their producer.


% All the data in the world won't tell you anything if you don't look at it.
% To learn from the data, we must ask it a question by querying the data.
% Once we have learnt one thing, we will find ourselves wanting to know more; knowledge is addictive.
% To learn many things from the data, we must not just ask it one question; we must interview it.
% We must query the data many times.
% 
% When our data is too large to fit in memory, we must read it from disk or over the network.
% Reading data from disk takes longer than reading from memory, and reading over the network takes longer still.
% When we query data over disk or network, we can spend more time waiting for data than performing computations with the data.
% For a computer, time spent waiting is time wasted, so we would like to limit the amount of time spent waiting for data.
% We cannot avoid reading the data altogether, but we may be able to limit the number of times we read the same data.
% If we perform multiple queries over the same input data separately, each query needs to read the same data, and we end up reading the same data multiple times.
% Ideally, we would perform multiple queries together, sharing the data among the queries, and reading the data only once.
% 
% Our data can exceed the size of memory; the working set of our query program cannot.
% If our query program requires more memory than the computer has, it will not be able to compute the answer we desire.
% When we write a query, we need to be sure that it will not run out of memory.
% With datasets that grow every day, we also need to be sure that the query will not run out of memory when we execute it tomorrow or the day after.
% Unless we want to keep adding new memory to the computer as the dataset grows, the memory usage should be as close as possible to constant, regardless of the data size.
% 
% One way to transform large datasets in constant memory is to write the query as a \emph{streaming program}\REFTODO{streaming}.
% We can write streaming programs by composing stream transformers together.
% Stream transformers consume data element by element, processing the elements in sequential order, and can only store a limited number of elements at a time.
% Because of these restrictions a stream transformer cannot, for example, sort all the input data, or read elements in random access.
% The upside of these restrictions is that if we can write our queries as streaming programs, we can be confident that they will not run out of memory when we execute them.
% 
% Composing stream transformers together adds some performance overhead, which is usually removed by \emph{fusing} multiple transformers into a single transformer \REFTODO{fusion}.
% Unfortunately, stream systems that support executing multiple queries at the same time do not perform fusion, or are limited in expressivity \REFTODO{background}.
% This thesis aims to address these limitations by proposing a streaming system based on concurrent process networks and fusion method.

% \TODO{Need to be explicit about the difference between `streaming' as a concept and and particular streaming models.} Different stream representations can represent different programs; we can think of a streaming model as the set of supported stream transformers, as well as the rules about how we can connect transformers together.

% If our input data is stored persistently on disk somewhere, we can perform multiple passes over the input if necessary.
% For example, if we wish to read elements in random access, we may be able to emulate this 
% This means that many streaming
% We would like to keep the number of iterations over the input data to a minimum.

