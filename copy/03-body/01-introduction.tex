\chapter{Introduction}
\label{chapter:introduction}

All the data in the world won't tell you anything if you don't look at it.
To learn from the data, we must ask it a question.
We must query the data.
Once we have learnt one thing, we will find ourselves wanting to know more; knowledge is addictive.
To learn many things from the data, we must not just ask it one question; we must interview it.
We must query the data many times.

When our data is too large to fit in memory, we must read it from disk or network.
Reading data from disk takes longer than reading from memory, and reading over the network takes longer still.
When we query data over disk or network, we can spend more time reading the data than performing the computation part of the query.
We would like to limit the amount of time spent reading.
We cannot avoid reading the data altogether, but we may be able to limit the number of times we read the same data.
If we perform multiple queries over the same input data separately, each query needs to read the same data, and we end up reading the same data multiple times.
Ideally, we would like to perform multiple queries together, sharing the data among the queries, and reading the data only once.

Our data can exceed the size of memory; the working set of our query program cannot.
If our query program requires more memory than the computer has, it will not be able to compute the answer we desire.
When we write a query, we need to be sure that it will not run out of memory.
With datasets that grow every day, we also need to be sure that the query will not run out of memory when we execute it tomorrow or the day after.
Unless we want to keep adding new memory to the computer as the dataset grows, the memory usage should be as close as possible to constant, regardless of the data size.

One way to transform large datasets in constant memory is to write the query as a \emph{streaming program}.
Streaming programs can be written by composing stream transformers together.
Stream transformers consume data element by element, processing the elements in sequential order.
Stream transformers can only store a limited number of elements at a time.
These restrictions mean that we cannot, for example, sort all the input data, or read elements in random access.
The upside of these restrictions is that if we can write our queries as streaming programs, we can be confident that they will not run out of memory.

\TODO{Need to be explicit about the difference between `streaming' as a concept and and particular streaming models.} Different stream representations can represent different programs; we can think of a streaming model as the set of supported stream transformers, as well as the rules about how we can connect transformers together.

% If our input data is stored persistently on disk somewhere, we can perform multiple passes over the input if necessary.
% For example, if we wish to read elements in random access, we may be able to emulate this 
% This means that many streaming
% We would like to keep the number of iterations over the input data to a minimum.

\section{Gold panning}
Even with the bounded memory and sequential access restrictions of streaming, we can still write interesting queries as streaming programs.
Suppose we have a file containing the historical prices for a particular stock.
The file contains many records; each record contains a date and the average cost for that day, and all the records in the file are sorted by the date.

Records are stored on-disk in comma-separated values (CSV) format, and represented in memory by the following Haskell datatype:

\begin{lstlisting}
data Record = Record
 { date  :: Date
 , price :: Double }
\end{lstlisting}

We wish to evaluate this stock to see whether it is a worthy investment.
One quality of a good investment is that its price increases over time.
We can roughly quantify whether or not price increases over time by computing the Pearson correlation coefficient of the two.
If we already have a function to compute the correlation, we write the query as follows.

% correlation :: (a $\to$ Double) (a $\to$ Double) $\to$ Stream a $\to$ Query Double
\begin{lstlisting}
priceOverTime :: Stream Record -> Query Double
priceOverTime stock =
  correlation (daysSinceEpoch . date) price stock
\end{lstlisting}

The @correlation@ function takes the stream of records, as well as functions to extract the time and price axes from each record.
To extract the time, we need to convert it from a @Date@ to a number using the @daysSinceEpoch@ function.
This query executes by repeatedly pulling records from the input stream, and folding over the values to compute various sums and means.

This is a simple query, and it might even tell us something interesting.
However, stock prices rarely follow linear functions of time; even the best stocks go down once in a while, and sometimes the market as a whole can go down.
Furthermore, even if this stock appears to be doing quite well if we consider it in isolation, we do not know whether it is an exceptional stock or an exceptional time in the market.
We might be interested, then, in comparing against the rest of the market as well.

To compare against the rest of the market, assume we have another file of records, this one containing the average price of a somewhat representative subset of stocks.
This representative subset is called an \emph{index}.
We want to compare each day's price for our stock against the average price for the corresponding day in the index.
To compare day against day, we join up the stock with the index based on the date --- we can join the two in a streaming manner because both files are sorted by date.
If the files were not sorted by date, we would need to perform a non-streaming join, for example a hash-join, which stores the entirety of one input in a hashtable in memory.
% eg keep all of one input in memory and perform a hash-join, or read the file multiple times.
The join function takes the two input streams, as well as functions to extract the key from each input record.


% join :: Ord k $\Rightarrow$ (a $\to$ k) (b $\to$ k) $\to$ Stream a $\to$ Stream b $\to$ Query (Stream (a,b))
\begin{lstlisting}
priceAgainstMarket :: Stream Record -> Stream Record -> Query Double
priceAgainstMarket stock index = do
  joined <- join date date stock index
  correlation (price . fst) (price . snd) joined
\end{lstlisting}

The @join@ in this query executes by pulling values from both of its input streams, and comparing the keys.
If the keys are equal, @join@ gives the pair of values to @correlation@, and pulls new values from both streams.
Otherwise, @join@ pulls again from the input stream with the smaller key: since both streams are sorted by the key, if the other stream has a higher key it means the other stream does not have a corresponding value for the smaller key.

Hopefully this query will tell us something interesting.
Perhaps even both of these queries will tell us something interesting, so it would be nice to be able to compute both of them at the same time.
Ideally, we could write the program to compute both like so:

\begin{lstlisting}
priceAnalysis :: Stream Record -> Stream Record -> Query (Double, Double)
priceAnalysis stock index = do
  time   <- priceOverTime stock
  market <- priceAgainstMarket stock index
  return (time, market)
\end{lstlisting}

However, to execute both queries together, we need to coordinate between the two uses of the @stock@ input stream.
The @join@ inside @priceAgainstMarket@ pulls from its two inputs depending on which has the smaller key, while the @correlation@ inside @priceOverTime@ repeatedly pulls from the @stock@ stream.
We must coordinate between the @join@ and the @correlation@, so that @correlation@ only receives a new value when @join@ agrees to read a new value.
We must also be careful not to let one consumer get too far ahead of the other, because if one consumer gets ahead of the other we must store all the unconsumed elements in memory to allow the other consumer to catch up.

\begin{figure}
\center
\begin{dot2tex}[dot]
digraph G {
  node [shape="none"];
  stock; index;

  stock -> pot_cor;
  stock -> pam_join;
  index -> pam_join;

  subgraph cluster_priceOverTime  {
    pot_cor [label="correlation"];
  };

  subgraph cluster_priceAgainstMarket {
    pam_join [label="join"];
    pam_cor [label="correlation"];
    pam_join -> pam_cor;
  };
}
\end{dot2tex}
\caption[Two queries]{process network for the queries @priceOverTime@ and @priceAgainstMarket@.}
\label{figs/procs/priceOverTime-priceAgainstMarket}
\end{figure}

We can implement this program as communicating processes in a concurrent process network.
Each combinator inside each query, as well as each input stream, becomes a process in the network.
The process network for both queries is shown in \autoref{figs/procs/priceOverTime-priceAgainstMarket}.
Processes communicate through channels which they can pull values from, waiting until values are available, or push values to.
By making the channels fixed-size, we do not have to worry about one process getting too far ahead of another.
When a process tries to push to a lagging process, the pusher waits until the lagger has caught up, and has room for more values.
The concurrent process network will naturally find the right coordination between the processes.

Concurrent process networks give us the desired high-level semantics, but they do not provide an ideal execution strategy.
All the communication between processes introduces extra overhead: consider when one process sends a machine integer to another process.
If the sending process has performed any kind of computation on the integer, the sending process would most likely have the integer stored in a register, where it can be operated on directly by the processor.
When the sending process wishes to send this value to the other process, it must lock the channel to prevent other writers or premature reads, copy the integer to memory where it can be read by the other process, and unlock the channel.
Now, the receiving process must perform the inverse operation by locking the channel, copying the integer from memory to a register, and unlock the channel.
In terms of absolute runtime, this all happens quite quickly, but it is quite a roundabout way to copy an integer from one register to another register --- perhaps even the \emph{same} register, if the sending process has been swapped out, and the receiving process swapped in.

Of course, concurrent process network implementations often amortise the cost of communication by chunking messages together, and sending messages consisting of arrays of thousands of integers rather than a single integer at a time.
However, amortising cost .... is bad.
You know what's better than amortising the cost? Copying directly from a register to a register.




Unfortunately, if our streaming model is anything like the vast majority, this program will not compute the result we desire.
try to run these two queries together as above in the , we might be surprised by the result.
The majority of streaming libraries are \emph{pull-based}, which means that a stream consumer asks the producer for values.
In pull-based
% write this program with a popular streaming library such as Conduit\footnote{\url{https://hackage.haskell.org/package/conduit}},
the majority of streaming libraries will not unlikely to compute the result we desire.
However, in most streaming systems, this program will not compute the result we desire.
The problem is that consuming a stream is an effectful operation: once we compute the price over time, we have already used the whole.


To demonstrate the kinds of queries we want to run on this data, we will assume for now that the data fits in memory and write the queries using list operations.
In some circumstances we can use lazy evaluation to execute list operations in a streaming fashion, but when a list is used multiple times, the computed elements must be kept in memory until the .
In this case, because we want to execute multiple queries over the input we will be mentioning the same thing many times.
List operations require that the input data be stored completely in memory.
Let us assume that we have the data in a lazy list, \lstinline/[Record]/.
We can treat lazy lists as a kind of stream where the next value is computed on demand, as long as we are careful not to reuse the same list multiple times.
When a list is only used once, its values can be computed, used, and collected by the garbage collector.
If we reuse the same list, once its values are computed by one consumer they must be retained in memory until the other consumer uses them.
Despite the disadvantage of not being sure whether your program will execute in a streaming fashion, list combinators provide a convenient and expressive way to write our example queries.

Suppose we want to compute the mean of prices over all time.
We can write that as:

\begin{lstlisting}
priceMean :: [Record] -> Double
priceMean file = mean $ map price file
\end{lstlisting}

However, the mean on its own is not terribly interesting.
There might be variations on the price according to the day of the week: for example, on Mondays the market has been closed for two days, which may affect the price.
This hypothesis about the data may or may not be true, but to find out we must at least compute the result.
To validate (or more likely invalidate) this hypothesis, we can group the records by the day of the week, and perform the mean over the prices for each day of the week separately.

\begin{lstlisting}
weekdayMeans :: [Record] -> Map DayOfWeek Double
weekdayMeans file = 
  grouped (dayOfWeek . date)
          (mean . map price)
          file
\end{lstlisting}

Now, if we wish to compute these two queries, we can do that simply enough with lists.
\begin{lstlisting}
queries :: [Record] -> (Double, Map DayOfWeek Double)
queries file = (priceMean file, weekdayMeans file)
\end{lstlisting}

However, this program requires two passes over the input data.
Looping over the data multiple times is fine for list programs, but not for streaming programs.
Worse still, the list will be held entirely in memory.
We need to rewrite the program to run in a single pass.

If we have several stocks, we might wish to know how they perform together: when one goes up, does the other one go up?
This is called correlation.

\begin{lstlisting}
correlation :: [Double] -> [Double] -> Double
correlation x y = covariance x y / (stddev x * stddev y)
\end{lstlisting}

\begin{lstlisting}
queries :: [(Date,Price)] -> [(Date,Price)] -> Result
queries asx bty =
  let joined  :: [(Date,(Price,Price))]
      joined  = innerJoin asx bty
      prices  :: [(Price,Price)]
      prices  = map snd joined
      asxMean = mean $ map snd asx
      btyMean = mean $ map snd bty
      both    = correlation prices
  in (asxMean, btyMean, both)
\end{lstlisting}

\begin{lstlisting}
queries :: IO Result
queries = $(network $ do
  asx     <- records "ASX.csv"
  bty     <- records "BTY.csv"
  joined  <- innerJoin asx bty
  prices  <- map [||snd||] joined
  asxMean <- mean =<< map [||snd||] asx
  btyMean <- mean =<< map [||snd||] bty
  both    <- correlation prices
  return $ \k -> [|| Result $(result'mean k), $(result'cor k) ||])
\end{lstlisting}


\begin{lstlisting}
stats :: [Record] -> (Double, Double, Double)
stats file =
  let m = mean $ map price file
      e = exponentialMovingAverage 0.75 $ map price file
      g = correlation (map price file) (map (daysFromEpoch . date) file)
  in (m, e, g)
\end{lstlisting}

\section{The stuff streams are made of}

Highlight the fact that we are working with finite streams.

If we can represent each query as a fold over a single input, then combining multiple queries becomes quite easy.
Sometimes the hard part is figuring out how to represent the query as a fold.

The first question is how to represent streams.

We want a moderately interesting query with: map on input stream; multiple folds at the end; probably a filter.

\begin{lstlisting}
query1 inputs =
  let xs = map (+1) inputs
      y  = maximum xs
      z  = sum inputs
  in (last inputs, y / z)
\end{lstlisting}

\subsection{Push streams}


\begin{lstlisting}
-- data Push a = Maybe a -> IO ()
data Push a r = Push
  { push :: a -> IO ()
  , done :: IO r }
\end{lstlisting}

The map function for a push stream is a bit backwards: rather than taking the input stream as an argument and returning the output stream, it takes the output stream and returns the input stream.
The returned input stream can then be passed to another stream transformer which expects a stream to push to.
We suffix the names of push stream transformers with ``@o@'' for ``output''.

\begin{lstlisting}
map_o :: (a -> b) -> Push b r -> Push a r
\end{lstlisting}

In the definition of push streams above, the element type appears to the left of the function arrow, making push streams \emph{contravariant} in the element type.
Most stream representations are \emph{functors}, while push streams are \emph{cofunctors}.
With push streams we have to write the program in a backwards way, because our program is constructing a description of what to do with a value once it gets it.
We have to start at the end of the computation, say what we want to do with the result, and work backwards to turn the result into a push stream which accepts the input.

To implement a fold using push streams, the fold takes a mutable reference which is where to store the result once it is available, as well as the usual fold arguments, and returns the push stream.

\begin{lstlisting}
## fold_o :: IORef b -> (b -> a -> b) -> b -> Push a
fold_o :: (r -> a -> r) -> r -> Push a r
\end{lstlisting}

\begin{lstlisting}
dup_oo :: (b -> c -> d) -> Push a b -> Push a c -> Push a d
\end{lstlisting}

Here is the @query1@ query from earlier using push streams.

\begin{lstlisting}
query1'push inputs = do
  yref    <- newIORef
  zref    <- newIORef
  lastref <- newIORef

  ypush   <- fold_o (flip (max . Just)) Nothing
  zpush   <- fold_o sum 0
  lpush   <- fold_o (const . Just) Nothing

  folds   <- dup3_ooo ypush zpush lpush
  x       <- map_o (+1) folds

  drain inputs x

  y       <- readIORef yref
  z       <- readIORef zref
  l       <- readIORef lref

  return (l, y / z)
\end{lstlisting}

The push version executes in a single loop over the input, but requires quite a bit of extra boilerplate, and the logic is backwards.
If we annotate push streams with a return value we might be able to clean it up a bit.

\begin{lstlisting}
query1'push = do
  y   <- fold_o (flip (max . Just)) Nothing
  z   <- fold_o sum 0
  l   <- fold_o (const . Just) Nothing

  yz  <- dup_oo (/) y z
  lyz <- dup_oo (,) l yz

  map (+1) lyz
\end{lstlisting}


This is still not as nice as it could be.

\subsection{Icicle}

Push queries let us do many things at once with the same stream, but they are not as natural to program in as plain old lists.
Icicle is a query language for performing folds with push streams, which aims to provide nice syntax to make it closer to dealing with lists.
Icicle also has a type-system for ensuring that queries can be executed in a single pass.

\begin{lstlisting}
   let x = input + 1
in let y = max x
in let z = sum x
in (last x, y / z)
\end{lstlisting}

As a fold...
\begin{lstlisting}
let (l,m,s) =
  foldl (\(l,m,s) input ->
          let x = input + 1
          in (Just x, maybe (Just x) (max x) m, s + x))
        (Nothing, Nothing, 0)
(l, m / s)
\end{lstlisting}

Using the foldl applicative library
\begin{lstlisting}
let x = fmap (+1) inputs
    yz = (/) <$> maximum <*> sum
    lyz = (,) <$> last <*> yz
in fold lyz x
\end{lstlisting}

More Icicle query examples.
\begin{lstlisting}
table stocks { open : Int; close : Int }
query
  more = filter open > close of count;
  less = filter open < close of count;
  mean = filter open > close of sum open / count;
\end{lstlisting}

Query example where we can't compute, but could rewrite as a group:
\begin{lstlisting}
table kvs { key : Date; value : Real }
query avg = let k = last key
            in filter (key == k) of mean value;
\end{lstlisting}


\subsection{Pull streams}

\begin{lstlisting}
data Pull a = IO (Maybe a)
\end{lstlisting}

\subsection{Polarized}
Mixture of push and pull, but requires manually classifying streams in the process network as push or pull.
As with push streams on their own, if we embed push stream transformations inside polarized dataflow, we still need to write the push streams in an awkard, backwards way.
Cannot fuse \lstinline/unzip . zip/, which might not seem useful because it is superfluous, but it is possible that a larger program would have this somewhere inside it.

\subsection{Kahn process networks}

Push streams require a computation to have one producer which pushes to arbitrarily many consumers, which in turn may push to many consumers.
This structure is a tree, rooted at the top-most producer.
Pull streams, likewise, can only have one consumer and many producers: again a tree, this time rooted at the bottom-most consumer.
Programs, however, are not trees; they are \emph{networks}, and they can mention the same variables as many times as they want.
We use abstract syntax trees to represent programs, but in order to fit a network into a tree we must prune the `bad' branches; in an AST we refer to variables by name rather than occurrence.
It feels as though we are constantly butting our heads against the disconnect between a program's true meaning and the tree representation we are so used to.


\section{Many queries}

Assume we have many queries, and we have written each one as a separate streaming program.
We wish to execute all the queries together, so we only pay the overhead of reading from disk once.

Suppose we have some historical share prices for a particular company.
For each day, we have the share's open price at the start of the day, and the close price at the end of the day.

\begin{lstlisting}
data DailyPrice = DailyPrice
  { date  :: Date
  , open  :: Double
  , close :: Double }
\end{lstlisting}

We wish to query this data.

If we assume that the list of days are distinct, we can count how many days we have data for by folding over the list and incrementing the count for every element.

\begin{lstlisting}
countDays :: [DailyPrice] -> Int
countDays prices = foldl (\count _ -> count + 1) 0 prices
\end{lstlisting}

We can write a slightly more interesting query if we only count the `profit' days where the close price was above the open price.

\begin{lstlisting}
countProfitDays :: [DailyPrice] -> Int
countProfitDays prices =
 let profits = filter (\p -> open p < close p) prices
 in  foldl (\count _ -> count + 1) 0 profits
\end{lstlisting}

These queries are not very useful on their own, but if we put them both together they will tell us a bit more about the company.
If we wanted to compute both of these with regular lists, we could just construct a tuple and call both functions.
This approach will loop over the list twice as well as retaining the whole list in memory, which is unsuitable for streaming.

Instead, we must \emph{fuse} the queries together, combining the two into a single function that computes both.
We put the work of both queries inside a single loop over the list, which stores as loop state the count of all days, and the count of `profit' days.
To count all days we increment the count for each element, essentially applying the original fold for each element.
To count the `profit' days we must perform both the filter and the fold inside the loop, so we only apply the fold if the filter predicate is true.

\begin{lstlisting}
countBoth :: [DailyPrice] -> (Int, Int)
countBoth prices = loop 0 0 prices
 where
  loop sCountDays sCountProfitDays (p:prices')
   = let sCountDays' = sCountDays + 1
         sCountProfitDays'
          | open p < close p
          = sCountProfitDays + 1
          | otherwise
          = sCountProfitDays
     in loop sCountDays' sCountProfitDays' prices
  loop sCountDays sCountProfitDays []
   = (sCountDays, sCountProfitDays)
\end{lstlisting}

This fused query is more efficient than running both queries separately as it only requires a single traversal, but we have lost the composability benefits of using high-level combinators.
We have had to intertwine all the logic of the folds and the filter into a single place.
This program is not too hard to write by hand, but it is more complex than each original query individually.

A more interesting query is to compute the sum of the \emph{gap}.
The gap is the difference between one day's close price and the next day's open price, which tells us how perception of the stock changed overnight while the market was closed.
We can compute this by pairing each day with the day after, finding the difference between each pair, then summing the differences.

\begin{lstlisting}
sumGap :: [DailyPrice] -> Double
sumGap prices =
 let pairs       = zip prices (tail prices)
     differences = map (\(p,pTail) -> close p - open pTail) pairs
 in  sum differences
\end{lstlisting}

We wish to perform all three queries together, but because this query is a bit more complex than the previous ones, let us start by looking at how we can write this query on its own as a single loop.

\begin{lstlisting}
sumGap' :: [DailyPrice] -> Double
sumGap' prices = loop 0 prices
 where
  loop sSum []
   = sSum
  loop sSum (p:prices')
   = let sSum'
          | (pTail:_) <- prices'
          = sSum + (close p - open pTail)
          | otherwise
          = sSum
     in loop sSum' prices'
\end{lstlisting}

% \begin{lstlisting}
% sumGap' :: [DailyPrice] -> Double
% sumGap' prices = loop 0 prices
%  where
%   loop sSum (p:pTail:prices')
%    = let diff  = close p - open pTail
%          sSum' = sSum + diff
%      in loop sSum' (pTail:prices')
%   loop sSum [p]
%    = sSum
%   loop sSum []
%    = sSum
% \end{lstlisting}

The main difference between this loop and the one for @countBoth@ is that this loop looks at the two elements at the start of the list in order to compare two adjacent days.

To perform all three queries, @sumGap@, @countDays@, and @countProfitDays@.

\begin{lstlisting}
sumGapCount :: [DailyPrice] -> (Int, Int, Double)
sumGapCount prices = loop 0 0 0 prices
 where
  loop sCountDays sCountProfitDays sSum []
   = (sCountDays, sCountProfitDays, sSum)
  loop sCountDays sCountProfitDays sSum (p:prices')
   = let sCountDays' = sCountDays + 1
         sCountProfitDays'
          | open p < close p
          = sCountProfitDays + 1
          | otherwise
          = sCountProfitDays
         sSum'
          | (pTail:_) <- prices'
          = sSum + (close p - open pTail)
          | otherwise
          = sSum
     in loop sCountDays' sCountProfitDays' sSum' prices
\end{lstlisting}



% \begin{lstlisting}
% sumGap' :: [DailyPrice] -> Double
% sumGap' prices = init
%  where
%   init []          = 0
%   init (p:prices') = loop 0 p prices'
% 
%   loop sSum pTail []
%    = sSum
%   loop sSum pTail (p:prices')  
%    = let diff   = p - pTail
%          pTail' = p
%          sSum'  = sSum + diff
%      in  loop sSum' pTail' prices'
% \end{lstlisting}


Combining two queries is hard enough; when we want to combine three or more queries, we soon reach the limit of what can be achieved with intuition alone.
As our queries become more complex and more numerous, the compound becomes more complex.
We need a system to help us: a rigorous approach to transforming multiple queries into a single compound query.
This transform is called fusion.

\section{Push streams}

The first part of this thesis looks at fusing queries that use push streams.
Push streams are those where the producer controls the flow of computation, and the flow of values.
The producer pushes values into the consumer, and the consumer must accept with these values.
Producers determine the order in which values are given to consumers.

In push streams, one producer can push to many consumers.
Each query is a consumer, and the input file is the producer.
By pushing the data to all the queries, we are able to perform any number of queries at the same time.
Push streams allow us to take any queries operating over the same input and fuse them together.
We can rely on fusion.

Push streams have limited expressivity, however.
Push streams cannot support most use-cases with multiple input streams, such as appending streams, pairing them together, or merging sorted streams so that the result remains sorted.
When we have multiple input files, the query has a particular order in which it needs to read elements from each file.
In push streams, the producer imposes its order upon the consumer.
The query has no choice in the matter.
This limited expressivity is the price we pay for guaranteed fusion.

Trying to read from multiple input files as push streams is a bit like putting the files together and randomly interspersing the lines.
You can read a line, but you don't know which file it came from until after you read it.

For datasets with a single input file, push streams are a perfect solution.
Push streams can express any streaming computation with a single input file, while guaranteeing that all queries over the same input can be fused together.

\section{Pushing and pulling with Kahn process networks}

When we have multiple input files, we can no longer use push queries.
The consumer must choose which order to read from its input files.
The consumer must pull from its inputs, rather than the inputs pushing.

Pull streams, however, introduce their own problems.
Just as push streams only support one producer, pull streams only support one consumer.
In pull streams, the consumer is in control of the computation.
There can only be one controller and there can only be one consumer.
Having one consumer limits us to one query.
Pull streams are unsuitable for performing multiple queries at the same time.


Pull streams and push streams have one thing in common: there is a single point of control for the computation.
Where the control lies differs: in push it is in the producer; in pull it is in the consumer.
Our problem is that we are limited to a single point of control, while we have several queries.

Having a single point of control is very useful for a machine; after all this is how single-threaded processors execute programs.
But we wish to write better programs.
We wish to help programmers write better programs.
One of the ways to do that is by abstracting over the machine itself, and abstracting over the accident of sequential shared state imperative programs.

A single point of control is not necessary or ideal for streaming programs.
We can write programs with multiple points of control: concurrent programs.
Concurrent programs with shared state are absurdly difficult to write, and even harder to get right.
A program's state space is quadratic in the number of points of control.
This state space means for a programmer to reason about correctness, a programmer must reason about a quadratic number of cases.
If we wish to reduce the burden of streaming and writing compound queries by hand, we cannot increase the burden of reasoning.

Determinism is the key ingredient to make concurrent programming tractable.
Requiring concurrent programs to be deterministic tames the state space explosion.
We no longer have to reason about a quadratic number of states, because we know that whichever order the programs execute in, we will get the right result.

We want a concurrent, deterministic streaming model.
Such a model is called a Kahn Process Network.
Kahn Process Networks are networks of concurrent processes, communicating via channels, with streams of elements flowing along channels.
Determinism is achieved by a few restrictions: all communication between processes is through channels; all reading from channels is blocking; and all channels are owned by a single process.
Kahn Process Networks turn non-deterministic concurrency into a deterministic computation.

% There is no state shared between processes, which means the only way processes can communicate is through channels.
% 
% A process cannot peek to see if a channel has a value before deciding whether to read it.
% Once a process is reading from a channel, it must wait until a value is available.
% Peeking would allow a process to observe the non-determinism in scheduling.
% 
% There is no race between two processes to see which can read from a channel first.
% Kahn Process Networks are just swell.

Kahn Process Networks are a good computational method, but not the best execution strategy.
Sending values across channels is expensive: we must lock the channel, copy the value onto the queue, and unlock it.
By the time the consumer receives the value, it is unlikely to be in cache, and definitely no longer in a register.
To ameliorate the concurrency overhead, implementations tend to chunk together values into arrays before sending them.
This can only reduce the overhead; it can never remove it entirely.
Instead, we wish to compile away the overhead.
We take the processes in a Kahn Process Network, and convert it into a single process that does the job of all the processes together, without any communication.

% In general, not all queries can be compounded together.
% 
% This compounding operation is called horizontal fusion.
% Horizontal in this case refers to how data flow graphs are arranged; nodes operating on the same input data, which can be performed in parallel or sequentially, are typically horizontally adjacent.
% Vertical fusion refers to nodes operating in a pipeline, where each node feeds values to the node below.
% 
% Stream fusion is a rich area of research which has for the most part focussed on vertical fusion.
% 
% The reason for compounding queries is to reduce execution time.
% It is quite important, then, that our compound query does not take longer to execute than the original queries, combined.
% In fact, we would like our compound query to execute in almost as little time as the slowest query on its own.
% What we would like is not always achievable, but it can be helpful to have something to strive for.



