\chapter{Introduction}
\label{introduction}
To learn interesting things from large datasets, we generally want to perform lots of queries.
When our query is small and our data is big, we might spend more time reading the data than we spend computing the answer.
In this case, we would like to amortise the cost of reading the data by performing multiple queries at the same time.

When querying datasets that do not fit in memory or disk, it can be hard to ensure that our query program's working set will fit in memory.
One way to transform large datasets in constant memory is to write the query as a \emph{streaming program}[ref taxonomy], which we can write by composing stream transformers together.
Composing stream transformers together adds some performance overhead, which is usually removed by \emph{fusing} together multiple transformers into a single transformer.

% The most common stream representation is a pull stream [ref]. Pull streams do not support multiple queries.

This thesis concerns low-overhead streaming models for executing multiple queries at a time.
We focus on two streaming models: push streams [ref taxonomy/push], and Kahn process networks [ref taxonomy/kpn].

Push streams support multiple queries, but these queries can be quite unwieldy to write as they must be constructed ``back-to-front'' [ref taxonomy/push/backwards].
In [ref icicle] we introduce a query language called Icicle, which allows programmers to write and reason about queries using a more familiar list-based semantics, while retaining the execution strategy of push streams.
The typesystem of Icicle aims to ensure that well-typed query programs have the same semantics whether they are executed as list programs or as stream programs.
% Icicle is a streaming query language, which means that queries can only perform a single pass over the input data.
% The Icicle compiler translates queries written in Icicle to push streams, and fuses multiple queries together to execute multiple queries at a time.

However, push streams do not support computations with multiple inputs except for non-deterministically merging two streams, and in some circumstances appending streams [ref taxonomy/push/many-inputs].
Kahn process networks support both multiple inputs and multiple queries, but require dynamic scheduling and inter-process communication, which can introduce significant overhead.
In [ref kpn/fusion] we introduce a method for taking multiple processes in a Kahn process network and fusing them together into a single process.
This fusion method generalises previous work on stream fusion [ref kpn/generalises] and demonstrates the connection between fusion and synchronised product of processes [ref kpn/synchronised-product], which is generally used as a proof technique rather than an optimisation.

[todo: clustering]

\section{Outline}
\section{Contributions}

The contributions of this thesis are:

\begin{itemize}
\item
r[ref icicle]:
When fusion is a requirement rather than an added bonus for optimisation, type systems can be used to enure that only fusible programs can be expressed.
The streaming query language Icicle uses modal types to ensure only fusible programs are valid, and that the stream program has the same semantics as if it were operating over lists.

\item
r[ref kpn]:
A method for fusing combinators; the first that allows multiple inputs, multiple queries, and arbitrary combinators.
This is achieved by treating each combinator as a sequential process, and the combinators together as a concurrent process network.
Processes are then fused together using an extension of synchronised product.

\item
r[ref kpn/proof]:
A proof of correctness for the above-mentioned fusion system, mechanised in the proof assistant Coq.
The proof states that when two processes are fused together, the fused process computes the same result as the original processes.
% (\refChapter{C:process-correct})

\item
r[ref clustering todo]:
When not all combinators can be fused into a single pass over the input, the decision of \emph{clustering} --- how to group the combinators together --- becomes important.
Choosing a clustering that minimises a particular objective is NP-hard \CITE{Alain Darte}.
\CITE{Megiddo}'s clustering algorithm converts imperative loop nests to integer linear programs to find the optimal clustering according to some objective function.
By working with high-level combinators rather than loop nests, we extend this clustering algorithm to recognise that filters can be fused with their consumer or their producer.
\end{itemize}



% All the data in the world won't tell you anything if you don't look at it.
% To learn from the data, we must ask it a question by querying the data.
% Once we have learnt one thing, we will find ourselves wanting to know more; knowledge is addictive.
% To learn many things from the data, we must not just ask it one question; we must interview it.
% We must query the data many times.
% 
% When our data is too large to fit in memory, we must read it from disk or over the network.
% Reading data from disk takes longer than reading from memory, and reading over the network takes longer still.
% When we query data over disk or network, we can spend more time waiting for data than performing computations with the data.
% For a computer, time spent waiting is time wasted, so we would like to limit the amount of time spent waiting for data.
% We cannot avoid reading the data altogether, but we may be able to limit the number of times we read the same data.
% If we perform multiple queries over the same input data separately, each query needs to read the same data, and we end up reading the same data multiple times.
% Ideally, we would perform multiple queries together, sharing the data among the queries, and reading the data only once.
% 
% Our data can exceed the size of memory; the working set of our query program cannot.
% If our query program requires more memory than the computer has, it will not be able to compute the answer we desire.
% When we write a query, we need to be sure that it will not run out of memory.
% With datasets that grow every day, we also need to be sure that the query will not run out of memory when we execute it tomorrow or the day after.
% Unless we want to keep adding new memory to the computer as the dataset grows, the memory usage should be as close as possible to constant, regardless of the data size.
% 
% One way to transform large datasets in constant memory is to write the query as a \emph{streaming program}\REFTODO{streaming}.
% We can write streaming programs by composing stream transformers together.
% Stream transformers consume data element by element, processing the elements in sequential order, and can only store a limited number of elements at a time.
% Because of these restrictions a stream transformer cannot, for example, sort all the input data, or read elements in random access.
% The upside of these restrictions is that if we can write our queries as streaming programs, we can be confident that they will not run out of memory when we execute them.
% 
% Composing stream transformers together adds some performance overhead, which is usually removed by \emph{fusing} multiple transformers into a single transformer \REFTODO{fusion}.
% Unfortunately, stream systems that support executing multiple queries at the same time do not perform fusion, or are limited in expressivity \REFTODO{background}.
% This thesis aims to address these limitations by proposing a streaming system based on concurrent process networks and fusion method.

% \TODO{Need to be explicit about the difference between `streaming' as a concept and and particular streaming models.} Different stream representations can represent different programs; we can think of a streaming model as the set of supported stream transformers, as well as the rules about how we can connect transformers together.

% If our input data is stored persistently on disk somewhere, we can perform multiple passes over the input if necessary.
% For example, if we wish to read elements in random access, we may be able to emulate this 
% This means that many streaming
% We would like to keep the number of iterations over the input data to a minimum.

