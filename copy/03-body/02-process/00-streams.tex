% TODO: merge into taxonomy if necessary. I think this entire file can be deleted
\chapter{Pulling and pushing}
\label{chapter:process:streams}

Streams can be roughly classified into two types: pull and push.
This classification denotes where the control of the computation lives.
For pull streams, the \emph{consumer} is in control of \emph{pulling} values from the stream producers.
For push streams, the \emph{producer} is in control of \emph{pushing} values to the stream consumers.
This distinction is not just a matter of taste, but in fact determines which streaming computations that can be performed \citep{kay2009you}.
Roughly, pull computations can express joins, where multiple input streams are joined into one stream --- for example zipping two streams together.
Push computations can express splits, where a single output stream is split into multiple streams --- for example partitioning according to a predicate.
Pull computations, however, cannot support splits, and push computations cannot support joins.

\section{Pull streams}
Pull streams are also known as sources, as they are a `source' which is always ready to return the next value.
We can encode this as a function that performs some effect (such as reading from a file), and returns a @Maybe@ value - either the value for the stream, or the end of the stream.
As a Haskell datatype, this is:

\begin{code}
data Pull a
  = Pull
  { pull :: IO (Maybe a) }
\end{code}

More sophisticated representations are possible and will be discussed later in \autoref{sec:process:streams:coaxing}, and while these representations may have better runtime performance, the expressivity remains the same.

In order to convert a list to a stream, we need to construct a mutable reference for the list, then each request to pull a value will inspect and update the reference.
If the reference contains an empty list, the stream is over and we return @Nothing@. If the reference contains a list cell, we update the reference to refer to the rest of the list, and return the current value.

\begin{code}
pullOfList :: [a] -> IO (Pull a)
pullOfList xs = do
  ref <- newIORef xs 
  Pull (go ref)
 where
  go ref = do
    list <- readIORef ref
    case list of
     []     -> return Nothing
     (l:ls) -> do
      writeIORef ref ls
      return l
\end{code}

Mapping a function over a list is simple: we take the function to perform, and the stream to apply it to.
We construct a new pull stream where each request for a value asks the input stream for its value, and inspects it.
If it is @Nothing@, the stream is over and we return @Nothing@ as well. If the stream is not over, we apply the function and return the new value.

\begin{code}
pullMap :: (a -> b) -> Pull a -> Pull b
pullMap f (Pull as) = Pull bs
 where
  bs = do
   v <- as
   case v of
    Nothing -> return Nothing
    Just a  -> return (f a)
\end{code}

If we wish to operate over some prefix of a stream, we can take just the first few elements as well. We start by constructing a reference for the current count, and each pull increments and checks if we've hit count yet --- if we have, it's the end of the stream.

\begin{code}
pullTake :: Int -> Pull a -> Pull a
pullTake count (Pull as) = do
  ref <- newIORef 0 
  Pull (go ref)
 where
  go ref = do
    i <- readIORef ref
    writeIORef ref (i + 1)
    if i < count
      then as
      else return Nothing
\end{code}

There are many ways to join streams together. Zipping takes two streams and returns the pair from each stream, pairing them together in parallel.
We can implement this by requesting a value from both input streams, then pairing them together if neither stream is finished.

\begin{code}
pullZip :: Pull a -> Pull b -> Pull (a,b)
pullZip (Pull as) (Pull bs) = Pull abs
 where
  abs = do
   a <- as
   b <- bs
   case (a, b) of
    (Just a', Just b') -> return (Just (a',b'))
    (_      , _      ) -> return  Nothing
\end{code}


\subsection{Linearity}
So far, pull streams have treated us well. We have been able to implement a few simple operations, and it shouldn't require too much imagination to believe that other operations such as append could be implemented too.
However, there is a large restriction identified by \citet{bernardy2015duality}: linearity.
Linearity means that each stream must be mentioned once: no more, no fewer.
(This restriction also applies to pull streams, as we will see later.)

To show this, let us try zipping a stream with itself: given an input stream defined by converting the list (@[1, 2, 3, 4]@), we will zip it with itself.

\begin{code}
xs <- pullOfList [1, 2, 3, 4]
pullZip xs xs
\end{code}

For this example, the result we desire is each element paired with itself: (@[(1, 1),@ @(2, 2),@ @(3, 3),@ @(4, 4)]@).
However, the result actually intersperses each element with the next: (@[(1, 2), (3, 4)]@).
This is because the two streams share the same underlying mutable reference, and pulling from one inadvertently updates the other.

In this case we could duplicate the call to @pullOfList@, but this is not always possible: if instead of reading from a list we were reading from a network socket, the network socket cannot be duplicated.
One proposed solution is to add an explicit caching operator that stores a buffer of unread values between each consumer. However, if the input data is large, this will cause memory problems --- in general, an unbounded buffer is required. 

This is not just a theoretical problem either, and there are real, practical programs that require sharing of streams.
One such example is @filterMax@, which is the core of the Quickhull algorithm.
This computes the maximum element of a stream at the same time as filtering the elements.

\begin{code}
filterMax :: Pull Int -> (Pull Int, Int)
filterMax pts
 = let above = pullFilter (>0) pts
       maxim = pullMaximum     pts
   in (above, maxim)
\end{code}

As it stands, we cannot execute this program: the maximum will consume the entire stream, leaving nothing left for the filter.
We could hand-fuse these two operations together, into a special kind of filter that performs a fold at the same time, or even more generally, a fold that passes stream values through.
This hypothetical new operation has some surprising interactions with existing combinators though.
Before, when we implemented @pullTake@ we assumed that we could throw away the rest of the stream when we are finished. We did the same with @pullZip@, when one of the input streams is shorter.
However, if something upstream is relying on the whole stream being pulled, we can no longer throw away remainder streams: take and zip must be modified to drain the rest of the stream even if they are never needed.
This is an unsatisfying solution, as not only do we have to rewrite our program, we have lost the nice abstraction layer that streaming combinators gave us in the first place.


\section{Push streams}

Push streams are the dual of pull streams: rather than the consumer controlling evaluation, the producer is in control of the operation.
Push streams are also known as sinks: they are a `sink' which can always be pushed into.
We encode push streams as a function taking a @Maybe@ value, and performing some effect.

\begin{code}
data Push a
  = Push
  { push :: Maybe a -> IO () }
\end{code}

Because the producer controls push streams, instead of converting a list to a stream as we did with pull streams, we must take the stream to push into as an argument.
We then recurse over the list, pushing each element in turn, then push @Nothing@ to note that the stream is finished.

\begin{code}
pushList :: [a] -> Push a -> IO ()
pushList as (Push into) = go as
 where
  go [] = into Nothing
  go (x:xs) = do
    into (Just x)
    go xs
\end{code}

Mapping is also a bit different for push streams, as they are contravariant. Rather than taking a function from @a@ to @b@ and a stream of @a@, we instead take a stream of @b@.
That is, given something we can push @b@s into, and a way to convert @a@s to @b@s, we can return something to push @a@s into.

\begin{code}
pushMap :: (a -> b) -> Push b -> Push a
pushMap f (Push bs) = Push as
 where
  as Nothing  = bs Nothing
  as (Just a) = bs (Just (f b))
\end{code}

Filter and take can be implemented in a similar way.

What push streams can do that pull streams cannot, is sharing.
If we have two push streams that accept elements, we can construct a new push stream that sends elements to both:

\begin{code}
pushDup :: Push a -> Push a -> Push a
pushDup (Push a) (Push b) = Push ab
 where
  ab v = do
   a v
   b v
\end{code}

A more familiar version of this operation is @unzip@: given a stream of pairs, we can push the first halves into the first stream, and the second halves into the second stream.
With this sharing, we \emph{can} implement the @filterMax@ example from before.

Push streams are not perfect, however, and we have simply traded one set of problems for another.
While pull streams are able to express zip but not unzip, push streams can express unzip but not zip.
More generally, push streams cannot express joins where the consumer needs to choose the order: zipping, appending, value-dependent merging, and so on.

Push streams can, however, express a kind of non-deterministic merge by simply reusing the same push stream multiple times.
In this case, the producers determine the merge order, and this can be used as a limited form of appending in some cases.

\section{Coaxing the compiler}
\label{sec:process:streams:coaxing}
There are other stream representations, but they do not afford any extra expressivity in terms of zip and unzip.

\subsection{Co-streams}
It is possible to invert the push stream to use a continuation, rather than the value itself.
This is a push stream where each outer value is a continuation of what to do with the actual value.

\begin{code}
data Copull a
  = Copull
  { copull :: (Maybe a -> IO ()) -> IO () }
\end{code}

\citet{bernardy2015duality} call this @CoSrc@.
According to \citet{biboudis2017expressive} this stream representation is better for Java JIT compilation, as it allows the producer to have a simple for-loop structure, while the consumer can be inlined at runtime.
It is not obvious how to implement either zip or unzip for this stream representation, so perhaps it is the intersection between push and pull: only straight-line computations.


\section{Pulling and pushing, together}

Recent work on Polarized Flow Fusion \citep{lippmeier2016polarized} supports both pulling and pushing, by constructing a data-flow network with sources for inputs and sinks for outputs.
Operations on streams have an explicit polarity attached, to make sure any connections can be performed without buffering.
Sources can have multiple inputs, while sinks can be explicitly duplicated and shared.
This requires inspecting the entire data-flow network to determine which parts should be sources, and which should be sinks --- more manual labour that reduces the level of abstraction the programmer is working at.

The key insight here is that pull streams \emph{can} be duplicated, but only if they are duplicated into a push stream.
This combinator is called @dup_ioi@, where @i@ stands for input or source, and @o@ stands for output or sink.
This means that it takes a pull stream and a push stream, and returns a new pull stream.
Here, we construct a pull stream that, whenever it is pulled, pulls from its input and pushes that value to the push stream, as well as returning the value.

\begin{code}
dup_ioi :: Pull a -> Push a -> Pull a
dup_ioi (Pull i) (Push o) = Pull (do
  v <- i
  o v
  return v)
\end{code}

Normal duplication of push streams is also allowed, and is called @dup_ooo@: taking two push streams and returning a push that pushes into both of them.
Other combinators with multiple outputs follow a similar protocol: unzip can be encoded as both @unzip_ioi@ where one of the outputs is a push and the other is a pull, or it can be encoded as @unzip_ooo@ where both outputs are pushes.
As mentioned earlier in pull streams, one must be careful when taking a prefix of a stream: if an upstream producer is relying on you to pull all the values because it is passing them along to another consumer, you must explicitly drain any leftovers on the stream, even if you are not interested in the remaining values.

It isn't always a simple matter of assigning polarities to combinators, however.
Some computations have both splits and joins, but when the splits occur \emph{before} the join, it cannot be separated into just sources and sinks.
If we had a stream of pairs and wanted to apply some function to the first half of each element, and another function to the second half.
We could write this by unzipping the stream into two halves, applying each map separately, and then zipping the sides back together.
\begin{code}
mapPairs :: (a -> c) -> (b -> d) -> Stream (a,b) -> Stream (c,d)
mapPairs f g abs
 = let (as, bs) = unzip abs
       cs       = map f as
       ds       = map g bs
       cds      = zip cs ds
   in  cds
\end{code}

This is a rather contrived example, and probably not a program you or I would write by hand, but it may still occur as a result of inlining and transforming some larger computation.

\section{Discussion}

Clearly, neither pull nor push alone are sufficient for the kind of streaming operations we wish to run.
We wish for a stream representation that gives the appearance of pulling from producers and pushing to consumers, but without prematurely tying ourselves in to a particular implementation.
We wish to be able to write streaming computations that consume from multiple streams, as well as producing multiple streams.
In the next chapter, I present a process calculus for streaming computations, and later I will show an efficient execution model, by \emph{fusing} them together.

