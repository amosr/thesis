\chapter{Implementation and code generation}
\label{chapter:process:implementation}


We have implemented this fusion system [the one described in chapter \ldots] in a library called @folderol@\footnote{\url{https://github.com/amosr/folderol}}.
Rather than modifying the compiler to support our particular fusion system, we use Template Haskell, which allows us to perform code-generation at compile-time: a form of macro metaprogramming.

This chapter explains how to generate code for processes, while taking advantage of the existing optimisations in the Glasgow Haskell Compiler (GHC).
Generating code for a single process is fairly straightforward in itself; the process language is a simple imperative language, and once the process network has been fused into a single process there is no longer any need for threading or inter-process communication.
Haskell supports imperative language constructs like mutable references, but being a functional language, the optimisations in GHC are more geared towards functional programs.
If we wish to get good performance, we must generate code closer to what the compiler expects.

\section{All this boxing and unboxing}
In Haskell, most values are boxed by default \citep{jones1991unboxed}.
Boxed values are stored as pointers to heap objects, which can in turn reference other boxed or unboxed values.
Boxed values are useful for implementing parametric polymorphism because they give a uniform representation to all the different types.
A list which is polymorphic in its element type can use a pointer to refer to its values regardless of the actual element type.

The problem with boxed values is that they require an extra allocation per object, as well as a pointer indirection for each access.
This can cause performance issues in tight loops, particularly because most objects are immutable.

Consider the following function, which loops over an array to compute its sum.
It starts by calling the local function @loop@ with the initial loop index, and the initial sum.
The definition of @loop@ checks if it has reached the end of the array, and if so returns the sum; otherwise it increments the running sum and proceeds to the next index.

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop 0 0
 where
  loop index running_sum
   | index == length vector
   = running_sum
   | otherwise
   = let value = vector ! index
     in loop (index + 1) (running_sum + value)
\end{lstlisting}

It is not clear from the program source alone, but the loop index and the running sum are both boxed values, because their type (@Int@) is boxed.
If this were compiled naively it would be rather disastrous for performance, as in order to process each element of the array, it must allocate two new boxed values.
Hilariously, all of these new boxed values except the very last iteration are used once by the next iteration and then thrown away.
While the garbage collector is tuned for small, short-lived objects, it is better to not introduce the garbage in the first place.

Removing boxing is not a novel thing, and there are many ways to do this.
The point to make is not that this is an interesting thing, just that we must know how it works in order to generate good code that fits it.
Boxed machine-word integers are represented by the following type, which defines @Int@ with a single constructor @I#@, taking an unboxed integer @Int#@. By convention, unboxed values and constructors that use them are named with the @#@ suffix.

\begin{lstlisting}
data Int = I# Int#
\end{lstlisting}

Now we know how machine-word integers are represented, we can look at an explicitly boxed version of @sum@.
This is still using boxed integers, but all arithmetic is explicitly unboxing and reboxing.
Unboxed literals are written as @0#@ or @1#@.
Unboxed arithmetic are written as @+#@ or @==#@, and @!#@ for unboxed indexing.
With explicit boxing, it should now be visible that the recursive call to @loop@ constructs new boxed integers.

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop (I# 0#) (I# 0#)
 where
  loop (I# index) (I# running_sum)
   | index ==# length vector
   = I# running_sum
   | otherwise
   = let value = vector !# index
     in loop (I# (index +# 1#)) (I# (running_sum +# value))
\end{lstlisting}

Constructor specialisation \cite{peyton2007call} is a loop optimisation that can remove these boxed arguments to recursive calls.
It looks at the constructors to recursive calls, and counts which ones are scrutinised or unwrapped at the start of the function definition.
In this case, @loop@ is first (and later, as well) called with the constructors @I#@ for both arguments, and both arguments are scrutinised.
So it creates a specialised version of @loop@ where both arguments are @I#@ constructors.
This specialised version is the same as the original, except the arguments are known to be @I#@ constructors, which means the pattern matching on the arguments can be simplified away.
We will call this specialised version @loop'I#'I#@.
Then everywhere that @loop@ is called with @I#@ constructors, it will be replaced with a call to @loop'I#'I#@.
So any function call that looks like (@loop (I# x) (I# y)@) is replaced by a call to our new function (@loop'I#'I# x y@).

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop'I#'I# 0# 0#
 where
  loop'I#'I# index running_sum
   | index ==# length vector
   = I# running_sum
   | otherwise
   = let value = vector !# index
     in loop'I#'I# (index +# 1#) (running_sum +# value)
\end{lstlisting}

Constructor specialisation has removed all the boxing except for the final return value, which is only constructed once anyway.
In this example, the original @loop@ function was no longer called, so it was able to be removed entirely.
It is not always the case that the original function can be removed, and constructor specialisation can duplicate the code many times: once for each combination of constructors.
This can cause quite a lot of copies of the original function, which can cause large intermediate programs that do not fit in memory.
To alleviate this, GHC implements some heuristics to limit the number of duplicates created, as well as only creating specialisations if the original function is not too large.
This makes sense for general purpose code, but for tight loops where we expect most of our runtime to be, we really want to be sure that all specialisations are created.
For tight loops, we want to \emph{force} constructor specialisation to occur as much as possible.
This is achieved by annotating the function to be specialised with the special constructor @SPEC@.
Going back to the original @sum@ function, if we want to force constructor specialisation on @loop@, we can do this by adding the @SPEC@ to the function binding as well as all calls to it:

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop SPEC 0 0
 where
  loop SPEC index running_sum
   | index == length vector
   = running_sum
   | otherwise
   = let value = vector ! index
     in loop SPEC (index + 1) (running_sum + value)
\end{lstlisting}

We have seen that GHC is able to eliminate boxing from function arguments, and we will take advantage of this during code generation.
We will make use of @SPEC@ to force constructor specialisation, to ensure as much can be unboxed as possible.
Sadly, mutable references are stored boxed, and an analogous constructor specialisation transform does not exist for mutable references.
This means that in order to get unboxed values, we must structure our generated code to pass values via function arguments instead of mutable references.

Unboxed mutable references do exist, but are unsuitable because they can \emph{only} store unboxed values.
Recursive types such as linked lists cannot be stored in unboxed references.
We desire an unboxed representation when possible, and boxed representation when necessary.

It may be surprising to users of other languages that we should move away from using mutable references in favour of function arguments.
Indeed, \citet{biboudis2017expressive} describes the \emph{opposite} transform when implementing stream fusion in MetaOCaml.
So this will not necessarily map to other languages, but it is true in the particular case of GHC.

In Data Flow Fusion \cite{lippmeier2013data} there is a transform called \emph{loop winding}, which converts mutable references to function arguments.
The motivation here is that GHC does not track aliasing information of arrays stored in mutable references, but does track it for arrays as function arguments.

\subsection{Extended constructor specialisation}

Constructor specialisation is not limited to boxing and unboxing, but works for arbitrary constructors, including types with multiple constructors such as (@Maybe a@) or (@Either a b@).
It even works for recursive types such as lists, which could produce an an infinite number of specialisations.
Constructor specialisation must be careful to limit the specialisations to a finite number of \emph{useful} ones.

Information about the initial state can be very helpful in finding the most specific call patterns.
In the following example, @go@ is first called with the call pattern (@go (Just _) (Just _)@).
Using this as the `seed' from which we start exploring, we can see that the initial call pattern proceeds to the next call pattern (@go Nothing (Just _)@), followed by a call to (@go Nothing Nothing@).
If we were to look at the body of @go@ without this initial seed, however, we would find the call patterns (@go Nothing _@) and (@go _ Nothing@).
These call patterns from the unseeded body are less specific than the call patterns for the seed, which means using them would not allow the second argument to be specialised away.
By starting from the initial seed, the extra information about the initial state can be propagated to the other states.

\begin{lstlisting}
initial = go (Just 1) (Just 2)
 where
  go (Just _) b       = go Nothing b
  go a       (Just _) = go a       Nothing
  go Nothing Nothing  = 0
\end{lstlisting}

Not all specialisations are useful.
To limit compilation time, memory usage and code blowup, it is important to limit the specialisations to those which will be used.
That is, \emph{only} those which are reachable from the initial state.
In the following example, the initial state is the call pattern (@go (Left _) (Right _)@).
At each step, the arguments are flipped, so from the initial state the next reachable call pattern is (@go (Right _) (Left _)@).
From here, we can get back to the original state.
This means in total there are only two reachable call patterns.

However, if we look at the body alone without the seed, the first two call patterns are (@go _ (Left _)@) and (@go _ (Right _)@).
From here, more call patterns can be found: (@go _ (Left _)@) calls (@go (Left _) (Left a)@) and (@go (Left _) (Right _)@).
Similarly, there are two call patterns reachable from @(go _ (Right _)@), and these are distinct from the two already seen.
In this way, starting from the initial state means we do not have to generate all the possible specialisations.

\begin{lstlisting}
reachable = go (Left 1) (Right 2)
 where
  go (Left  a) b = go b (Left  a)
  go (Right a) b = go b (Right a)
\end{lstlisting}

TODO: diagrams.

Using the initial calls as the seed is important.
This \emph{was} implemented, but it only occurred for locally bound functions, not for top-level bindings.
The problem is that even though our examples were locally bound functions, other transforms such as let-floating occur before constructor specialisation, which means locally bound functions can be `floated' up to top-level bindings, where seeding does not work.
The other issue is that top-level bound functions can be exported; if functions are exported, we cannot know their initial call pattern, as they may be called from other modules.
So for exported top-level functions, we must seed the call-patterns using all initial calls in the current module, as well as those in the body.
For non-exported top-level functions, we can be sure that the initial state is in the current module, and so use any initial calls outside of the body as the seed.

When arguments are of recursive types, there can be an infinite number of reachable call patterns.
Suppose we wish to reverse a linked list.
We can write this using a helper function, which takes the list that is reversed so far, as well as the list to reverse.

\begin{lstlisting}
reverse :: [Int] -> [Int]
reverse xs0 = go [] xs0
 where
  go zs []     = zs
  go zs (x:xs) = go (x:zs) xs
\end{lstlisting}

The helper function @go@ could be specialised an infinite number of times, but this would lead to non-terminating compilation.
First, the call to @go@ is seeded with the call pattern (@go [] _@).
Then, at every step in the evaluation, a list constructor is moved from the second argument to the first, resulting in the infinite chain of call patterns, (@go [_] _@), (@go [_, _] _@), and so on.
Usually, these specialisations would not be produced because they do not reduce allocation.
However, in the original implementation, when @SPEC@ is used to force constructor specialisation, an infinite number of specialisations were produced, and the compiler did not terminate.
I implemented Roman Leshchinskiy's suggestion to fix this by setting a limit on how many times recursive types can be specialised, even when forcing constructor specialisation.


\section{Template Haskell}
Template Haskell is a metaprogramming extension for Haskell.
Template Haskell is a limited form of staged computation, where the only `stages' are compile-time and run-time.
It has two modes: splicing and quasiquoting.
Splicing \verb/$$(f)/ expects the type of @f@ to be @Q (TExp t)@, and evaluates @f@ to an expression at compile time.
The splice is replaced with the result expression.
Quasiquoting \lstinline/[||x||]/ constructs the expression representation of @x@.

The original Template Haskell paper \cite{sheard2002template} doesn't include \emph{typed} Template Haskell, which is what we really want to talk about.
In the original paper, generated expressions don't have types.
Typechecking does still occur on the expressions, though, but only after they have been spliced together.
This is less of a problem for generation alone --- if you trust the generation code.
But when you want higher-order templates that take expressions as arguments, you want to be able to make sure your caller gives you an expression of the right type.
Otherwise the type error will be very well hidden, somewhere inside the generated code.
For the user, having to figure out where their input expression ended up inside the generated code, is a real hassle.
So the typed Template Haskell attaches a type argument to each expression: \lstinline|TExp t| is an expression that, when evaluated (under an empty environment?) returns a value of type \lstinline|t|.

There is also a @Q@ monad which is used when constructing and splicing expressions.
The @Q@ monad mainly gives a fresh name supply, so that when names are bound inside expressions they can be given unique names. This way they will not interfere with other bindings.

The obligatory example for staged computation is a power function, where the exponent is known at compile time, but the mantissa is not known until runtime.
In general staged computation, the exponent is known one stage before the mantissa, but because Template Haskell only supports two stages, the exponent stage is necessarily compile-time.

We define the @power@ function with the type \lstinline/Int -> Q (TExp (Int -> Int))/. This means it takes an integer at compile-time, and produces a computation in the quote monad (@Q@), returning an expression which, at runtime, will take an integer and return an integer.

\begin{lstlisting}
power :: Int -> Q (TExp (Int -> Int))
power 0 = [||\i -> 1                       ||]
power n = [||\i -> @$$(power (n - 1))@ i * i ||]
\end{lstlisting}

The power function pattern matches on the exponent.
When it is zero, we enter quasiquoting mode and construct a function that always returns 1.
When the exponent is non-zero, we again enter quasiquoting mode, and construct a function.
Inside the quasiquote, we need to handle the recursive case, so we enter back into splicing mode with \lstinline/@$$(power (n - 1))@/ to compute the one-smaller power, which returns the function expression, to which we apply @i@. Finally, we multiply the smaller power of @i@ with @i@ itself.
This function does not handle negative exponents: it is just to show the use of staging.

We can then, in another module, define a specialised power function that computes the square.
We define this as a top-level binding, by performing a splice, and inside that splice we call @power@ with the statically known at compile-time argument @2@.
So the type inside the splice is \lstinline/Q (TExp (Int -> Int))/: a quoted computation returning an expression of type \lstinline/Int -> Int/.
After this is spliced in, it unwraps the quote computation and expression and we end up reifying it to a real function of \lstinline/Int -> Int/.

\begin{lstlisting}
power2 :: Int -> Int
power2 = @$$(power 2)@
\end{lstlisting}

This is a ``top-level splice'', because it is not inside a quasiquotation.
Top-level splices can only refer to bindings imported from other modules, not ones defined locally.
This is why it needs to be in a different module.
This is just a silly restriction to be aware of. Is it even worth mentioning?

If we turn on the compiler option @-ddump-splices@ we can view the resulting code.
The names of variables have changed slightly for for readability.

\begin{lstlisting}
Splicing expression
    power 2 => \i0 -> (\i1 -> (\i2 -> 1) i1 * i1) i0 * i0
\end{lstlisting}

It's a pretty roundabout way to multiply a number by itself. There are a lot of opportunities for simplifying that code. And while GHC should be able to remove these, it would be better to not introduce them in the first place.
So let us fix it.
The function @powerS'@ (@S@ for \emph{simpler}) takes the argument as a `real' argument, rather than returning an expression of function type: its type is \lstinline/Int -> Q (TExp Int) -> Q (TExp Int)/.
We've moved the function from later (expression) to now (value).
But the argument is still an expression.

\begin{lstlisting}
powerS' :: Int -> Q (TExp Int) -> Q (TExp Int)
powerS' 0 i = [||1                          ||]
powerS' n i = [||@$$(powerS' (n-1) i)@ * @$$(i)@||]
\end{lstlisting}

The definition of @powerS@ introduces the lambda binding as before, but passes the expression of this binding to the worker function.
We enter quoting mode, then introduce a lambda. Then we go back into splicing mode, in order to call the helper function @powerS'@.
Then, in order to pass the mantissa @i@ to the helper function, we need to go back into quoting mode.
Note that the quoted expresssion \lstinline/[||i||]/ is \emph{open}: it refers to bindings outside the environment.
This means if you somehow kept a hold of that expression and used it in a different context, outside the lambda binding, it would be incorrect.
So you can construct bad, ill-typed expressions with Template Haskell. This is certainly not ideal, but is just something to be aware of.

Is it worth noting that MetaOCaml has similar problems, including BER MetaOCaml?
You don't need to rant about let-insertion in MetaOCaml: how it is allegedly safe, but it is not \emph{type} safe.

\begin{lstlisting}
powerS :: Int -> Q (TExp (Int -> Int))
powerS n = [||\i -> @$$(powerS' n [||i||])@||]
\end{lstlisting}

The output is a lot simpler now.
Whereas before there was a lambda introduced and applied at each recursive step, now there is only a single lambda.

\begin{lstlisting}
Splicing expression
    powerS 2 => \i -> ((1 * i) * i)
\end{lstlisting}

There is still more we could do to improve the function, for example removing the multiplication by one, but this is sufficient to show the core splicing and quoting ideas behind Template Haskell.
For more information on staging in general, \citet{rompf2010lightweight} takes this example further.

Meta-Repa similar idea but for flat data parallel computations, not really for streaming computations \cite{ankner2013edsl}.
But it also uses Template Haskell, so that's worth mentioning.

\subsection{Untyped expressions}
We can also go between the typed and untyped representation.
The type @Exp@ is an untyped Template Haskell expression.
It still has a type, but this is not known or checked until it is spliced in.
Like typed expressions, one can quasiquote untyped expressions with \lstinline/[|e|]/ and splice with \lstinline/@$(e)@/.
There is only one pipe or dollar-sign in the untyped case, rather than two.

One can convert from a typed expression to an untyped expression.
This does not affect the expression itself, and once it is spliced in it will have the same type.
It just throws away some type-level information.

\begin{lstlisting}
unTypeQ :: Q (TExp a) -> Q Exp
\end{lstlisting}

Conversely, if one is very careful, one can convert an untyped expression to a typed one.
This is an unsafe operation, because one can choose any type at all for the expression.
The expression is not checked against the chosen type until it is spliced in.
\begin{lstlisting}
unsafeTExpCoerce :: Q Exp -> Q (TExp a)
\end{lstlisting}

\section{Constructor Specialisation}
Constructor specialisation is another idea originating in staged compilation / partial evaluation \cite{mogensen1993constructor}.
Staged variables can be classified as static or dynamic, but this can be too coarse: sometimes you have partially-static constructors with some dynamic values.
For example, if you have a value datatype with two constructors, \lstinline/VInt :: Int -> Value/ and \lstinline/VString :: String -> Value/.
If you evaluate a type-correct program to add two numbers, you know statically that the values should be ints: but you don't know what the values should be.
When it is statically known that some constructors are only instantiated with partially static values, rather than treating the values dynamically, these values can be lifted out to separate constructors.
So you add new constructors for each combination of statically-known information.
Actually, this isn't so related. It's close but not worth going into the details.

In Haskell, there isn't so much connection with staged compilation, but the distinction between static and runtime is still useful.
Talk about SpecConstr --- what it does, example of how it's used.
Is it worth mentioning worker/wrapper transform?

Constructor specialisation, or call-pattern specialisation \cite{peyton2007call}, is an optimisation for recursive functions.
When recursive calls are made with particular constructors, and the next recursive step will perform case analysis on the newly allocated constructor, we can remove the middle step.
Instead of allocating a new constructor, scrutinising it, and then throwing it away, we can define specialised versions of the function, for each particular constructor.
There are two good bits. First, we don't need to allocate anything, and second, we can jump straight to the right part of the function.
An example.
The naive way to write @last@, which takes a list of @a@ and returns @Maybe a@: if the list is empty, it returns @Nothing@; otherwise it returns @Just@ the last element.

\begin{lstlisting}
last :: [a] -> Maybe a
last []     = Nothing
last (x:[]) = Just x
last (_:xs) = last xs
\end{lstlisting}

This isn't so good.
It isn't a particularly good example, either.

\begin{lstlisting}
last :: [a] -> Maybe a
last []     = Nothing
last (x:xs) = Just (last' x xs)
 where
  last' x []     = x
  last' _ (x:xs) = last x s
\end{lstlisting}

Well let's try an example from Stream Fusion \cite{coutts2007stream}.
This is what zip looks like.

\begin{lstlisting}
zipS :: Stream a -> Stream b -> Stream (a,b)
zipS (Stream stepA initA) (Stream stepB initB)
 = Stream step' (Left (initA, initB))
 where
  step' (Left (stateA, stateB))
   = case stepA stateA of
      Yield a stateA' -> Skip (Right (stateA', stateB, a))
      Skip    stateA' -> Skip (Left  (stateA', stateB))
      Done            -> Done
  step' (Right (stateA, stateB, a))
   = case stepB stateB of
      Yield b stateB' -> Yield (a,b) (Left  (stateA, stateB'))
      Skip    stateB' -> Skip        (Right (stateA, stateB', a))
      Done            -> Done
\end{lstlisting}

Zip is too complicated. Try append.

I don't really want to explain stream fusion here, so this isn't a great example either.
But the idea here is that we define a non-recursive stepper function. The non-recursive part is important, because recursive functions are much harder to inline.
So we deconstruct the @Stream@ for each input, and pull out the step function (@stepL@, @stepR@) and the initial state (@initL@, @initR@).
The result stream has the step function @step'@, and the initial state (@Left initL@): starting with the `left' side of the append, with the initial state for the left stream.
The step function looks at the current state.
If it is left, we evaluate and scrutinise the left input stream's step function.
If the left stream produces a value (@Yield@), the appended stream produces that value, and the new state is still running the @Left@ stream, but with the updated state @stateL'@.
If the left stream produces no value (@Skip@), but just updates its state, the append also produces no value and updates its state accordingly.
Finally, if the left stream is finished (@Done@), we update the state to the right stream with the initial state: (@Right initR@).
We do the same for the right side, except when it finishes, the entire stream is finished.

\begin{lstlisting}
appendS :: Stream a -> Stream a -> Stream a
appendS (Stream stepL initL) (Stream stepR initR)
 = Stream step' (Left initL)
 where
  step' (Left  stateL)
   = case stepL stateL of
      Yield a stateL' -> Yield a (Left  stateL')
      Skip    stateL' -> Skip    (Left  stateL')
      Done            -> Skip    (Right initR)
  step' (Right stateR)
   = case stepR stateR of
      Yield a stateR' -> Yield a (Right stateR')
      Skip    stateR' -> Skip    (Right stateR')
      Done            -> Done
\end{lstlisting}

For illustration, we will look at the list case. The function above shows the stream version, but we are interested in the list version.
We can obtain this list version by converting the input lists to streams, executing @appendS@, then converting its result back to a list.
When we put a conversion to list on either side of this function, we end up with this function.
We have taken the step function above and turned it into a recursive function that takes the current state, and returns a list.
Each step with a new state is replaced by a recursive call.
The input streams are replaced by lists.
Where the output produced a @Yield@ before, we now produce a list cons, with the value part the yielded element, and the rest of the list is the recursive call with the updated state.
So @Yield a (Left stateL')@ becomes @a : go (Left ls')@.
Similarly, @Skip@ becomes a recursive call and @Done@ becomes the empty list.


\begin{lstlisting}
append :: [a] -> [a] -> [a]
append ls0 rs0
 = go (Left ls0)
 where
  go (Left (a:ls'))
   = a : go (Left ls')
  go (Left  [])
   = go (Right rs0)
  go (Right (a:rs'))
   = a : go (Right rs')
  go (Right [])
   = []
\end{lstlisting}

The problem here is that, at each iteration of the recursive loop, we are constructing a new @Left@ or @Right@ at the end of one iteration, and immediately scrutinising it at the start of the next iteration.
So all the machinery of stream fusion hasn't given us any benefit yet --- we only have a single combinator, and we're allocating more intermediate rubbish than the original list implementation.
Constructor specialisation takes the recursive calls with constructors, and specialises them to a function for that constructor.
So where we have (@go (Left ls')@), we want to create a specialised function for the @Left@ constructor, and call it with (@go'Left ls'@).
The body of @go'Left@ is the same as @go@, except that we know the argument is a @Left@, so we can simplify the right-hand side.
We perform the same specialisation for the @Right@ case.

\begin{lstlisting}
append :: [a] -> [a] -> [a]
append ls0 rs0
 = go'Left ls0
 where
  go'Left (a:ls')
   = a : go'Left ls'
  go'Left []
   = go'Right rs0

  go'Right (a:rs')
   = a : go'Right rs'
  go'Right []
   = []
\end{lstlisting}

Now our new version doesn't construct any @Left@ or @Right@ values.
We have removed the intermediate allocations, which gives a significant performance increase.

\subsection{ForceSpecConstr}
For these stream programs, we want to force constructor specialisation to remove as much as possible.
Constructor specialisation is generally conservative because it duplicates the code.
If we duplicated a function a thousand times, we would need a thousand copies of the original function.
Duplicating too much code leads to very large intermediate programs, and long compilation times.
The Glasgow Haskell Compiler has heuristics designed to only duplicate code when it knows it will be a benefit.
But with the stream fusion, we need to be sure that we have removed all the extra intermediate state allocations, because otherwise stream fusion could be a \emph{pessimisation} instead of an optimisation.

To tell GHC to always do as much constructor specialisation as possible, we construct an annotated type @SPEC@. Whenever a function has an argument of type @SPEC@, it will be fully specialised.
We define a datatype with two constructors, @SPEC@ and @SPEC2@, even though we will only use one of the constructors.
The reason for \emph{two} constructors is interesting.
A datatype with one constructor and no arguments is called a ``unit type'' and is treated specially by GHC, because it conveys no information.
If a function has an argument of unit, you already know its value before looking at it.
Equationally, GHC is allowed to remove these ``information-free'' arguments before constructor specialisation runs, but if they are removed, constructor specialisation no longer knows it has to force them.
We trick GHC into keeping the @SPEC@s around until constructor specialisation, by adding another constructor.

\begin{lstlisting}
data SPEC = SPEC | SPEC2
{-# ANN type SPEC ForceSpecConstr #-}
\end{lstlisting}

Then, when we have a recursive function we want to force, at every recursive call we pass @SPEC@, and in the function definition, we add a \emph{bang pattern} to the @SPEC@ argument to force it.
\begin{lstlisting}
append :: [a] -> [a] -> [a]
append ls0 rs0
 = go SPEC (Left ls0)
 where
  go !_ (Left (a:ls'))
   = a : go SPEC (Left ls')
  go !_ (Left  [])
   = go SPEC (Right rs0)
  go !_ (Right (a:rs'))
   = a : go SPEC (Right rs')
  go !_ (Right [])
   = []
\end{lstlisting}

Not all is well, however.
We can't always force constructor specialisation, because recursive types.
Example of infinite looping with recursive constructors.


\section{Example}

Look at a simple example first: we just want to read some file and do some things.
It doesn't matter what.
\begin{lstlisting}
mapFilter :: FilePath -> FilePath -> IO ()
mapFilter fileIn fileOut = do
  @$$(fuse $ do
     ins    <- source @[||sourceOfFile fileIn||]@
     above  <- filter @[||\i -> i > 0        ||]@ ins
     double <- map    @[||\i -> i * 2        ||]@ above
     sink double      @[||sinkToFile fileOut ||]@)@
\end{lstlisting}

We start with the Template Haskell splice \verb/$$(fuse ...)/. In the code it is blue. 
It has the following type.
\begin{lstlisting}
fuse :: Monad m => Network m () -> Q (TExp (m ()))
\end{lstlisting}
That is, it takes a process network with underlying monad @m@ and returns the expression for the underlying computation in the monad @m@.
The process network @Network m ()@ is a monad as well.

The process network first constructs a source that reads from a file.
\begin{lstlisting}
source :: Q (TExp (Source m a))              -> Network m (Channel a)
filter :: Q (TExp (a -> Bool))  -> Channel a -> Network m (Channel a)
map    :: Q (TExp (a -> b))     -> Channel a -> Network m (Channel b)
sink   :: Q (TExp (Sink m a))   -> Channel a -> Network m (Channel a)
\end{lstlisting}
The @source@ function takes a quasiquoted expression of how to construct the source at runtime.

Now show the generated code.

\section{Types}
The first thing to do is implement sources and sinks: pull and push streams.
Folderol does use pull and push streams, but only as the end-points for computations.
The input streams at the very start of computations will be sources, and the output streams at the very end are sinks.
These are how we interface with the outside world.
Streams inside the computation use internal communication, and can be fused away in the end. They can be implemented as values passed via function calls.

\begin{lstlisting}
data Source m a
 = forall s.
   Source
 { init :: m s
 , pull :: s -> m (Maybe a, s)
 , done :: s -> m ()
 }
\end{lstlisting}

\begin{lstlisting}
data Sink m a
 = forall s.
   Sink
 { init :: m s
 , push :: s -> a -> m s
 , done :: s -> m ()
 }
\end{lstlisting}

\section{Size hints}
\label{s:implementation:sizehints}
Talk about @vectorSizeIO@ and why it's useful.
Reference \autoref{s:Future:SizeInference} for how to infer this.

% -----------------------------------------------------------------------------
\subsection{Optimisation and Drop Instructions}
\label{s:Optimisation}
After we have fused two processes together, it may be possible to simplify the result before fusing in a third. Consider the result of fusing @group@ and @merge@ which we saw back in Figure~\ref{fig:Process:Fused}. At labels @F1@ and @F2@ are two consecutive @jump@ instructions.
The update expressions attached to these instructions are also non-interfering, which means we can safely combine these instructions into a single @jump@.
In general, we prefer to have @jump@ instructions from separate processes scheduled into consecutive groups, rather than spread out through the result code.
The (PreferJump) clauses of Figure~\ref{fig:Fusion:Def:StepPair} implement a heuristic that causes jump instructions to be scheduled before all others, so they tend to end up in these groups.

Other @jump@ instructions like the one at @F5@ have no associated update expressions, and thus can be eliminated completely. Another simple optimization is to perform constant propagation, which in this case would allow us to eliminate the first @case@ instruction. 

Minimising the number of states in an intermediate process has the follow-on effect that the final fused result also has fewer states. Provided we do not change the order of instructions that require synchronization with other processes (@pull@, @push@ or @drop@), the fusibility of the overall process network will not be affected.

Another optimization is to notice that in some cases, when a heap variable is updated it is always assigned the value of another variable. In Fig.\ref{fig:Process:Fused}, the @v@ and @x1@ variables are only ever assigned the value of @b1@, and @b1@ itself is only ever loaded via a @pull@ instruction. Remember from \S\ref{s:Fusion:FusingPulls} that the variable @b1@ is the stream buffer variable. Values pulled from stream @sIn1@ are first stored in @b1@ before being copied to @v@ and @x1@. When the two processes to be fused share a common input stream, use of stream buffer variable allows one process to continue using the value that was last pulled from the stream, while the other moves onto the next one. 


When the two processes are able to accept the next variable from the stream at the same time, there is no need for the separate stream buffer variable. This is the case in Figure~\ref{fig:Process:Fused}, and we can perform a copy-propagation optimisation, replacing all occurrences of @v@ and @x1@ with the single variable @b1@. To increase the chance that we can perform copy-propagation, we need both processess to want to pull from the same stream at the same time. Moving the @drop@ instruction for a particular stream as late as possible prevents a @pull@ instruction from a second process being scheduled in too early.

To increase the chance that we can perform this above copy-propagation, we need both processess to want to pull from the same stream at the same time. In the definition of a particular process, moving the @drop@ instruction for a particular stream as late as possible prevents a @pull@ instruction from a second process being scheduled in too early. In general, the @drop@ for a particlar stream should be placed just before a @pull@ from the same stream. 


\section{Untyped interface}
Talk about how convert from @TExp a@ to @Exp@, and why.
Our processes can have arbitrarily many input channels and output channels, which makes it hard to construct a type-safe process, since each channel can be a different type.
We would need a list of types --- a type-level list --- for the type of each input channel, and another list for the output channels.
Each process also has a store (heap) for local variables, and these would need a different type for each variable as well.

These problems are not insurmountable: one can use type-level lists and fake dependent types in Haskell \CITE, but this is not really the point of this thesis.
We take a simpler tack.
Instead, we provide an untyped core for the processes and networks, then build on top of this to provide a typed interface for constructing networks.

Type safety.
This is a similar idea as our untyped core providing a ``trusted base''.
One must be very careful that it is indeed safe, but this is not exposed to the user, and is small enough to reason about by hand.
Also, the code generation will still end up being typechecked by the Haskell compiler.
We are not losing any actual typesafety, then, because the code will still eventually be typechecked.
What we are losing is good error locations. If there are bugs in the untyped core, the errors will show up somewhere in the generated code.
This may make it hard to track down the original error location.
But if the core is correct, this should not happen.
So this is the idea at least.

\section{Instructions}

\begin{lstlisting}
data Info
 = Info
 { infoBindings     :: Set Var
 , infoInstruction  :: Instruction
 }

data Instruction
 = I'Pull Channel Var Next Next
 | I'Push Channel Haskell.Exp Next
 | I'Jump Next
 | I'Bool Haskell.Exp Next Next
 | I'Drop Channel Next
 | I'Done

data Next
 = Next
 { nextLabel    :: Label
 , nextUpdates  :: Map Var Haskell.Exp
 }
\end{lstlisting}

\begin{lstlisting}
data Process
 = Process
 { pName          :: [Char]
 , pInputs        :: Set Channel
 , pOutputs       :: Set Channel
 , pInitial       :: Next
 , pInstructions  :: Map Label Info
 }
\end{lstlisting}
