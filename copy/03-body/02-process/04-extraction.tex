\chapter{Implementation and code generation}
\label{chapter:process:implementation}


Folderol\footnote{\url{https://github.com/amosr/folderol}} is a Haskell implementation of Machine Fusion \REFTODO{processes and fusion}.
Folderol uses Template Haskell, a form of macro meta-programming, to perform custom code-generation at compile-time.
In this way, we can perform non-local\footnote{although not necessarily `global' in the whole-program sense} transforms over a process network, as well as providing fusion guarantees by allowing the programmer to designate when fusion failures should be compile-time errors.
This is in contrast to fusion systems using local rewrite rules, which can only perform local shortcut fusion, and where the only means of telling when fusion did \emph{not} occur is to inspect intermediate code \REFTODO{shortcut; this will be expanded in an earlier chapter, so this is just a reminder}.

This chapter looks at the implementation, in particular how to generate efficient code for a fused process; other parts of this chapter are mere prerequisites for this.
Generating code for a single process is fairly straightforward in itself: the process language is a simple imperative language, and once the process network has been fused into a single process there is no longer any need for threading or inter-process communication.
However, code generation needs to be tailored specifically to take advantage of the optimisations in the target language, which in this case is the Glasgow Haskell Compiler (GHC).
Haskell does support imperative constructs like mutable references, but being a functional language, the optimisations in GHC are more geared towards functional programs.
Mutable references are not optimised particularly well.
If we wish to generate efficient code, we must --- perhaps surprisingly --- avoid mutable references, and generate code closer to what the compiler expects.

\section{Template Haskell}
Template Haskell is a metaprogramming extension for Haskell \cite{sheard2002template}.
Template Haskell can be thought of as an untyped, limited form of staged computation, where the only `stages' are compile-time and run-time.
There is also a typed variant, which we will use for defining a typesafe interface, while using the untyped variant for code generation.
Let us study the untyped variant first.

\subsection{Untyped expressions}

Template Haskell extends regular Haskell with two syntactic constructs: quasiquoting and splicing.
Quasiquoting and splicing allow us to jump back and forth between executing code at compile-time and generating code to be evaluated at run-time.

Quasiquoting converts the syntactic form of an expression to an abstract syntax tree representation which can be manipulated.
For example, we can quasiquote some arithmetic using the syntax \lstinline/[|1+2|]/, which produces the corresponding abstract syntax tree for an infix operator with two integer arguments:

\begin{lstlisting}
InfixE (Just (LitE (IntegerL 1))) (VarE GHC.Num.+) (Just (LitE (IntegerL 2)))
\end{lstlisting}

Quasiquoting is a purely syntactic convenience, as any quasiquoted expressions could be produced using the abstract syntax tree constructors directly.
That said, it is a particularly useful convenience which we will use extensively.

Splicing is the opposite of quasiquoting: it takes a compile-time abstract syntax tree and converts it back to an expression which will be evaluated at runtime.
For example, splicing back the quasiquoted arithmetic using the syntax \lstinline/$([|1+2|])/, will evaluate (@1+2@) at runtime.

Splicing and quasiquoting operate in the @Q@ monad.
The @Q@ monad gives a fresh name supply, so that when names are bound inside expressions they can be given unique names.
This way newly bound names will not interfere with existing bindings.

Now let us look at a concrete example of how to use Template Haskell.
Suppose we wish to define a power function where the exponent is known at compile time, but the mantissa is not known until runtime.

We define @power@ as a function with the type \lstinline/Int ->  Q Exp/.
This means it takes an integer at compile-time, and produces a quoted expression in the quote monad.
This result expression will, once spliced in, have type \lstinline/Int ->  Int/.
Later we shall use typed Template Haskell to make this extra type information explicit, but for now we must perform typechecking in our heads.

\begin{lstlisting}
power :: Int -> Q Exp
power 0 = [|\m -> 1                      |]
power e = [|\m -> $(power (n - 1)) i * i |]
\end{lstlisting}

The @power@ function pattern-matches on the exponent.
When the exponent is zero, we enter quasiquoting mode and construct a function that always returns one.
This function ignores its input argument, the mantissa.
When the exponent (@e@) is non-zero, we again enter quasiquoting mode and construct a function, taking the mantissa (@m@) as its argument.
Inside the quasiquote, we need to handle the recursive case for the one-smaller exponent (\lstinline/e - 1/), so we enter into splicing mode with \lstinline/$(power (e - 1))/ to compute the one-smaller power.
The splice for the one-smaller power returns a function, to which we apply the mantissa.
Finally, we multiply the one-smaller power of the mantissa by the mantissa itself.

We can now define a specialised @power@ function to compute the square.
We define this as a top-level binding which performs a splice, and inside that splice we call @power@ with the statically known argument @2@.
The type inside the splice is \lstinline/Q Exp/, and once splicing is complete the result is an expression of type \lstinline/Int ->  Int/.

\begin{lstlisting}
power2 :: Int -> Int
power2 = @$(power 2)@
\end{lstlisting}

% This is a ``top-level splice'' because it is not inside a quasiquotation.
% Top-level splices can only refer to bindings imported from other modules, not ones defined locally.
% For this reason, @power2@ must be defined in a different module to @power@.
% Cross-stage persistence.

The reason for specialising @power2@ is efficiency, so it is worth inspecting the resulting code to check that it is indeed efficient.
The compiler option (@-ddump-splices@) outputs the result splices, as follows.
% The names of variables have changed slightly for for readability.

\begin{lstlisting}
$(power 2) =
  \m0 -> (\m1 -> (\m2 -> 1) m1 * m1) m0 * m0
\end{lstlisting}

This is a roundabout way to multiply a number by itself, and there are a lot of opportunities for simplifying that code.
While we expect the compiler to be able to remove these inefficiencies by beta reduction, it would be even better to not introduce them in the first place.
If we do not introduce inefficiencies in the first case, there is no uncertainty about whether the compiler will be able to remove them.
The problem is that @power@ introduces a lambda for each recursive step, while we only really need one lambda at the top-level.
So let us fix this by defining a top-level function which introduces the lambda, and a recursive helper function to compute the power.

The top-level function @powerS@ takes the exponent as an argument, then inside quasiquote, constructs a lambda expression for the mantissa (@m@).
Now in order to call the recursive helper function, @powerS'@, which also returns an expression (@Q Exp@), we need to re-enter splicing mode.
The helper @powerS'@ takes two arguments, the first being the exponent, and secondly the expression denoting the mantissa (@m@), since the actual mantissa is not known yet.
To pass the expression denoting the mantissa, we need to switch once more into quasiquoting mode.

\begin{lstlisting}
powerS :: Int -> Q Exp
powerS e = [|\m -> $(powerS' e [|m|])|]
\end{lstlisting}

We now define the recursive helper function, @powerS'@, with type \lstinline/Int ->  Q Exp ->  Q Exp/.
Like the original @power@, it starts by pattern-matching on the exponent.
Unlike in @power@, we have already bound the expression denoting the mantissa to the parameter @m@, so we need not introduce any lambdas.
When the exponent is zero, we return the quasiquoted expression for one.
When the exponent is non-zero, we recursively compute the one-smaller power, and multiply by the splice of the mantissa expression.

\begin{lstlisting}
powerS' :: Int -> Q Exp -> Q Exp
powerS' 0 m = [|1                        |]
powerS' e m = [|$(powerS' (e-1) m) * $(m)|]
\end{lstlisting}

% Note that the quoted expresssion \lstinline/[|m|]/ is \emph{open}: it refers to bindings outside the environment.
% This means if you somehow kept a hold of that expression and used it in a different context, outside the lambda binding, it would be incorrect.
% So you can construct bad, ill-typed expressions with Template Haskell. This is certainly not ideal, but is just something to be aware of.

The output for (@powerS 2@) is a lot simpler than that for (@power 2@).
Whereas before there was a lambda introduced and applied at each recursive step, now there is only a single lambda.

\begin{lstlisting}
$(powerS 2) =
  \m -> ((1 * m) * m)
\end{lstlisting}

There is still more we could do to improve the function: for example, (@1 * i@) could be replaced by @i@.
However, this is sufficient to show the core splicing and quoting ideas behind Template Haskell.
For more information on staging in general, \citet{rompf2010lightweight} takes this example further.

\subsection{Typed expressions}

One problem with Template Haskell shown above is that there are no types attached to expressions.
Quasiquoting a string \lstinline/[|"one"|]/ and quasiquoting an integer \lstinline/[|1|]/ both produce a value of the same type: @Q Exp@.
We can then use these expressions to construct larger, completely untypable expressions; for example we could try to subtract a string from an integer, which should surely fail: \lstinline/[|1 - "one"|]/.
Typechecking does still occur on the generated expressions eventually, but only after they have been spliced in to the main program.
This means that the when the programmer sees the type error, it may be nested arbitrarily deep within the generated code.
That is, the error will be shown in code the program wrote, rather than code the programmer wrote.
This complicates the matter of finding the root cause and fixing the problem.
This is an even bigger problem when we wish to provide a library for other programmers: it is unacceptable to require a user of the library to understand the internal workings of code generation, in order to understand type errors.

% The original Template Haskell paper doesn't include \emph{typed} Template Haskell, which is what we really want to talk about.
% In the original paper, generated expressions don't have types.
% Typechecking does still occur on the expressions, though, but only after they have been spliced together.
% This is less of a problem for generation alone --- if you trust the generation code.
% But when you want higher-order templates that take expressions as arguments, you want to be able to make sure your caller gives you an expression of the right type.
% Otherwise the type error will be very well hidden, somewhere inside the generated code.
% For the user, having to figure out where their input expression ended up inside the generated code, is a real hassle.

Typed Template Haskell extends the Template Haskell we have seen with typed expressions, typed splicing and typed quasiquoting.
The type of typed expressions, @TExp@, is annotated with a type argument denoting the type that the expression will have when spliced in.
Syntactically, typed quasiquotation uses two pipes, while typed splicing uses two dollar signs.
For example, using a typed quasiquote on a string will produce a @String@-typed expression: \lstinline/[||"one"||] :: Q (TExp String)/
Similarly, typed splicing eliminates the @Q@ monad and the @TExp@ wrapper, leaving only the result type: \lstinline/$$([||"one"||]) :: String/.

Typed expressions are invaluable for providing a typesafe way to construct process networks.
We need to construct process networks at compile-time in order to fuse them at compile-time, while keeping the information required to execute them at runtime.
Consider the standard @filter@ function for lists, with type \lstinline/(a ->  Bool) ->  [a] ->  [a]/.
In order to implement a process network version of @filter@, the function argument of type \lstinline/(a ->  Bool)/ must be converted to an expression, as it will be evaluated at runtime.
Providing a @filter@ function which takes as an argument the typed expression (\lstinline/Q (TExp (a ->  Bool))/) instead of the untyped expression (\lstinline/Q Exp/) means that type errors can be caught early.

However, for code generation the untyped variant is an easier target.
While typesafe code generation is no doubt possible, it is an interesting research problem in its own right\CITE{Accelerate?}.
Instead, we provide an untyped core for the processes and networks, then build on top of this to provide a typed interface for constructing networks.
For this reason, we need to be able to convert from typed expressions to untyped expressions.
We can convert from a typed expression to an untyped expression using the @unTypeQ@ function, which has type \lstinline/Q (TExp a) ->  Q Exp/.
This does not affect the expression itself, and once it is spliced in it will have the same type.
It just throws away some type-level information.

% Conversely, if one is very careful, one can convert an untyped expression to a typed one.
% This is an unsafe operation, because one can choose any type at all for the expression.
% The expression is not checked against the chosen type until it is spliced in.
% \begin{lstlisting}
% unsafeTExpCoerce :: Q Exp -> Q (TExp a)
% \end{lstlisting}

By using untyped expressions for code generation, we do not lose any actual type safety, since the generated code will still end up being typechecked by the Haskell compiler.
What we do lose are good error locations, but these errors will only occur if there are bugs in the code generator.
Any type errors in the user of the library will be using the typesafe interface, with better error messages.

% Although it is possible to construct expressions with the wrong type, this generally requires explicitly unsafe operations.
% Most of the time, this is unlikely to occur by accident alone.

Meta-Repa \cite{ankner2013edsl} also uses Template Haskell for code generation, and provides a similar distinction between typed interface and untyped code generation.
It provides a generalised abstract datatype (GADT) for its typed interface rather than using typed Template Haskell, but this is presumably only because it was implemented before typed Template Haskell.

\section{Constructing a process network}

Now let us construct an example process network in Folderol.
In due course we will inspect its generated code, but first it is necessary to see how process networks are constructed.
For this example we shall write a program to read input lines from a file, filter out all except those starting with the string \lstinline/"Good"/, and finally write the results to file.
In a Unix-style environment, we would write this as ``@grep ^Good@''; the caret (@^@) meaning ``starting with''.
For this reason we call the function @grepGood@.

\begin{lstlisting}
grepGood :: FilePath -> FilePath -> IO ()
grepGood fileIn fileOut =
  $$(fuse $ do
     input  <- source [||sourceOfFile fileIn ||]
     goods  <- filter [||isPrefixOf   "Good" ||] input
     sink      goods  [||sinkToFile   fileOut||])
\end{lstlisting}

The function @grepGood@ takes two arguments of type @FilePath@.
These are the input and output filenames.
Inside the definition, the Template Haskell splice \lstinline/$$(fuse ...)/ takes the process network as an argument, fuses the processes together, and generates output code.
We are constructing the process network inside the Template Haskell splice at compile-time, while we wish to execute it at run-time.
The process network is the static representation of the computation, and must be statically known and finite.

Inside the process network, we start by creating a \emph{source} to read from the input file @fileIn@.
The @source@ function constructs a process network which, at runtime, reads from a source.
The @input@ binding in this case refers to the abstract name of the stream in the process network, rather than the runtime values of the stream.
This is an important distinction, as expressions operating over runtime values need to be quasiquoted in order to delay them from compile-time to run-time.
Thus, the argument to @source@ describing how to read from the input source (read from a file using @sourceOfFile@) must be surrounded in a (typed) Template Haskell quasiquote.

Now we take the values in @input@ and filter them to those starting with the string \lstinline/"Good"/.
Again considering the compile-time/run-time distinction, the fact that the @filter@ process uses @input@ as its input is known at compile-time, and is not quasiquoted; while the predicate, which depends on the run-time stream values, must be quasiquoted.
We call the output filtered stream @goods@.

Finally, we send the filtered output to a file, by creating a \emph{sink}.
The @sink@ function is the opposite of @source@, and just like @source@ it requires the description of how to sink (writing to a file using @sinkToFile@) to be quasiquoted.

\TODO{Show the process network and the processes.}
Soon enough, we shall return to this.


% \begin{lstlisting}
% applyTransactions :: FilePath -> FilePath -> IO ()
% applyTransactions fileIn fileOut =
%   $$(fuse $ do
%      cust  <- source [||sourceOfFile fileCust||]
%      txns  <- source [||sourceOfFile fileTxns||]
%      cust' <- map    [||parseCust            ||] cust
%      txns' <- map    [||parseTxns            ||] txns
% 
%      (newCust, invalid) <- groupLeft [||applyTxn||] cust' txns'
% 
%      sink newCust [||sinkToFile fileOutCust   ||]
%      sink invalid [||sinkToFile fileOutInvalid||])
% \end{lstlisting}

% We start with the Template Haskell splice \lstinline/$$(fuse ...)/. In the code it is blue. 
% It has the following type.
% \begin{lstlisting}
% fuse :: Network () -> Q (TExp (IO ()))
% \end{lstlisting}
% That is, it takes a process network and returns the expression for the underlying @IO@ computation.
% The process network @Network ()@ is a monad as well.
% 
% The process network first constructs a source that reads from a file.
% \begin{lstlisting}
% source :: Q (TExp (Source a))                -> Network (Channel a)
% filter :: Q (TExp (a -> Bool))  -> Channel a -> Network (Channel a)
% map    :: Q (TExp (a -> b))     -> Channel a -> Network (Channel b)
% sink   :: Q (TExp (Sink a))     -> Channel a -> Network (Channel a)
% \end{lstlisting}
% The @source@ function takes a quasiquoted expression of how to construct the source at runtime.
% 
% Now show the generated code.
% 



\section{All this boxing and unboxing}
In Haskell, most values are boxed by default \citep{jones1991unboxed}.
Boxed values are stored as pointers to heap objects, which can in turn reference other boxed or unboxed values.
Boxed values are useful for implementing parametric polymorphism because they give a uniform representation to all the different types.
A list which is polymorphic in its element type can use a pointer to refer to its values regardless of the actual element type.

The problem with boxed values is that they require an extra allocation per object, as well as a pointer indirection for each access.
This can cause performance issues in tight loops, particularly because most objects are immutable.

Consider the following function, which loops over an array to compute its sum.
It starts by calling the local function @loop@ with the initial loop index, and the initial sum.
The definition of @loop@ checks if it has reached the end of the array, and if so returns the sum; otherwise it increments the running sum and proceeds to the next index.

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop 0 0
 where
  loop index running_sum
   | index == length vector
   = running_sum
   | otherwise
   = let value = vector ! index
     in loop (index + 1) (running_sum + value)
\end{lstlisting}

It is not clear from the program source alone, but the loop index and the running sum are both boxed values, because their type (@Int@) is boxed.
If this were compiled naively it would be rather disastrous for performance, as in order to process each element of the array, it must allocate two new boxed values.
Hilariously, all of these new boxed values except the very last iteration are used once by the next iteration and then thrown away.
While the garbage collector is tuned for small, short-lived objects, it is better to not introduce the garbage in the first place.

Removing boxing is not a novel thing, and there are many ways to do this.
The point to make is not that this is an interesting thing, just that we must know how it works in order to generate good code that fits it.
Boxed machine-word integers are represented by the following type, which defines @Int@ with a single constructor @I#@, taking an unboxed integer @Int#@. By convention, unboxed values and constructors that use them are named with the @#@ suffix.

\begin{lstlisting}
data Int = I# Int#
\end{lstlisting}

Now we know how machine-word integers are represented, we can look at an explicitly boxed version of @sum@.
This is still using boxed integers, but all arithmetic is explicitly unboxing and reboxing.
Unboxed literals are written as @0#@ or @1#@.
Unboxed arithmetic are written as @+#@ or @==#@, and @!#@ for unboxed indexing.
With explicit boxing, it should now be visible that the recursive call to @loop@ constructs new boxed integers.

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop (I# 0#) (I# 0#)
 where
  loop (I# index) (I# running_sum)
   | index ==# length vector
   = I# running_sum
   | otherwise
   = let value = vector !# index
     in loop (I# (index +# 1#)) (I# (running_sum +# value))
\end{lstlisting}

Constructor specialisation \cite{peyton2007call} is a loop optimisation that can remove these boxed arguments to recursive calls.
It looks at the constructors to recursive calls, and counts which ones are scrutinised or unwrapped at the start of the function definition.
In this case, @loop@ is first (and later, as well) called with the constructors @I#@ for both arguments, and both arguments are scrutinised.
So it creates a specialised version of @loop@ where both arguments are @I#@ constructors.
This specialised version is the same as the original, except the arguments are known to be @I#@ constructors, which means the pattern-matching on the arguments can be simplified away.
We will call this specialised version @loop'I#'I#@.
Then everywhere that @loop@ is called with @I#@ constructors, it will be replaced with a call to @loop'I#'I#@.
So any function call that looks like (@loop (I# x) (I# y)@) is replaced by a call to our new function (@loop'I#'I# x y@).

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop'I#'I# 0# 0#
 where
  loop'I#'I# index running_sum
   | index ==# length vector
   = I# running_sum
   | otherwise
   = let value = vector !# index
     in loop'I#'I# (index +# 1#) (running_sum +# value)
\end{lstlisting}

Constructor specialisation has removed all the boxing except for the final return value, which is only constructed once anyway.
In this example, the original @loop@ function was no longer called, so it was able to be removed entirely.
It is not always the case that the original function can be removed, and constructor specialisation can duplicate the code many times: once for each combination of constructors.
This can cause quite a lot of copies of the original function, which can cause large intermediate programs that do not fit in memory.
To alleviate this, GHC implements some heuristics to limit the number of duplicates created, as well as only creating specialisations if the original function is not too large.
This makes sense for general purpose code, but for tight loops where we expect most of our runtime to be, we really want to be sure that all specialisations are created.
For tight loops, we want to \emph{force} constructor specialisation to occur as much as possible.
This is achieved by annotating the function to be specialised with the special constructor @SPEC@.
Going back to the original @sum@ function, if we want to force constructor specialisation on @loop@, we can do this by adding the @SPEC@ to the function binding as well as all calls to it:

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop SPEC 0 0
 where
  loop SPEC index running_sum
   | index == length vector
   = running_sum
   | otherwise
   = let value = vector ! index
     in loop SPEC (index + 1) (running_sum + value)
\end{lstlisting}

\subsection{Mutable references}
\label{ss:extraction:mutablerefs}

We have seen that GHC is able to eliminate boxing from function arguments, and we will take advantage of this during code generation.
We will make use of @SPEC@ to force constructor specialisation, to ensure as much can be unboxed as possible.
Sadly, mutable references are stored boxed, and an analogous constructor specialisation transform does not exist for mutable references.
This means that in order to get unboxed values, we must structure our generated code to pass values via function arguments instead of mutable references.

Unboxed mutable references do exist, but are unsuitable because they can \emph{only} store unboxed values.
Recursive types such as linked lists cannot be stored in unboxed references.
We desire an unboxed representation when possible, and boxed representation when necessary.

It may be surprising to users of other languages that we should move away from using mutable references in favour of function arguments.
Indeed, \citet{biboudis2017expressive} describes the \emph{opposite} transform when implementing stream fusion in MetaOCaml.
So this will not necessarily map to other languages, but it is true in the particular case of GHC.

In Data Flow Fusion \cite{lippmeier2013data} there is a transform called \emph{loop winding}, which converts mutable references to function arguments.
The motivation here is that GHC does not track aliasing information of arrays stored in mutable references, but does track it for arrays as function arguments.

\subsection{Extended constructor specialisation}

Constructor specialisation is not limited to boxing and unboxing, but works for arbitrary constructors, including types with multiple constructors such as (@Maybe a@) or (@Either a b@).
It even works for recursive types such as lists, which could produce an an infinite number of specialisations.
Constructor specialisation must be careful to limit the specialisations to a finite number of \emph{useful} ones.

Information about the initial state can be very helpful in finding the most specific call patterns.
In the following example, @go@ is first called with the call pattern (@go (Just _) (Just _)@).
Using this as the `seed' from which we start exploring, we can see that the initial call pattern proceeds to the next call pattern (@go Nothing (Just _)@), followed by a call to (@go Nothing Nothing@).
If we were to look at the body of @go@ without this initial seed, however, we would find the call patterns (@go Nothing _@) and (@go _ Nothing@).
These call patterns from the unseeded body are less specific than the call patterns for the seed, which means using them would not allow the second argument to be specialised away.
By starting from the initial seed, the extra information about the initial state can be propagated to the other states.

\begin{lstlisting}
initial = go (Just 1) (Just 2)
 where
  go (Just _) b       = go Nothing b
  go a       (Just _) = go a       Nothing
  go Nothing Nothing  = 0
\end{lstlisting}

Not all specialisations are useful.
To limit compilation time, memory usage and code blowup, it is important to limit the specialisations to those which will be used.
That is, \emph{only} those which are reachable from the initial state.
In the following example, the initial state is the call pattern (@go (Left _) (Right _)@).
At each step, the arguments are flipped, so from the initial state the next reachable call pattern is (@go (Right _) (Left _)@).
From here, we can get back to the original state.
This means in total there are only two reachable call patterns.

However, if we look at the body alone without the seed, the first two call patterns are (@go _ (Left _)@) and (@go _ (Right _)@).
From here, more call patterns can be found: (@go _ (Left _)@) calls (@go (Left _) (Left a)@) and (@go (Left _) (Right _)@).
Similarly, there are two call patterns reachable from @(go _ (Right _)@), and these are distinct from the two already seen.
In this way, starting from the initial state means we do not have to generate all the possible specialisations.

\begin{lstlisting}
reachable = go (Left 1) (Right 2)
 where
  go (Left  a) b = go b (Left  a)
  go (Right a) b = go b (Right a)
\end{lstlisting}

\TODO{diagrams}

Using the initial calls as the seed is important.
This \emph{was} implemented, but it only occurred for locally bound functions, not for top-level bindings.
The problem is that even though our examples were locally bound functions, other transforms such as let-floating occur before constructor specialisation, which means locally bound functions can be `floated' up to top-level bindings, where seeding does not work.
The other issue is that top-level bound functions can be exported; if functions are exported, we cannot know their initial call pattern, as they may be called from other modules.
So for exported top-level functions, we must seed the call-patterns using all initial calls in the current module, as well as those in the body.
For non-exported top-level functions, we can be sure that the initial state is in the current module, and so use any initial calls outside of the body as the seed.

When arguments are of recursive types, there can be an infinite number of reachable call patterns.
Suppose we wish to reverse a linked list.
We can write this using a helper function, which takes the list that is reversed so far, as well as the list to reverse.

\begin{lstlisting}
reverse :: [Int] -> [Int]
reverse xs0 = go [] xs0
 where
  go zs []     = zs
  go zs (x:xs) = go (x:zs) xs
\end{lstlisting}

The helper function @go@ could be specialised an infinite number of times, but this would lead to non-terminating compilation.
First, the call to @go@ is seeded with the call pattern (@go [] _@).
Then, at every step in the evaluation, a list constructor is moved from the second argument to the first, resulting in the infinite chain of call patterns, (@go [_] _@), (@go [_, _] _@), and so on.
Usually, these specialisations would not be produced because they do not reduce allocation.
However, in the original implementation, when @SPEC@ is used to force constructor specialisation, an infinite number of specialisations were produced, and the compiler did not terminate.
We implemented Roman Leshchinskiy's suggestion to fix this by setting a limit on how many times recursive types can be specialised, even when forcing constructor specialisation.

\section{Sources and Sinks}
In order to write meaningful streaming computations, we need to interact with the outside world.
The processes in our process networks are pure and have no way of interacting with the world: they simply shuffle data along channels.
At the start of the process network, for the inputs, we use \emph{sources} to pull from the outside, such as reading from a file.
At the end of the process network, for the outputs, we use \emph{sinks} to push to the outside, such as writing to a file.

These sources and sinks are really the pull and push streams we have seen before \REFTODO{background}, but in this case we do not need to implement combinators over them; such plumbing will be expressed as processes in the process network.
Sources and sinks are in many ways opposites of each other, but they also share many similarities, so let us refer to them collectively as \emph{endpoints}.

Endpoints need to encapsulate some internal state: for example writing to a file requires a filehandle, and perhaps a buffer to fill before writing, to amortise the cost of the system call.
As explained previously (\autoref{ss:extraction:mutablerefs}), using mutable references for this internal state would lead to poor performance due to boxing.
We need to use the same approach of passing this state as function arguments so they can be unboxed by constructor specialisation.
This is a tad more complicated than it sounds, because the each endpoint requires a different type of state: reading from a file requires a filehandle, while reading from an in-memory array requires the array and the current index.
On the other hand, this state type is an internal thing and should not be exposed to the user, which rules out adding it as a type parameter on the endpoint.
Just because we need to pass the state around as function arguments should not change the external interface.

In order to `wrap up' the internal state type, so only the endpoint itself can inspect the internal state, while the user can only hold on to the abstract state and pass it to the endpoint, we use existentially quantified types.
This is similar to how existential types are used in Stream Fusion \cite{coutts2007stream}, to hide the internal state of a pull stream.

We say that each endpoint has an internal state type, and only it knows what the type is.
We provide some operations with the state: a way to construct an initial state, for example opening the file and returning the handle; a pull or push function which takes the state and returns a new state; and a close function for when we have finished reading from or writing to the endpoint.

\subsection{Sources}

We define sources in Haskell with the following datatype (@Source a@), where the type parameter @a@ is the type of values to be pulled.
The internal state type is bound to @s@, and we define a record with three fields.
The first field, @sourceInit@, contains an effectful computation which returns the initial state.
The second field, @sourcePull@, is a function which takes the current state and returns a pair containing the pulled value, and the updated state.
The pulled value is wrapped in a @Maybe@, because streams are finite: @Nothing@ means the end of the stream, and (@Just v@) means the value @v@.
The third and final field, @sourceDone@, is a function which takes the current state and closes the stream.

\begin{lstlisting}[mathescape=true]
data Source a
 = $\exists$s. Source
 { sourceInit ::      IO s
 , sourcePull :: s -> IO (Maybe a, s)
 , sourceDone :: s -> IO ()
 }
\end{lstlisting}

Streams end only once, and after pulling a @Nothing@, the source should not be pulled on again.
The source should not be pulled again after it is closed, but these invariants are not checked.
The state must be used linearly: after passing a state to @sourcePull@, a new state is returned, and the old state must not be used again.
This linearity constraint also enforces that @sourcePull@ cannot be called after @sourceDone@, since @sourceDone@ consumes the old state but does not produce a new state.

We can define a @Source@ that reads lines of text from a file.
Here the internal state is simply a filehandle.
To initialise the source, we open the file in reading mode, with the @openFile@ function.
When the source is done, we close the file handle with @hClose@.
To pull from the source, we define a helper function @pull@, which takes the filehandle as an argument.
The @pull@ function checks whether the end of the file has been reached (@hIsEof@).
If the end of the file, it returns @Nothing@, along with the original filehandle.
Otherwise, it reads a line from the handle (@hGetLine@) and wraps the line in a @Just@ constructor.

\begin{lstlisting}
sourceOfFile :: FilePath -> Source String
sourceOfFile filepath
  = Source
  { sourceInit = openFile ReadMode filepath
  , sourcePull = pull
  , sourceDone = hClose }
 where
  pull handle = do
    eof <- hIsEof
    case eof of
     True  -> return (Nothing, handle)
     False -> do
      line <- hGetLine handle
      return (Just line, handle)
\end{lstlisting}

For the sake of example, this is a simplified version.
Certainly, this could be improved in terms of error handling: what if the file does not exist; and performance: reading a single line at a time will not give the best performance.

\subsection{Sinks}

We define sinks in Haskell very similar to sources, above.
The datatype (@Sink a@) represents a sink which accepts values pushed into it.
Again, the internal state type is bound to the existential type @s@, and we define a record with three fields.
The first and third fields are initialisation (@sinkInit@) and closing (@sinkDone@), and are the same as for sources.
The second field, @sinkPush@, takes the current state and the value to push, and returns the new state.
Unlike with (@Source a@) which pulls (@Maybe a@), we push a value of @a@ without the @Maybe@.
This is because for push streams we signal the end of the stream with @sinkDone@.

\begin{lstlisting}[mathescape=true]
data Sink a
 = $\exists$s. Sink
 { sinkInit ::           IO s
 , sinkPush :: s -> a -> IO s
 , sinkDone :: s ->      IO ()
 }
\end{lstlisting}

As with sources, sinks also require that the states are used linearly.
This precludes pushing to a closed stream.

Let us define a @Sink@ that writes lines of text to a file.
As with the @Source@ that reads, the internal state is a filehandle.
Initialisation opens the file in write mode, and when we are done we close the file.
To push a value, the helper function @push@ takes the filehandle and the line and use @hPutStrLn@ to write it, then returns the filehandle.

\begin{lstlisting}
sinkToFile :: FilePath -> Sink String
sinkToFile filepath
  = Sink
  { sinkInit = openFile WriteMode filepath
  , sinkPush = push
  , sinkDone = hClose }
 where
  pull handle line = do
    hPutStrLn handle line
    return handle
\end{lstlisting}

\TODO{Spend a lot of time talking about why we need the state, but the examples only use the same filehandle. Need an example, eg to/from Vector or file IO with buffering, which uses the state.}

\section{Process heap}
While our original process formulation \REFTODO{processes and fusion} used a global heap of mutable variables, in our implementation we wish to take advantage of constructor specialisation for unboxing.
Instead of a global heap, we will treat each label as a separate function definition that explicitly passes variables as function arguments.


\section{Single process example}
Let us perform code generation for a single-process example.
There is no fusion in this example, but it suffices to show the code generation part; a program with multiple processes fused together would simply end up as a more complex process.

In this example, we create a source to read from the file @fpIn@, then @map@ to take the @tail@ of each line, to remove the first character, and finally create a sink to write each tailed line to the file @fpOut@.

\begin{lstlisting}
tailsIO :: FilePath -> FilePath -> IO ()
tailsIO fpIn fpOut =
 $$(fuse $ do
  lines <- source [||sourceOfFile fpIn||]
  tails <- map    [||\v -> tail v     ||]
  sink tails      [||sinkToFile fpOut ||])
\end{lstlisting}

When we print the intermediate process network before code generation, we get the following.
Sources and sinks are specified for @lines@ and @tails@, as in the code above.
The map process starts at label @l0@, which pulls from the source @lines@.
If the pull succeeds, it proceeds to @l1@, which then pushes to the sink @tails@.
If the pull fails, it proceeds to @l3@, which closes the output sink before finishing processing at @l4@.

\begin{lstlisting}
NetworkGraph
  sources: lines = sourceOfFile fpIn
  sinks:   tails = sinkOfFile   fpOut
  processes:
    Process "map"
      inputs:  lines
      outputs: tails
      initial: l0
      instructions:
        l0   = Pull  lines l1       l3
        l1 v = Push  tails (tail v) l2
        l2   = Drop  lines l0
        l3   = Close tails l4
        l4   = Done
\end{lstlisting}

Let us now go through the generated code, piece by piece.
We start by unpacking the @lines@ source (@sourceOfFile fpIn@) and getting its initialisation function (@lines'init@), its pull function (@lines'pull@), and its close function (@lines'done@).
We need to use a case analysis here, rather than the accessor functions @sourceInit@ etc, because one cannot use accessor functions on existential types.
We perform the same unpacking for the @tails@ sink.

\begin{lstlisting}
tailsIO fpIn fpOut = do
  case sourceOfFile fpIn of
   Source lines'init lines'pull lines'done -> do
    case sinkOfFile fpOut of
     Sink tails'init tails'push tails'done -> do
\end{lstlisting}

\begin{lstlisting}
      let l0 !_ lines's tails's = do
            (v, lines's') <- lines'pull lines's
            case v of
             Nothing -> l3 SPEC lines's' tails's
             Just v' -> l1 SPEC lines's' tails's v'
\end{lstlisting}

\begin{lstlisting}
      let l1 !_ lines's tails's v = do
            tails's' <- tails'push tails's v
            l2 SPEC lines's tails's
\end{lstlisting}

\begin{lstlisting}

      let l2 !_ lines's tails's =
            l0 SPEC lines's tails's
\end{lstlisting}

\begin{lstlisting}

      let l3 !_ lines's tails's = do
            tails'done tails's
            l4 SPEC lines's
\end{lstlisting}

\begin{lstlisting}

      let l4 !_ lines's = do
            lines'done lines's
            return ()
\end{lstlisting}

\begin{lstlisting}
      lines's0 <- lines'init
      tails's0 <- tails'init
      l0 SPEC lines's0 tails's0
\end{lstlisting}

\section{Size hints}
\label{s:implementation:sizehints}
Talk about @vectorSizeIO@ and why it's useful.
Reference \autoref{s:Future:SizeInference} for how to infer this.

\section{Constructing processes}
\begin{lstlisting}
filter :: Q (TExp (a -> Bool))  -> Channel a -> Network (Channel a)
filter predicate as = do
 bs <- channel
 predicate' <- liftQ $ unTypeQ predicate
 process (Next (Label 0) [])
   [ (Label 0, InstructionBinding []
              (pull as (Next (Label 1) []) (Next (Label 3) []))
   , (Label 1, InstructionBinding [Var 0]
              (If [|$predicate' $(var 0)|]
                (Next (Label 2) [var 0])
                (Next (Label 3) [])))
   , (Label 2, InstructionBinding [Var 0]
              (push bs (var 0) (Next (Label 3) [])))
   , (Label 3, InstructionBinding []
              (drop as (Next Label 0)))

   , (Label 4, InstructionBinding []
              (Done))]

 return bs
\end{lstlisting}

\section{Types for constructing networks}

Untyped names
\begin{lstlisting}
data Label = Label Name
data Var = Var Name

data ChannelU = ChannelU Name
data Channel a = Channel { getChannel :: Channel }
\end{lstlisting}


\begin{lstlisting}
data NetworkGraph
 = NetworkGraph
 { networkGraphSources   :: Map ChannelU Exp
 , networkGraphSinks     :: Map ChannelU Exp
 , networkGraphProcesses :: [Process]
 }
\end{lstlisting}

\begin{lstlisting}
type Network = StateT Q NetworkGraph
\end{lstlisting}

\begin{lstlisting}
channel :: Network (Channel a)
channel = Channel <$> runQ freshName
\end{lstlisting}

\begin{lstlisting}
source :: Q (TExp (Source a)) -> Network (Channel a)
source source = do
 c  <- channel
 s' <- unTypeQ source
 tell graph { networkGraphSources = Map.singleton (getChannel c) s' }
 return c
\end{lstlisting}


\begin{lstlisting}
data Process
 = Process
 { processInputs       :: Set ChannelU
 , processOutputs      :: Set ChannelU
 , processInitial      :: Next
 , processInstructions :: Map Label InstructionBinding
 }
\end{lstlisting}

\begin{lstlisting}
data Next
 = Next
 { nextLabel    :: Label
 , nextAssigns  :: [Exp]
 }
\end{lstlisting}

\begin{lstlisting}
data InstructionBinding
 = Info
 { infoBindings     :: [Var]
 , infoInstruction  :: Instruction
 }
\end{lstlisting}

\begin{lstlisting}
data Instruction
 = I'Pull ChannelU Next Next
 | I'Push ChannelU Exp Next
 | I'Jump Next
 | I'Bool Exp Next Next
 | I'Drop ChannelU Next
 | I'CloseOutput Channel Next
 | I'Done
\end{lstlisting}

\section{Types for code generation}

\begin{lstlisting}
data FusedNetwork
 { fusedNetworkSources      :: Map ChannelU Exp
 , fusedNetworkSinks        :: Map ChannelU Exp
 , fusedNetworkInstructions :: Map Label InstructionBinding
 , fusedNetworkInitial      :: Next
 }
\end{lstlisting}

\begin{lstlisting}
genNetwork1 :: NetworkGraph -> Q Exp
\end{lstlisting}


\begin{lstlisting}
data SourceBound
 { sourceBoundInit :: Exp
 , sourceBoundPull :: Exp -> Exp
 , sourceBoundDone :: Exp -> Exp
 }
\end{lstlisting}

\begin{lstlisting}
data SinkBound
 { sinkBoundInit :: Exp
 , sinkBoundPush :: Exp -> Exp -> Exp
 , sinkBoundDone :: Exp -> Exp
 }
\end{lstlisting}

\begin{lstlisting}
data EnvironmentGlobal
 { envGlobalSources :: Map ChannelU SourceBound
 , envGlobalSinks   :: Map ChannelU SinkBound
 }
\end{lstlisting}

\begin{lstlisting}
data EnvironmentLocal
 { envLocalSources :: Map ChannelU Exp
 , envLocalSinks   :: Map ChannelU Exp
 }
\end{lstlisting}

\begin{lstlisting}
bindStates :: EnvironmentGlobal -> Q (EnvLocal, [Bind])
updateState :: EnvironmentLocal -> Channel -> Exp -> EnvironmentLocal
stateArguments :: EnvironmentLocal -> [Exp]
initialiseAll :: EnvironmentGlobal -> (EnvironmentLocal -> Exp) -> Q Exp
openEnvironment :: FusedNetwork -> (EnvironmentGlobal -> Q Exp) -> Q Exp
\end{lstlisting}



For a @Source@ expression (@s :: Exp (TExp (Source a))@):
\begin{lstlisting}
bindSource :: Exp -> (SourceBound -> Q Exp) -> Q Exp
bindSource s with =
 case $s of
  Source init pull done -> do
   s0 <- init
   $(with (SourceBind
         { sourceBoundState = [|s0|]
         , sourceBoundPull  = \s -> [|pull $s|]
         , sourceBoundDone  = \s -> [|done $s|]
         }))
\end{lstlisting}

\begin{lstlisting}
genNext :: EnvironmentBound -> Next -> Q Exp
genNext env (Next label expressions) =
  foldl AppE (expOfLabel label) arguments
 where
  arguments = spec : argumentsOfEnvironment env ++ expressions
  spec = ConE 'SPEC
\end{lstlisting}

\begin{lstlisting}
genInstruction :: EnvironmentBound -> Instruction -> Q Exp
genInstruction env i = case i of
  Pull c n n' -> [|do
    (v,s') <- $(sourcePullOfEnvironment env)
    case v of
     Nothing -> $(genNext n' (updateEnvironmentState e c [|s'|]))
     Just v' -> $(genNext n (updateEnvironmentState e c [|s'|])) v'
    |]
\end{lstlisting}

\begin{lstlisting}
genInstructionBinding :: EnvironmentGlobal -> InstructionBinding -> Q Exp
genInstructionBinding global (InstructionBinding vars instr) = do
  local <- envLocalOfGlobal global
  code  <- genInstruction global local instr
  return $ LamE spec $ lams (bindingsOfLocal local) $ lams (fmap bindOfVar vars) code
 where
  lams = foldl LamE
  spec = ConP 'SPEC
\end{lstlisting}

\section{Other transforms}
\subsection{Cull outputs}
When fusing a producer with a consumer, the result process still \emph{produces}, and writes values to the output channel, as well as copying them into variables for the consumer to use.
This is necessary because there may be multiple consumers reading from the one producer, and these other consumers may not be fused in yet.
The producer needs to keep pushing to its output, until all consumers have been fused in.
Once all consumers of the output channel have been fused though, (if the output channel is not used as a sink), it is no longer required.
The \emph{cull outputs} transform finds outputs that are not used by the rest of the networks.
Any pushes to these channels can be replaced with jumps to where the push would have jumped to.
This is a very simple transform to remove some extraneous stuff.

% \subsection{Insert dups}
% Let's pretend that the fusion process handles this case, as is the proof and paper version

\subsection{Fusing a network}
The fusion algorithm described in \REFTODO{processes and fusion} fuses \emph{pairs of processes} at a time, but network generally contain more than two processes.
To fuse a whole network, we start by constructing a dependency graph of the process network.
Then we find all the terminal processes: those with no successors, only predecessors.
For each terminal process, fuse with each predecessor, and recurse.
This is like a bottom-up transform.
The rationale for starting from the bottom is that consumers force a particular access pattern on their producers.

\TODO{expand}

\subsection{Fusing a pair of processes}
Fusing a pair of processes is slightly different because variables are passed as function arguments, instead of a global heap.
It also needs to take into account variable bindings per label, because of the locally-bound variables.
For a pair of labels, the variables is the union of variables in the original processes, as well as any \emph{new} channel buffers which need to be bound.
These are the ones which have an input state of \emph{have}.

\subsection{Minimise}
Minimise performs simple skip/jump contraction.
If label @l@ is @jump m[x=e]@, anywhere that jumps to @l@, we want to replace with @m[x=e]@.
This is complicated a bit by the variable updates attached to each label.
So a label @l[u]@, which in turn jumps to @l'[u']@, must in fact be replaced with @l'[u'[u]]@, because both substitutions must be applied.

However, blindly joining the substitutions can duplicate work.
If we have the following label update: (@l[x = expensive 2]@) where @expensive@ is some costly operation, and we also have @l'[a=x, b=x]@, we do not want to duplicate the call to @expensive@ as @l'[a = expensive 2, b = expensive 2]@.
So, as a conservative approach, we only apply minimisation when all substitutions are only simple variables.

One must also beware of infinite loops, as in the case of @l = jump l[u]@.
Now, when looking up the `next' label for @l@, we keep a set of labels already seen.
If we encounter the same label twice, it is time to stop unfolding the definition.

This transform also performs a simple form of dead code removal, by removing unreachable labels.

Why do we do this?
The truth is that GHC supports these operations on recursive loops already.
It is unlikely that we are gaining much by performing such simple optimisations.
However, there are two main reasons to do this.
First, we can perform these simplifications between every pair of processes fused.
By making the input to fusion smaller we can produce smaller programs to being with, and simplify as we go, rather than relying on a monolithic simplifcation at the end.
Secondly, it makes the intermediate code smaller, making it easier to read and debug in the case that fusion fails.



% -----------------------------------------------------------------------------
\section{Optimisation}
\label{s:Optimisation}
\TODO{elsewhere}
After we have fused two processes together, it may be possible to simplify the result before fusing in a third. Consider the result of fusing @group@ and @merge@ which we saw back in Figure~\ref{fig:Process:Fused}. At labels @F1@ and @F2@ are two consecutive @jump@ instructions.
The update expressions attached to these instructions are also non-interfering, which means we can safely combine these instructions into a single @jump@.
In general, we prefer to have @jump@ instructions from separate processes scheduled into consecutive groups, rather than spread out through the result code.
The (PreferJump) clauses of Figure~\ref{fig:Fusion:Def:StepPair} implement a heuristic that causes jump instructions to be scheduled before all others, so they tend to end up in these groups.

Other @jump@ instructions like the one at @F5@ have no associated update expressions, and thus can be eliminated completely. Another simple optimization is to perform constant propagation, which in this case would allow us to eliminate the first @case@ instruction. 

Minimising the number of states in an intermediate process has the follow-on effect that the final fused result also has fewer states. Provided we do not change the order of instructions that require synchronization with other processes (@pull@, @push@ or @drop@), the fusibility of the overall process network will not be affected.

Another optimization is to notice that in some cases, when a heap variable is updated it is always assigned the value of another variable. In Fig.\ref{fig:Process:Fused}, the @v@ and @x1@ variables are only ever assigned the value of @b1@, and @b1@ itself is only ever loaded via a @pull@ instruction. Remember from \S\ref{s:Fusion:FusingPulls} that the variable @b1@ is the stream buffer variable. Values pulled from stream @sIn1@ are first stored in @b1@ before being copied to @v@ and @x1@. When the two processes to be fused share a common input stream, use of stream buffer variable allows one process to continue using the value that was last pulled from the stream, while the other moves onto the next one. 


% When the two processes are able to accept the next variable from the stream at the same time, there is no need for the separate stream buffer variable. This is the case in Figure~\ref{fig:Process:Fused}, and we can perform a copy-propagation optimisation, replacing all occurrences of @v@ and @x1@ with the single variable @b1@. To increase the chance that we can perform copy-propagation, we need both processess to want to pull from the same stream at the same time. Moving the @drop@ instruction for a particular stream as late as possible prevents a @pull@ instruction from a second process being scheduled in too early.
% To increase the chance that we can perform this above copy-propagation, we need both processess to want to pull from the same stream at the same time. In the definition of a particular process, moving the @drop@ instruction for a particular stream as late as possible prevents a @pull@ instruction from a second process being scheduled in too early. In general, the @drop@ for a particlar stream should be placed just before a @pull@ from the same stream. 

\section{What's the deal with drop anyway}
\TODO{elsewhere}

The purpose of the @drop@ instructions is to keep two consumers more closely synchronised.
One consumer cannot start processing the next element until the other has finished processing the current element.
Drop is not necessary for correctness, or even for ensuring boundedness of buffers: without it, the result program would still be correct, and one process could not `overtake' another by processing more than one element before the other one.

If we have two consumers @P@ and @Q@, both pulling from the same channel @C@, they both need to agree about when to pull from the channel.
Suppose that @P@ and @Q@ both pull from @C@, then push to their own output channel, @CP@ and @CQ@ respectively, drop their input, then loop back to pull again.
If we are executing @P@, and it pulls from @C@, it can keep executing and push to its output channel @CP@, then drop the input.
Now, @P@ has dropped its input, but cannot pull again yet, because there is no new value available to pull.
There is no new value available to pull because the producer cannot \emph{push} to @C@ yet, because @Q@ has not consumed its input.
Now, @Q@ can run, and pulls from its input.
This transitions its input from \emph{pending} to \emph{have}, which means the producer still cannot push yet, until @Q@ drops its input.
In this way, the drop allows the the producer to push only once all consumers have dealt with their input.
Without drops, @P@ would be able to process the next element before @Q@ had finished the previous one.

\begin{lstlisting}
P = process
  P1: c <- pull C
  P2: push CP c
  P3: drop C
  P4: jump P1

Q = process
  Q1: c <- pull C
  Q2: push CQ c
  Q3: drop C
  Q4: jump Q1
\end{lstlisting}

By synchronising the two processes together, when we fuse we will only have one copy of the code that pulls each element.
Because @P@ can only start pulling again by the time @Q@ has dropped, this means @P@ and @Q@ must both be trying to pull at the same time, which means we can reuse the same instructions generated from the previous time they both pulled.
The example @PQ_drop@ shows the fused process with drop instructions.
Note that there is only one copy of each input process' code.
On the other hand, @PQ_no_drop@ shows the fused process without drop instructions.
Here, there are two copies of pushing to @CP@, though the main loop only executes one per iteration: pushing the current element to @CP@, and the previous element to @CQ@.
As the processes get larger, and more processes are fused together, the issue of duplicating code becomes more serious.
There are two parts to this: first, we need to hold the entire process in memory in order to generate its code.
Secondly, as the generated assembly code gets larger, it is less likely to fit into the processor's cache.
Smaller code is generally better for performance.
Having two copies of the push to @CP@ means that any consumers of @CP@ must in turn have their code duplicated, with the pull instructions from @CP@ copied into both sites of the pushes.

\TODO{diagrams}
\begin{lstlisting}
PQ_drop = process
  P1Q1: c_buf <- pull C
        c_p    = c_buf
        c_q    = c_buf
        push CP c_p
        push CQ c_q
        drop C
        jump P1Q1

PQ_no_drop = process
  P1Q1:  c_buf <- pull C
         c_p    = c_buf
         c_q    = c_buf
         push CP c_p
  P2Q2:  c_buf <- pull c
         c_p    = c_buf
         push CP c_p
         push CQ c_q
         c_q    = c_buf
         jump P2Q2
\end{lstlisting}



