\chapter{Implementation and code generation}
\label{chapter:process:implementation}


Folderol\footnote{\url{https://github.com/amosr/folderol}} is a Haskell implementation of Machine Fusion \REFTODO{fusion}.
Machine Fusion uses the topology of the entire process network to perform fusion.
When fusing one producer with multiple consumers, the fusion algorithm must coordinate between all the consumers to reach consensus on when to pull the next value.
This coordination between all consumers means the fusion algorithm requires global knowledge of the process network.
In contrast, shortcut fusion systems \REFTODO{shortcut} use rewrite rules to remove intermediate buffers and require only local knowledge, but cannot coordinate between multiple consumers.
In cases where shortcut fusion cannot fuse it fails silently, leaving the programmer unaware of the failure.
This silence is also due to the local nature of rewrite rules: if we wish to know whether all the processes have been fused, we need to know about all the processes.
To fuse the entire process network, as well as to inform the programmer when fusion fails, Folderol uses Template Haskell, a form of metaprogramming.

This chapter looks at the implementation of Folderol, in particular how to generate efficient code for a fused process.
Generating code for a single process is fairly straightforward in itself: the process language is a simple imperative language, and once the process network has been fused into a single process there is no longer any need for threading or inter-process communication.
However, code generation needs to be tailored specifically to take advantage of the optimisations in the target compiler, which in this case is the Glasgow Haskell Compiler (GHC).
Haskell does support imperative constructs like mutable references, but being a functional language, the optimisations in GHC are more geared towards functional programs.
Mutable references are not optimised particularly well.
If we wish to generate efficient code, we must --- perhaps surprisingly --- avoid mutable references, and generate code closer to what the compiler expects.

\section{Template Haskell}
Template Haskell is a metaprogramming extension for Haskell \cite{sheard2002template} which provides a limited form of staged computation, where the only `stages' are compile-time and runtime.
There are two variants of Template Haskell: untyped and typed.
Untyped Template Haskell does not enforce well-typedness of the generated program until after the program is generated, which means it is quite easy to generate ill-typed programs.
Even though we can generate ill-typed programs, overall soundness is preserved because the generated program is still typechecked and compiled at compile-time.
On the other hand, typed Template Haskell enforces well-typedness of the generated program by lifting types in the generated program up to the meta-level as types in the generator program.

We use untyped Template Haskell for code generation.
Performing typed code generation is equivalent to proving that the result program is well-typed, whereas untyped code generation requires no such proof.
This makes the untyped version easier to implement.
While typed code generation is no doubt possible, it is an interesting research problem on its own \CITE{Accelerate}.

The problem with using untyped code generation is that if there are type errors they will not be found until quite late, once the generated code has been spliced into the main program.
This means that when the programmer sees the type error, the error location will be in the generated code.
That is, the error is shown in code the program wrote, rather than code the programmer wrote.
This complicates finding the underlying cause of the error and fixing the problem.
Type safety is a big problem when we wish to provide a library for other programmers: it is unacceptable to require a user of the library to understand the internal workings of code generation to understand type errors.
Typed Template Haskell allows us to provide a type-safe interface to users of the library.

\subsection{Untyped expressions}
\TODO{find a better example. This is placeholder.}

Template Haskell extends regular Haskell with two syntactic constructs: quasiquoting and splicing, for moving between runtime and compile-time stages.
Quasiquoting converts a runtime expression to a compile-time value, while splicing performs the opposite conversion.
For example, we can quasiquote some arithmetic using the syntax \lstinline/[|1+2|]/, which produces the corresponding abstract syntax tree for an infix operator with two integer arguments:

\begin{lstlisting}
InfixE (Just (LitE (IntegerL 1))) (VarE GHC.Num.+) (Just (LitE (IntegerL 2)))
\end{lstlisting}

Quasiquoting is a purely syntactic convenience, as any quasiquoted expressions could be produced using the abstract syntax tree constructors directly.
That said, it is a particularly useful convenience which we will use extensively.

Splicing takes a compile-time abstract syntax tree and converts it to an expression to be evaluated at runtime.
For example, splicing back the quasiquoted arithmetic using the syntax \lstinline/$([|1+2|])/, will evaluate (@1+2@) at runtime.

Splicing and quasiquoting operate in the @Q@ monad.
The @Q@ monad gives a fresh name supply, so that when names are bound inside expressions they can be given unique names.
These fresh names ensure that newly bound names will not interfere with existing bindings.

Now let us look at a concrete example of how to use Template Haskell.
Suppose we wish to define a power function where the exponent is known at compile time, but the mantissa is not known until runtime.

We define @power@ as a function with the type \lstinline/Int ->  Q Exp/.
This means it takes an integer at compile-time, and produces a quoted expression in the quote monad.
This result expression will, once spliced in, have type \lstinline/Int ->  Int/.
Later we shall use typed Template Haskell to make this extra type information explicit, but for now we must perform typechecking in our heads.

\begin{lstlisting}
power :: Int -> Q Exp
power 0 = [|\m -> 1                      |]
power e = [|\m -> $(power (e - 1)) m * m |]
\end{lstlisting}

The @power@ function pattern-matches on the exponent.
When the exponent is zero, we enter quasiquoting mode and construct a function that always returns one.
When the exponent is non-zero, we again enter quasiquoting mode and construct a function, taking the mantissa as its argument.
Inside the quasiquote, we need to handle the recursive case for the one-smaller exponent (\lstinline/e - 1/), so we enter into splicing mode with \lstinline/$(power (e - 1))/ to compute the one-smaller power.
The splice for the one-smaller power returns a function, to which we apply the mantissa.
Finally, we multiply the one-smaller power of the mantissa by the mantissa itself.

We can now define a specialised @power@ function to compute the square.
We define this as a top-level binding which performs a splice, and inside that splice we call @power@ with the statically known argument @2@.
The type inside the splice is \lstinline/Q Exp/, and once splicing is complete the result is an expression of type \lstinline/Int ->  Int/.

\begin{lstlisting}
power2 :: Int -> Int
power2 = $(power 2)
\end{lstlisting}

The reason for this specialisation is to produce optimised code: by knowing the exponent at compile-time, we can perform the recursion once at compile-time rather than many times at runtime.
It is therefore worth inspecting the resulting code to check whether it is indeed optimal.
The compiler option (@-ddump-splices@) outputs the result splices, as follows.

\begin{lstlisting}
$(power 2) =
  \m0 -> (\m1 -> (\m2 -> 1) m1 * m1) m0 * m0
\end{lstlisting}

This is a roundabout way to multiply a number by itself, and there are a lot of opportunities to simplify that code.
While we expect the compiler to remove the extra lambdas by beta reduction, it would be even better to not introduce them in the first place.
If we do not introduce simplification opportunities in the first case, there is no uncertainty about whether the compiler will be able to remove them.
The problem is that @power@ introduces a lambda for each recursive step, while we only want one lambda at the top-level.
So let us fix this by defining a top-level function which introduces the lambda, and a recursive helper function to compute the power.

\begin{lstlisting}
powerS :: Int -> Q Exp
powerS e = [|\m -> $(powerS' e [|m|])|]
\end{lstlisting}

The top-level function @powerS@ introduces the lambda for the mantissa inside a quasiquote, then calls the helper function @powerS'@.
The exponent is a compile-time binding, while the mantissa is a runtime binding.
When the helper function is called at compile-time, the exponent can be passed as-is, while the mantissa must be quasiquoted to wrap the runtime binding into a compile-time expression.

The recursive helper function, @powerS'@, has type \lstinline/Int ->  Q Exp ->  Q Exp/.

\begin{lstlisting}
powerS' :: Int -> Q Exp -> Q Exp
powerS' 0 m = [|1                        |]
powerS' e m = [|$(powerS' (e-1) m) * $(m)|]
\end{lstlisting}

Like the original @power@ function, it pattern-matches on the exponent and in the recursive case multiplies by itself.
The difference is that the mantissa is bound as a compile-time expression with type (@Q Exp@) rather than inside the quasiquote, so it must be spliced when it is used.

The output for (@powerS 2@) is a lot simpler than that for (@power 2@).
Whereas before there was a lambda introduced and applied at each recursive step, now there is only a single lambda.

\begin{lstlisting}
$(powerS 2) =
  \m -> ((1 * m) * m)
\end{lstlisting}

There is still more we could do to improve the function: for example, (@1 * m@) could be replaced by @m@.
However, this is sufficient to show the core splicing and quoting ideas behind Template Haskell.
For more information on staging in general, \citet{rompf2010lightweight} takes this example further.

\subsection{Typed expressions}

% The problem with Template Haskell shown above is that there are no types attached to expressions.
% Quasiquoting a string \lstinline/[|"one"|]/ and quasiquoting an integer \lstinline/[|1|]/ both produce a value of the same type: @Q Exp@.
% We can then use these expressions to construct larger, completely untypable expressions; for example we could try to subtract a string from an integer, which should surely fail: \lstinline/[|1 - "one"|]/.

Typed Template Haskell extends the Template Haskell we have seen with typed expressions, typed splicing and typed quasiquoting.
The type of typed expressions, @TExp@, is annotated with a meta-level (compile-time) type argument denoting the object-level type of the expression.
Syntactically, typed quasiquotation uses two pipes, while typed splicing uses two dollar signs.
For example, using a typed quasiquote on a string will produce a @String@-typed expression: (\lstinline/[||"one"||] :: Q (TExp String)/).
Similarly, typed splicing eliminates the @Q@ monad and the @TExp@ wrapper, leaving only the result type: (\lstinline/$$([||"one"||]) :: String/).

Typed expressions are invaluable for providing a typesafe way to construct process networks.
We need to construct process networks at compile-time to fuse them at compile-time, while keeping the information required to execute them at runtime.
Consider the standard @filter@ function for lists, with type (\lstinline/(a ->  Bool) ->  [a] ->  [a]/).
In order to implement a process network version of @filter@, the function argument of type (\lstinline/(a ->  Bool/) must be converted to an expression, as it will be evaluated and applied at runtime.
Providing a process network @filter@ function which takes as an argument the typed expression (\lstinline/Q (TExp (a ->  Bool))/) instead of the untyped expression (\lstinline/Q Exp/) means that type errors can be caught early.

We provide an untyped core for the processes and networks, then build on top of this to provide a typed interface for constructing networks.
For this reason, we need to be able to convert from typed expressions to untyped expressions.
We can convert from a typed expression to an untyped expression using the @unTypeQ@ function, which has type (\lstinline/Q (TExp a) ->  Q Exp/).
This does not affect the underlying expression, it just throws away the meta-level type information.
The object-level type information remains, and once it is spliced in it will have the same type.

% Conversely, if one is very careful, one can convert an untyped expression to a typed one.
% This is an unsafe operation, because one can choose any type at all for the expression.
% The expression is not checked against the chosen type until it is spliced in.
% \begin{lstlisting}
% unsafeTExpCoerce :: Q Exp -> Q (TExp a)
% \end{lstlisting}
 
% By using untyped expressions for code generation, we do not lose any actual type safety, since the generated code will still end up being typechecked by the Haskell compiler.
% What we do lose are good error locations, but these errors will only occur if there are bugs in the code generator.
% Any type errors in the user of the library will be using the typesafe interface, with better error messages.

% Although it is possible to construct expressions with the wrong type, this generally requires explicitly unsafe operations.
% Most of the time, this is unlikely to occur by accident alone.

\section{Constructing a process network}
\label{s:extraction:grepGood}

The following example constructs a process network in Folderol.
In due course we will inspect its generated code, but first it is necessary to see how process networks are constructed.

\begin{lstlisting}
grepGood :: FilePath -> FilePath -> IO ()
grepGood fileIn fileOut =
  $$(fuse $ do
     input  <- source [||sourceOfFile fileIn ||]
     goods  <- filter [||isPrefixOf   "Good" ||] input
     sink      goods  [||sinkToFile   fileOut||])
\end{lstlisting}

This example reads input lines from a file, filters out all except those starting with the string \lstinline/"Good"/, and finally writes the results to file.
In a Unix-style environment, we would write this as ``@grep ^Good@''; the caret (@^@) meaning ``starting with''.

The function @grepGood@ takes two arguments of type @FilePath@ for the input and output filenames.
Inside the definition, the Template Haskell splice \lstinline/$$(fuse ...)/ takes the process network as an argument, fuses the processes together, and generates output code.
We are constructing the process network inside the Template Haskell splice at compile-time, while we wish to execute it at runtime.
The process network is the static representation of the computation, and must be statically known and finite.

Inside the process network we start by creating a \emph{source} to read from the input file @fileIn@.
The @source@ function creates an input stream in the process network which can be used by other processes.
The @input@ binding in this case refers to the abstract name of the stream in the process network, rather than the runtime values of the stream.
This is an important distinction, as expressions operating over runtime values need to be quasiquoted to delay them from compile-time to runtime.
Similarly, the input file @fileIn@ will not be known until runtime.
The choice of source does not affect fusion, and does not need to be known at compile time.
Since most sources depend on runtime values in some way, the entire source is delayed until runtime and must be quasiquoted.

Now we take the values in the @input@ stream and filter them to those starting with the string \lstinline/"Good"/.
Again considering the compile-time/runtime distinction, the fact that the @filter@ process uses the stream named @input@ as its input is known at compile-time, and is not quasiquoted; while the predicate, which depends on the runtime stream values, must be quasiquoted.
We call the output filtered stream @goods@.

Finally, we send the filtered output to a file, by creating a \emph{sink}.
The @sink@ function is the opposite of @source@, and just like @source@ it requires the description of how to sink (writing to a file using @sinkToFile@) to be quasiquoted.

\TODO{Show the process network and the processes.}
Soon enough, we shall return to this.


% \begin{lstlisting}
% applyTransactions :: FilePath -> FilePath -> IO ()
% applyTransactions fileIn fileOut =
%   $$(fuse $ do
%      cust  <- source [||sourceOfFile fileCust||]
%      txns  <- source [||sourceOfFile fileTxns||]
%      cust' <- map    [||parseCust            ||] cust
%      txns' <- map    [||parseTxns            ||] txns
% 
%      (newCust, invalid) <- groupLeft [||applyTxn||] cust' txns'
% 
%      sink newCust [||sinkToFile fileOutCust   ||]
%      sink invalid [||sinkToFile fileOutInvalid||])
% \end{lstlisting}

% We start with the Template Haskell splice \lstinline/$$(fuse ...)/. In the code it is blue. 
% It has the following type.
% \begin{lstlisting}
% fuse :: Network () -> Q (TExp (IO ()))
% \end{lstlisting}
% That is, it takes a process network and returns the expression for the underlying @IO@ computation.
% The process network @Network ()@ is a monad as well.
% 
% The process network first constructs a source that reads from a file.
% \begin{lstlisting}
% source :: Q (TExp (Source a))                -> Network (Channel a)
% filter :: Q (TExp (a -> Bool))  -> Channel a -> Network (Channel a)
% map    :: Q (TExp (a -> b))     -> Channel a -> Network (Channel b)
% sink   :: Q (TExp (Sink a))     -> Channel a -> Network (Channel a)
% \end{lstlisting}
% The @source@ function takes a quasiquoted expression of how to construct the source at runtime.
% 
% Now show the generated code.
% 



\section{All this boxing and unboxing}
\TODO{Reorganise: what does boxing have to do with fusion?}

In Haskell, most values are boxed by default \citep{jones1991unboxed}.
Boxed values are stored as pointers to heap objects, which can in turn reference other boxed or unboxed values.
Pointers have a uniform representation regardless of the type of the object they point to.
This uniform representation allows the same generated code for a polymorphic data structure or function to be used for any boxed type.
A list which is polymorphic in its element type can use the same pointer type to refer to its values regardless of the actual element type.

The problem with boxed values is that they require at least one allocation per object and a pointer indirection for each access.
Incrementing an unboxed integer stored in a register is a single instruction.
Incrementing a boxed integer requires more work.
First the value is read from memory into a register, where it is incremented.
In Haskell most heap objects are immutable; rather than updating the original heap object, a new object must be allocated.
Finally, the newly allocated object is filled in by copying the value from the register to memory.
This copying and allocation makes boxed arithmetic at least an order of magnitude slower than unboxed arithmetic.

Consider the following function, which loops over an array to compute its sum.
The function starts by calling the local function @loop@ with the initial loop index, and the initial sum.
The definition of @loop@ checks if it has reached the end of the array, and if so returns the sum; otherwise it increments the running sum and proceeds to the next index.

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop 0 0
 where
  loop index running_sum
   | index == length vector
   = running_sum
   | otherwise
   = let value = vector ! index
     in loop (index + 1) (running_sum + value)
\end{lstlisting}

It is not explicit in the program source, but the loop index and the running sum are both boxed values, because their type (@Int@) is boxed.
This function, if compiled naively, would spend more time boxing and unboxing than actually computing the sum.
For each element of the array, the function allocates two new boxed values: the updated index and the updated sum.
All of these new boxed values except the very last iteration are used once by the next iteration and then thrown away.
While the garbage collector is tuned for small, short-lived objects, it is better to not create any garbage in the first place.

Compiler optimisations to replace boxed values with unboxed values are well-known, and there are many different ways to do this.
The point to make is not that this is an interesting thing, just that we must know which optimisations our compiler performs to generate code that our compiler can optimise.

In GHC, boxed machine-word integers are represented by the following type, which defines @Int@ with a single constructor @I#@, taking an unboxed integer @Int#@. By convention, unboxed values and constructors that use them are named with the @#@ suffix.

\begin{lstlisting}
data Int = I# Int#
\end{lstlisting}

Now we know how machine-word integers are represented, we can look at an explicitly boxed version of @sum@.
This version still uses boxed integers, but all arithmetic operations explicitly unbox and rebox the arguments and return values.
Unboxed literals are written as @0#@ or @1#@.
Unboxed arithmetic operations are written as @+#@ or @==#@, and @!#@ for unboxed indexing.
With explicit boxing, it should now be visible that the recursive call to @loop@ constructs new boxed integers.

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop (I# 0#) (I# 0#)
 where
  loop (I# index) (I# running_sum)
   | index ==# length vector
   = I# running_sum
   | otherwise
   = let value = vector !# index
     in loop (I# (index +# 1#)) (I# (running_sum +# value))
\end{lstlisting}

Constructor specialisation \cite{peyton2007call} is a loop optimisation that can remove these boxed arguments to recursive calls.
It looks at the constructors to recursive calls, and counts which ones are scrutinised or unwrapped at the start of the function definition.
In this case, @loop@ is first (and later, as well) called with the constructors @I#@ for both arguments, and both arguments are scrutinised.
So it creates a specialised version of @loop@ where both arguments are @I#@ constructors.
This specialised version is the same as the original, except the arguments are known to be @I#@ constructors, which means the pattern-matching on the arguments can be simplified away.
We will call this specialised version @loop'I#'I#@.
Then everywhere that @loop@ is called with @I#@ constructors, it will be replaced with a call to @loop'I#'I#@.
So any function call that looks like (@loop (I# x) (I# y)@) is replaced by a call to our new function (@loop'I#'I# x y@).

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop'I#'I# 0# 0#
 where
  loop'I#'I# index running_sum
   | index ==# length vector
   = I# running_sum
   | otherwise
   = let value = vector !# index
     in loop'I#'I# (index +# 1#) (running_sum +# value)
\end{lstlisting}

Constructor specialisation has removed all the boxing except for the final return value, which is only constructed once anyway.
In this example, the original @loop@ function was no longer called, so it was able to be removed entirely.
It is not always the case that the original function can be removed, and constructor specialisation can duplicate the code many times: once for each combination of constructors.
This can cause quite a lot of copies of the original function, which can cause large intermediate programs that do not fit in memory.
To alleviate this, GHC implements some heuristics to limit the number of duplicates created, as well as only creating specialisations if the original function is not too large.
This makes sense for general purpose code, but for tight loops where we expect most of our runtime to be, we really want to be sure that all specialisations are created.
For tight loops, we want to \emph{force} constructor specialisation to occur as much as possible.
This is achieved by annotating the function to be specialised with the special constructor @SPEC@.
Going back to the original @sum@ function, if we want to force constructor specialisation on @loop@, we can do this by adding the @SPEC@ to the function binding as well as all calls to it:

\begin{lstlisting}
sum :: Vector Int -> Int
sum vector = loop SPEC 0 0
 where
  loop SPEC index running_sum
   | index == length vector
   = running_sum
   | otherwise
   = let value = vector ! index
     in loop SPEC (index + 1) (running_sum + value)
\end{lstlisting}

\subsection{Mutable references}
\label{ss:extraction:mutablerefs}

We have seen that GHC is able to eliminate boxing from function arguments, and we will take advantage of this during code generation.
We will make use of @SPEC@ to force constructor specialisation, to ensure as much can be unboxed as possible.
Sadly, mutable references are stored boxed, and an analogous constructor specialisation transform does not exist for mutable references.
This means that to get unboxed values, we must structure our generated code to pass values via function arguments instead of mutable references.

Unboxed mutable references do exist, but are unsuitable because they can \emph{only} store unboxed values.
Recursive types such as linked lists cannot be stored in unboxed references.
We desire an unboxed representation when possible, and boxed representation when necessary.

It may be surprising to users of other languages that we should move away from using mutable references in favour of function arguments.
Indeed, \citet{biboudis2017expressive} describes the \emph{opposite} transform when implementing Stream Fusion in MetaOCaml.
So this will not necessarily map to other languages, but it is true in the particular case of GHC.

In Data Flow Fusion \cite{lippmeier2013data} there is a transform called \emph{loop winding}, which converts mutable references to function arguments.
The motivation here is that GHC does not track aliasing information of arrays stored in mutable references, but does track it for arrays as function arguments.

\subsection{Extended constructor specialisation}

Constructor specialisation is not limited to boxing and unboxing, but works for arbitrary constructors, including types with multiple constructors such as (@Maybe a@) or (@Either a b@).
It even works for recursive types such as lists, which could produce an an infinite number of specialisations.
Constructor specialisation must be careful to limit the specialisations to a finite number of \emph{useful} ones.

Information about the initial state can be very helpful in finding the most specific call patterns.
In the following example, @go@ is first called with the call pattern (@go (Just _) (Just _)@).
Using this as the `seed' from which we start exploring, we can see that the initial call pattern proceeds to the next call pattern (@go Nothing (Just _)@), followed by a call to (@go Nothing Nothing@).
If we were to look at the body of @go@ without this initial seed, however, we would find the call patterns (@go Nothing _@) and (@go _ Nothing@).
These call patterns from the unseeded body are less specific than the call patterns for the seed, which means using them would not allow the second argument to be specialised away.
By starting from the initial seed, the extra information about the initial state can be propagated to the other states.

\begin{lstlisting}
initial = go (Just 1) (Just 2)
 where
  go (Just _) b       = go Nothing b
  go a       (Just _) = go a       Nothing
  go Nothing Nothing  = 0
\end{lstlisting}

Not all specialisations are useful.
To limit compilation time, memory usage and code blowup, it is important to limit the specialisations to those which will be used.
That is, \emph{only} those which are reachable from the initial state.
In the following example, the initial state is the call pattern (@go (Left _) (Right _)@).
At each step, the arguments are flipped, so from the initial state the next reachable call pattern is (@go (Right _) (Left _)@).
From here, we can get back to the original state.
This means in total there are only two reachable call patterns.

However, if we look at the body alone without the seed, the first two call patterns are (@go _ (Left _)@) and (@go _ (Right _)@).
From here, more call patterns can be found: (@go _ (Left _)@) calls (@go (Left _) (Left a)@) and (@go (Left _) (Right _)@).
Similarly, there are two call patterns reachable from @(go _ (Right _)@), and these are distinct from the two already seen.
In this way, starting from the initial state means we do not have to generate all the possible specialisations.

\begin{lstlisting}
reachable = go (Left 1) (Right 2)
 where
  go (Left  a) b = go b (Left  a)
  go (Right a) b = go b (Right a)
\end{lstlisting}

\TODO{diagrams}

Using the initial calls as the seed is important.
This \emph{was} implemented, but it only occurred for locally bound functions, not for top-level bindings.
The problem is that even though our examples were locally bound functions, other transforms such as let-floating occur before constructor specialisation, which means locally bound functions can be `floated' up to top-level bindings, where seeding does not work.
The other issue is that top-level bound functions can be exported; if functions are exported, we cannot know their initial call pattern, as they may be called from other modules.
So for exported top-level functions, we must seed the call-patterns using all initial calls in the current module, as well as those in the body.
For non-exported top-level functions, we can be sure that the initial state is in the current module, and so use any initial calls outside of the body as the seed.

When arguments are of recursive types, there can be an infinite number of reachable call patterns.
Suppose we wish to reverse a linked list.
We can write this using a helper function, which takes the list that is reversed so far, as well as the list to reverse.

\begin{lstlisting}
reverse :: [Int] -> [Int]
reverse xs0 = go [] xs0
 where
  go zs []     = zs
  go zs (x:xs) = go (x:zs) xs
\end{lstlisting}

The helper function @go@ could be specialised an infinite number of times, but this would lead to non-terminating compilation.
First, the call to @go@ is seeded with the call pattern (@go [] _@).
Then, at every step in the evaluation, a list constructor is moved from the second argument to the first, resulting in the infinite chain of call patterns, (@go [_] _@), (@go [_, _] _@), and so on.
Usually, these specialisations would not be produced because they do not reduce allocation.
However, in the original implementation, when @SPEC@ is used to force constructor specialisation, an infinite number of specialisations were produced, and the compiler did not terminate.
We implemented Roman Leshchinskiy's suggestion to fix this by setting a limit on how many times recursive types can be specialised, even when forcing constructor specialisation.

\section{Sources and Sinks}
In order to write meaningful streaming computations, we need to interact with the outside world.
The processes in our process networks are pure and have no way of interacting with the world: they simply shuffle data along channels.
At the start of the process network, for the inputs, we use \emph{sources} to pull from the outside, such as reading from a file.
At the end of the process network, for the outputs, we use \emph{sinks} to push to the outside, such as writing to a file.

These sources and sinks are really the pull and push streams we have seen before \REFTODO{background}, but in this case we do not need to implement combinators over them; such plumbing will be expressed as processes in the process network.
Sources and sinks are in many ways opposites of each other, but they also share many similarities, so let us refer to them collectively as \emph{endpoints}.

Endpoints need to encapsulate some internal state: for example writing to a file requires a filehandle, and perhaps a buffer to fill before writing, to amortise the cost of the system call.
As explained previously (\autoref{ss:extraction:mutablerefs}), using mutable references for this internal state would lead to poor performance due to boxing.
We need to use the same approach of passing this state as function arguments so they can be unboxed by constructor specialisation.
This is a tad more complicated than it sounds, because the each endpoint requires a different type of state: reading from a file requires a filehandle, while reading from an in-memory array requires the array and the current index.
On the other hand, this state type is an internal thing and should not be exposed to the user, which rules out adding it as a type parameter on the endpoint.
Just because we need to pass the state around as function arguments should not change the external interface.

In order to `wrap up' the internal state type, so only the endpoint itself can inspect the internal state, while the user can only hold on to the abstract state and pass it to the endpoint, we use existentially quantified types.
This is similar to how existential types are used in Stream Fusion \cite{coutts2007stream}, to hide the internal state of a pull stream.

We say that each endpoint has an internal state type, and only it knows what the type is.
We provide some operations with the state: a way to construct an initial state, for example opening the file and returning the handle; a pull or push function which takes the state and returns a new state; and a close function for when we have finished reading from or writing to the endpoint.

\subsection{Sources}

We define sources in Haskell with the following datatype (@Source a@), where the type parameter @a@ is the type of values to be pulled.
The internal state type is bound to @s@, and we define a record with three fields.
The first field, @sourceInit@, contains an effectful computation which returns the initial state.
The second field, @sourcePull@, is a function which takes the current state and returns a pair containing the pulled value, and the updated state.
The pulled value is wrapped in a @Maybe@, because streams are finite: @Nothing@ means the end of the stream, and (@Just v@) means the value @v@.
The third and final field, @sourceDone@, is a function which takes the current state and closes the stream.

\begin{lstlisting}[mathescape=true]
data Source a
 = $\exists$s. Source
 { sourceInit ::      IO s
 , sourcePull :: s -> IO (Maybe a, s)
 , sourceDone :: s -> IO ()
 }
\end{lstlisting}

Streams end only once, and after pulling a @Nothing@, the source should not be pulled on again.
The source should not be pulled again after it is closed, but these invariants are not checked.
The state must be used linearly: after passing a state to @sourcePull@, a new state is returned, and the old state must not be used again.
This linearity constraint also enforces that @sourcePull@ cannot be called after @sourceDone@, since @sourceDone@ consumes the old state but does not produce a new state.

We can define a @Source@ that reads lines of text from a file.
Here the internal state is simply a filehandle.
To initialise the source, we open the file in reading mode, with the @openFile@ function.
When the source is done, we close the file handle with @hClose@.
To pull from the source, we define a helper function @pull@, which takes the filehandle as an argument.
The @pull@ function checks whether the end of the file has been reached (@hIsEof@).
If the end of the file, it returns @Nothing@, along with the original filehandle.
Otherwise, it reads a line from the handle (@hGetLine@) and wraps the line in a @Just@ constructor.

\begin{lstlisting}
sourceOfFile :: FilePath -> Source String
sourceOfFile filepath
  = Source
  { sourceInit = openFile ReadMode filepath
  , sourcePull = pull
  , sourceDone = hClose }
 where
  pull handle = do
    eof <- hIsEof
    case eof of
     True  -> return (Nothing, handle)
     False -> do
      line <- hGetLine handle
      return (Just line, handle)
\end{lstlisting}

For the sake of example, this is a simplified version.
Certainly, this could be improved in terms of error handling: what if the file does not exist; and performance: reading a single line at a time will not give the best performance.

\subsection{Sinks}

We define sinks in Haskell very similar to sources, above.
The datatype (@Sink a@) represents a sink which accepts values pushed into it.
Again, the internal state type is bound to the existential type @s@, and we define a record with three fields.
The first and third fields are initialisation (@sinkInit@) and closing (@sinkDone@), and are the same as for sources.
The second field, @sinkPush@, takes the current state and the value to push, and returns the new state.
Unlike with (@Source a@) which pulls (@Maybe a@), we push a value of @a@ without the @Maybe@.
This is because for push streams we signal the end of the stream with @sinkDone@.

\begin{lstlisting}[mathescape=true]
data Sink a
 = $\exists$s. Sink
 { sinkInit ::           IO s
 , sinkPush :: s -> a -> IO s
 , sinkDone :: s ->      IO ()
 }
\end{lstlisting}

As with sources, sinks also require that the states are used linearly.
This precludes pushing to a closed stream.

Let us define a @Sink@ that writes lines of text to a file.
As with the @Source@ that reads, the internal state is a filehandle.
Initialisation opens the file in write mode, and when we are done we close the file.
To push a value, the helper function @push@ takes the filehandle and the line and use @hPutStrLn@ to write it, then returns the filehandle.

\begin{lstlisting}
sinkToFile :: FilePath -> Sink String
sinkToFile filepath
  = Sink
  { sinkInit = openFile WriteMode filepath
  , sinkPush = push
  , sinkDone = hClose }
 where
  pull handle line = do
    hPutStrLn handle line
    return handle
\end{lstlisting}

\TODO{Spend a lot of time talking about why we need the state, but the examples only use the same filehandle. Need an example, eg to/from Vector or file IO with buffering, which uses the state.}

\section{Code generation}
We are now in a position to return to perform code generation on our @grepGood@ example (\autoref{s:extraction:grepGood}), which filters lines in a file.
It has the following process network.

\begin{lstlisting}
Process network:
  sources: input = sourceOfFile fileIn
  sinks:   goods = sinkOfFile   fileOut
  processes:
    Process "filter"
      inputs:  input
      outputs: goods
      initial: l0
      instructions:
        l0   = Pull  input l1 l4
        l1 e = If (isPrefixOf "Good" e)
                          (l2 e)
                           l3
        l2 e = Push  goods e  l3
        l3   = Drop  input    l0

        l4   = Close goods l5
        l5   = Done
\end{lstlisting}

Sources and sinks are specified for @input@ and @goods@.
The right-hand side of each endpoint definition is the expression to construct the endpoint, used by code generation.

Next there is room for multiple processes.
In networks with multiple processes, the processes are fused together into a single process before code generation.
Our example only has one process, the @filter@ process, which pulls from the source stream @input@ and pushes to the sink stream @goods@.

The @filter@ process starts at label @l0@, which pulls from the @input@ stream.
Labels @l1@ to @l3@ perform the filtering for each element, while labels @l4@ and @l5@ clean up and end the process once the @input@ stream is finished.

The original process formulation \REFTODO{processes} used a global mutable heap.
This would require a mutable reference for each variable in the heap.
As explained in \autoref{ss:extraction:mutablerefs}, we do not wish to generate code that uses mutable references, because we wish to take advantage of constructor specialisation for unboxing.
Instead of a global mutable heap, we treat each label as a function with arguments.
Each label definition has a list of variable names which are the function parameters and can be used in the body of the instruction.
Likewise, each reference to a label, either as the initial state or next state of an instruction, has a list of expressions which are the function arguments.

Labels @l1@ and @l2@ both take the current element as a parameter.
The pull instruction at @l0@ has as its `success' destination the label @l1@ with no arguments.
Pull expects its `success' destination label to take one extra argument, which will be called with the pulled element at runtime.

Let us now go through the generated code, piece by piece.

\begin{lstlisting}
grepGood fileIn fileOut =
  case sourceOfFile fileIn of
   Source input'init input'pull input'done ->
    case sinkOfFile fileOut of
     Sink goods'init goods'push goods'done -> do
\end{lstlisting}

First we unpack the @input@ source (@sourceOfFile fileIn@) to get its initialisation function (@input'init@), its pull function (@input'pull@), and its close function (@input'done@).
We need to use a case analysis here, rather than the accessor functions @sourceInit@ etc, because accessor functions cannot be used to unpack existential types.
We perform the same unpacking for the @goods@ sink.
These source and sink component functions are in scope for the rest of the @grepGood@ function.

\begin{lstlisting}
      let l0 _ input's goods's = do
            (v, input's') <- input'pull input's
            case v of
             Just v' -> l1 SPEC input's' goods's v'
             Nothing -> l4 SPEC input's' goods's
\end{lstlisting}

Next, we define a function for each label of the process.
We start with label @l0@.
In the process @l0@ has no parameters, but we add extra parameters to each function during code generation.
The first parameter is the @SPEC@ used to force constructor specialisation.
The next two parameters are the current state of the @input@ source and @goods@ sink.

The instruction for @l0@ pulls from @input@, so our function @l0@ must also pull from @input@ by calling the @input'pull@ function with the current state @input's@.
The pulled value is bound to @v@, while the new state for the @input@ source is bound to @input's'@.
We unpack the pulled value @v@ to check whether pull succeeded or the stream has finished.
If the pull succeeds, we call the function for label @l1@, passing @SPEC@ for constructor specialisation, the updated @input@ source state, the unchanged @goods@ sink state, and the actual pulled value.
If the stream has finished, we call the function for label @l4@, again with the specialisation and endpoint states.


\begin{lstlisting}
      let l1 _ input's goods's e = do
            case isPrefixOf "Good" e of
             True  -> l2 SPEC input's goods's e
             False -> l3 SPEC input's goods's
\end{lstlisting}

The function for @l1@ takes as parameters the specialisation and endpoint state, as well as the current element (@e@).
We check whether the current element starts with the string \lstinline/"Good"/; if so it calls @l2@ passing along current element, otherwise @l3@.

\begin{lstlisting}
      let l2 _ input's goods's e = do
            goods's' <- goods'push goods's e
            l3 SPEC input's goods's'
\end{lstlisting}

The function for @l2@ pushes to the @goods@ sink by calling @goods'push@.
The new @goods@ state is bound to @goods's'@ and passed to @l3@.

\begin{lstlisting}
      let l3 _ input's goods's =
            l0 SPEC input's goods's
\end{lstlisting}

The instruction for @l3@ drops the pulled value from @input@.
Drop instructions exist solely as coordination hints for processes during fusion.
For code generation, it just calls @l0@.

\begin{lstlisting}
      let l4 _ input's goods's = do
            goods'done goods's
            l5 SPEC input's goods's
\end{lstlisting}

The function for @l4@ closes the @goods@ sink by calling @goods'done@ with the current state for @goods@.
This invalidates the old state, but @goods'done@ does not return a new state.
The function for @l5@ takes the current state for @goods@ as an argument, but does not use it.
Once a process has closed an output stream it can no longer push to the stream or close the stream again, although this invariant is not checked.
This means a process cannot use the state after closing an output stream, and we can safely reuse the old state to pass to @l5@.

\begin{lstlisting}
      let l5 _ input's goods's = do
            input'done input's
            return ()
\end{lstlisting}

The last label, @l5@, closes the input sink and finishes the process.

\begin{lstlisting}
      input's0 <- input'init
      goods's0 <- goods'init
      l0 SPEC input's0 goods's0
\end{lstlisting}

To start executing the process, we initialise the @input@ source and @goods@ sink, and call the initial label @l0@ with the endpoint states.

\section{Vector endpoints}

We might wish to perform array computations, rather than streaming from disk or network.
In this case, we need a way to create a source from a vector, and to convert a sink to a vector.

Creating a source from a vector is fairly easy.
The source state is the index into the vector, and pulling from the source checks if the index is within the vector, looks up the element at that index, and increments the index.

\begin{lstlisting}
sourceOfVector :: Vector a -> Source a
sourceOfVector vec
  = Source init pull done
 where
  init               = return 0

  pull ix
   | ix < length vec = return (Just (unsafeIndex vec ix), ix + 1)
   | otherwise       = return (Nothing, ix)

  done _             = return ()
\end{lstlisting}

We use \lstinline/unsafeIndex/ to forgo the usual bounds checks, which would be redundant here.

Converting a sink into a vector is a little more involved, but not overly so.
The first complication is that the sink does not know upfront the size of the output stream.
This means the sink must dynamically grow the output vector as it receives elements.
It starts with a mutable vector of size 4, then every push checks if there is room for the new element, and doubles the vector if necessary.
Once all elements are written, it freezes the mutable vector to an immutable one and discards the unused portion at the end of the vector.

The second complication is that sinks do not `return' a value, so the sink must take an argument describing where to store the vector.
Despite going to great lengths to avoid mutable references in code generation, some kind of mutable reference is unavoidable here.
The boxing overheads described earlier are negligible in this case though, because the reference is only written to and read from once at the end of the process, rather than once for every iteration.

\begin{lstlisting}
sinkVector :: IORef (Vector a) -> Sink a
sinkVector out
  = Sink init push done
 where
  init = do
    vec <- Mutable.unsafeNew 4
    return (vec, 0)

  push (vec, ix) e = do
    vec' <- if ix < Mutable.length vec
            then return vec
            else Mutable.unsafeGrow vec (Mutable.length vec)
    Mutable.unsafeWrite vec' ix e
    return (vec', ix + 1)

  done (vec, ix) = do
    vec' <- unsafeFreeze (Mutable.unsafeSlice vec ix)
    writeIORef out vec'
\end{lstlisting}

The runtime cost of resizing the vector, as well as the bounds check for every push, have a significant performance cost \REFTODO{benchmarks}.
When the upper bound of the length of the output stream is known before streaming starts, we can take advantage of this knowledge by allocating a large enough vector to start with, removing the need for dynamic resizing and bounds checks.


\begin{lstlisting}
sinkVectorSize :: Int -> IORef (Vector a) -> Sink a
sinkVectorSize maxSize out
  = Sink init push done
 where
  init = do
    vec <- Mutable.unsafeNew maxSize
    return (vec, 0)

  push (vec, ix) e = do
    Mutable.unsafeWrite vec ix e
    return (vec, ix + 1)

  done (vec, ix) = do
    vec' <- unsafeFreeze (Mutable.unsafeSlice vec ix)
    writeIORef out vec'
\end{lstlisting}

By removing the bounds check for each push, we have made it faster at the expense of safety.
The sink is no longer `safe', as if we pass too small an upper bound the program will write past the end of the allocated vector, violating another part of the program's memory or causing a segmentation fault.
This puts the burden on the user of the sink to be very sure that the upper bound is correct.

\subsection{Size hints}
\label{s:implementation:sizehints}

Ideally this size information would be tracked by the process network itself, rather than having to be specified by the user.
In Stream Fusion, these size hints are attached to the constructor of the Stream datatype \cite{coutts2007rewriting}.

\begin{lstlisting}
data Size
 = Exact Int
 | Max   Int
 | Unknown
\end{lstlisting}

Refer to \REFTODO{s:Future:SizeInference} for how to infer this.


\section{Constructing processes}
\begin{lstlisting}
filter :: Q (TExp (a -> Bool))  -> Channel a -> Network (Channel a)
filter predicate as = do
 bs <- channel
 predicate' <- liftQ $ unTypeQ predicate
 process (Next (Label 0) [])
   [ (Label 0, InstructionBinding []
              (pull as (Next (Label 1) []) (Next (Label 3) []))
   , (Label 1, InstructionBinding [Var 0]
              (If [|$predicate' $(var 0)|]
                (Next (Label 2) [var 0])
                (Next (Label 3) [])))
   , (Label 2, InstructionBinding [Var 0]
              (push bs (var 0) (Next (Label 3) [])))
   , (Label 3, InstructionBinding []
              (drop as (Next Label 0)))

   , (Label 4, InstructionBinding []
              (Done))]

 return bs
\end{lstlisting}

\section{Types for constructing networks}

Untyped names
\begin{lstlisting}
data Label = Label Name
data Var = Var Name

data ChannelU = ChannelU Name
data Channel a = Channel { getChannel :: Channel }
\end{lstlisting}


\begin{lstlisting}
data NetworkGraph
 = NetworkGraph
 { networkGraphSources   :: Map ChannelU Exp
 , networkGraphSinks     :: Map ChannelU Exp
 , networkGraphProcesses :: [Process]
 }
\end{lstlisting}

\begin{lstlisting}
type Network = StateT Q NetworkGraph
\end{lstlisting}

\begin{lstlisting}
channel :: Network (Channel a)
channel = Channel <$> runQ freshName
\end{lstlisting}

\begin{lstlisting}
source :: Q (TExp (Source a)) -> Network (Channel a)
source source = do
 c  <- channel
 s' <- unTypeQ source
 tell graph { networkGraphSources = Map.singleton (getChannel c) s' }
 return c
\end{lstlisting}


\begin{lstlisting}
data Process
 = Process
 { processInputs       :: Set ChannelU
 , processOutputs      :: Set ChannelU
 , processInitial      :: Next
 , processInstructions :: Map Label InstructionBinding
 }
\end{lstlisting}

\begin{lstlisting}
data Next
 = Next
 { nextLabel    :: Label
 , nextAssigns  :: [Exp]
 }
\end{lstlisting}

\begin{lstlisting}
data InstructionBinding
 = Info
 { infoBindings     :: [Var]
 , infoInstruction  :: Instruction
 }
\end{lstlisting}

\begin{lstlisting}
data Instruction
 = I'Pull ChannelU Next Next
 | I'Push ChannelU Exp Next
 | I'Jump Next
 | I'Bool Exp Next Next
 | I'Drop ChannelU Next
 | I'CloseOutput Channel Next
 | I'Done
\end{lstlisting}

\section{Types for code generation}

\begin{lstlisting}
data FusedNetwork
 { fusedNetworkSources      :: Map ChannelU Exp
 , fusedNetworkSinks        :: Map ChannelU Exp
 , fusedNetworkInstructions :: Map Label InstructionBinding
 , fusedNetworkInitial      :: Next
 }
\end{lstlisting}

\begin{lstlisting}
genNetwork1 :: NetworkGraph -> Q Exp
\end{lstlisting}


\begin{lstlisting}
data SourceBound
 { sourceBoundInit :: Exp
 , sourceBoundPull :: Exp -> Exp
 , sourceBoundDone :: Exp -> Exp
 }
\end{lstlisting}

\begin{lstlisting}
data SinkBound
 { sinkBoundInit :: Exp
 , sinkBoundPush :: Exp -> Exp -> Exp
 , sinkBoundDone :: Exp -> Exp
 }
\end{lstlisting}

\begin{lstlisting}
data EnvironmentGlobal
 { envGlobalSources :: Map ChannelU SourceBound
 , envGlobalSinks   :: Map ChannelU SinkBound
 }
\end{lstlisting}

\begin{lstlisting}
data EnvironmentLocal
 { envLocalSources :: Map ChannelU Exp
 , envLocalSinks   :: Map ChannelU Exp
 }
\end{lstlisting}

\begin{lstlisting}
bindStates :: EnvironmentGlobal -> Q (EnvLocal, [Bind])
updateState :: EnvironmentLocal -> Channel -> Exp -> EnvironmentLocal
stateArguments :: EnvironmentLocal -> [Exp]
initialiseAll :: EnvironmentGlobal -> (EnvironmentLocal -> Exp) -> Q Exp
openEnvironment :: FusedNetwork -> (EnvironmentGlobal -> Q Exp) -> Q Exp
\end{lstlisting}



For a @Source@ expression (@s :: Exp (TExp (Source a))@):
\begin{lstlisting}
bindSource :: Exp -> (SourceBound -> Q Exp) -> Q Exp
bindSource s with =
 case $s of
  Source init pull done -> do
   s0 <- init
   $(with (SourceBind
         { sourceBoundState = [|s0|]
         , sourceBoundPull  = \s -> [|pull $s|]
         , sourceBoundDone  = \s -> [|done $s|]
         }))
\end{lstlisting}

\begin{lstlisting}
genNext :: EnvironmentBound -> Next -> Q Exp
genNext env (Next label expressions) =
  foldl AppE (expOfLabel label) arguments
 where
  arguments = spec : argumentsOfEnvironment env ++ expressions
  spec = ConE 'SPEC
\end{lstlisting}

\begin{lstlisting}
genInstruction :: EnvironmentBound -> Instruction -> Q Exp
genInstruction env i = case i of
  Pull c n n' -> [|do
    (v,s') <- $(sourcePullOfEnvironment env)
    case v of
     Nothing -> $(genNext n' (updateEnvironmentState e c [|s'|]))
     Just v' -> $(genNext n (updateEnvironmentState e c [|s'|])) v'
    |]
\end{lstlisting}

\begin{lstlisting}
genInstructionBinding :: EnvironmentGlobal -> InstructionBinding -> Q Exp
genInstructionBinding global (InstructionBinding vars instr) = do
  local <- envLocalOfGlobal global
  code  <- genInstruction global local instr
  return $ LamE spec $ lams (bindingsOfLocal local) $ lams (fmap bindOfVar vars) code
 where
  lams = foldl LamE
  spec = ConP 'SPEC
\end{lstlisting}

\section{Other transforms}
\subsection{Cull outputs}
When fusing a producer with a consumer, the result process still \emph{produces}, and writes values to the output channel, as well as copying them into variables for the consumer to use.
This is necessary because there may be multiple consumers reading from the one producer, and these other consumers may not be fused in yet.
The producer needs to keep pushing to its output, until all consumers have been fused in.
Once all consumers of the output channel have been fused though, (if the output channel is not used as a sink), it is no longer required.
The \emph{cull outputs} transform finds outputs that are not used by the rest of the networks.
Any pushes to these channels can be replaced with jumps to where the push would have jumped to.
This is a very simple transform to remove some extraneous stuff.

% \subsection{Insert dups}
% Let's pretend that the fusion process handles this case, as is the proof and paper version

\subsection{Fusing a network}
The fusion algorithm described in \REFTODO{processes and fusion} fuses \emph{pairs of processes} at a time, but network generally contain more than two processes.
To fuse a whole network, we start by constructing a dependency graph of the process network.
Then we find all the terminal processes: those with no successors, only predecessors.
For each terminal process, fuse with each predecessor, and recurse.
This is like a bottom-up transform.
The rationale for starting from the bottom is that consumers force a particular access pattern on their producers.

\TODO{expand}

\subsection{Fusing a pair of processes}
Fusing a pair of processes is slightly different because variables are passed as function arguments, instead of a global heap.
It also needs to take into account variable bindings per label, because of the locally-bound variables.
For a pair of labels, the variables is the union of variables in the original processes, as well as any \emph{new} channel buffers which need to be bound.
These are the ones which have an input state of \emph{have}.

\subsection{Minimise}
Minimise performs simple skip/jump contraction.
If label @l@ is @jump m[x=e]@, anywhere that jumps to @l@, we want to replace with @m[x=e]@.
This is complicated a bit by the variable updates attached to each label.
So a label @l[u]@, which in turn jumps to @l'[u']@, must in fact be replaced with @l'[u'[u]]@, because both substitutions must be applied.

However, blindly joining the substitutions can duplicate work.
If we have the following label update: (@l[x = expensive 2]@) where @expensive@ is some costly operation, and we also have @l'[a=x, b=x]@, we do not want to duplicate the call to @expensive@ as @l'[a = expensive 2, b = expensive 2]@.
So, as a conservative approach, we only apply minimisation when all substitutions are only simple variables.

One must also beware of infinite loops, as in the case of @l = jump l[u]@.
Now, when looking up the `next' label for @l@, we keep a set of labels already seen.
If we encounter the same label twice, it is time to stop unfolding the definition.

This transform also performs a simple form of dead code removal, by removing unreachable labels.

Why do we do this?
The truth is that GHC supports these operations on recursive loops already.
It is unlikely that we are gaining much by performing such simple optimisations.
However, there are two main reasons to do this.
First, we can perform these simplifications between every pair of processes fused.
By making the input to fusion smaller we can produce smaller programs to being with, and simplify as we go, rather than relying on a monolithic simplifcation at the end.
Secondly, it makes the intermediate code smaller, making it easier to read and debug in the case that fusion fails.



% -----------------------------------------------------------------------------
\section{Optimisation}
\label{s:Optimisation}
\TODO{elsewhere}
After we have fused two processes together, it may be possible to simplify the result before fusing in a third. Consider the result of fusing @group@ and @merge@ which we saw back in Figure~\ref{fig:Process:Fused}. At labels @F1@ and @F2@ are two consecutive @jump@ instructions.
The update expressions attached to these instructions are also non-interfering, which means we can safely combine these instructions into a single @jump@.
In general, we prefer to have @jump@ instructions from separate processes scheduled into consecutive groups, rather than spread out through the result code.
The (PreferJump) clauses of Figure~\ref{fig:Fusion:Def:StepPair} implement a heuristic that causes jump instructions to be scheduled before all others, so they tend to end up in these groups.

Other @jump@ instructions like the one at @F5@ have no associated update expressions, and thus can be eliminated completely. Another simple optimization is to perform constant propagation, which in this case would allow us to eliminate the first @case@ instruction. 

Minimising the number of states in an intermediate process has the follow-on effect that the final fused result also has fewer states. Provided we do not change the order of instructions that require synchronization with other processes (@pull@, @push@ or @drop@), the fusibility of the overall process network will not be affected.

Another optimization is to notice that in some cases, when a heap variable is updated it is always assigned the value of another variable. In Fig.\ref{fig:Process:Fused}, the @v@ and @x1@ variables are only ever assigned the value of @b1@, and @b1@ itself is only ever loaded via a @pull@ instruction. Remember from \S\ref{s:Fusion:FusingPulls} that the variable @b1@ is the stream buffer variable. Values pulled from stream @sIn1@ are first stored in @b1@ before being copied to @v@ and @x1@. When the two processes to be fused share a common input stream, use of stream buffer variable allows one process to continue using the value that was last pulled from the stream, while the other moves onto the next one. 


% When the two processes are able to accept the next variable from the stream at the same time, there is no need for the separate stream buffer variable. This is the case in Figure~\ref{fig:Process:Fused}, and we can perform a copy-propagation optimisation, replacing all occurrences of @v@ and @x1@ with the single variable @b1@. To increase the chance that we can perform copy-propagation, we need both processess to want to pull from the same stream at the same time. Moving the @drop@ instruction for a particular stream as late as possible prevents a @pull@ instruction from a second process being scheduled in too early.
% To increase the chance that we can perform this above copy-propagation, we need both processess to want to pull from the same stream at the same time. In the definition of a particular process, moving the @drop@ instruction for a particular stream as late as possible prevents a @pull@ instruction from a second process being scheduled in too early. In general, the @drop@ for a particlar stream should be placed just before a @pull@ from the same stream. 

\section{What's the deal with drop anyway}
\TODO{elsewhere}

The purpose of the @drop@ instructions is to keep two consumers more closely synchronised.
One consumer cannot start processing the next element until the other has finished processing the current element.
Drop is not necessary for correctness, or even for ensuring boundedness of buffers: without it, the result program would still be correct, and one process could not `overtake' another by processing more than one element before the other one.

If we have two consumers @P@ and @Q@, both pulling from the same channel @C@, they both need to agree about when to pull from the channel.
Suppose that @P@ and @Q@ both pull from @C@, then push to their own output channel, @CP@ and @CQ@ respectively, drop their input, then loop back to pull again.
If we are executing @P@, and it pulls from @C@, it can keep executing and push to its output channel @CP@, then drop the input.
Now, @P@ has dropped its input, but cannot pull again yet, because there is no new value available to pull.
There is no new value available to pull because the producer cannot \emph{push} to @C@ yet, because @Q@ has not consumed its input.
Now, @Q@ can run, and pulls from its input.
This transitions its input from \emph{pending} to \emph{have}, which means the producer still cannot push yet, until @Q@ drops its input.
In this way, the drop allows the the producer to push only once all consumers have dealt with their input.
Without drops, @P@ would be able to process the next element before @Q@ had finished the previous one.

\begin{lstlisting}
P = process
  P1: c <- pull C
  P2: push CP c
  P3: drop C
  P4: jump P1

Q = process
  Q1: c <- pull C
  Q2: push CQ c
  Q3: drop C
  Q4: jump Q1
\end{lstlisting}

By synchronising the two processes together, when we fuse we will only have one copy of the code that pulls each element.
Because @P@ can only start pulling again by the time @Q@ has dropped, this means @P@ and @Q@ must both be trying to pull at the same time, which means we can reuse the same instructions generated from the previous time they both pulled.
The example @PQ_drop@ shows the fused process with drop instructions.
Note that there is only one copy of each input process' code.
On the other hand, @PQ_no_drop@ shows the fused process without drop instructions.
Here, there are two copies of pushing to @CP@, though the main loop only executes one per iteration: pushing the current element to @CP@, and the previous element to @CQ@.
As the processes get larger, and more processes are fused together, the issue of duplicating code becomes more serious.
There are two parts to this: first, we need to hold the entire process in memory to generate its code.
Secondly, as the generated assembly code gets larger, it is less likely to fit into the processor's cache.
Smaller code is generally better for performance.
Having two copies of the push to @CP@ means that any consumers of @CP@ must in turn have their code duplicated, with the pull instructions from @CP@ copied into both sites of the pushes.

\TODO{diagrams}
\begin{lstlisting}
PQ_drop = process
  P1Q1: c_buf <- pull C
        c_p    = c_buf
        c_q    = c_buf
        push CP c_p
        push CQ c_q
        drop C
        jump P1Q1

PQ_no_drop = process
  P1Q1:  c_buf <- pull C
         c_p    = c_buf
         c_q    = c_buf
         push CP c_p
  P2Q2:  c_buf <- pull c
         c_p    = c_buf
         push CP c_p
         push CQ c_q
         c_q    = c_buf
         jump P2Q2
\end{lstlisting}


\section{Related work}
Meta-Repa \cite{ankner2013edsl} also uses Template Haskell for code generation, and provides a similar distinction between typed interface and untyped code generation.
It provides a generalised abstract datatype (GADT) for its typed interface rather than using typed Template Haskell.
It was implemented before typed Template Haskell.

