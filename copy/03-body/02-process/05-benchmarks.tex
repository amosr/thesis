\chapter{Evaluation}
\label{s:Benchmarks}
Stream fusion is ultimately performed for practical reasons: we want the fused result program to run faster than the original unfused program.
We also need to be sure that the fused program is not too large, to ensure that compilation and fusion do not consume too much memory.
This chapter shows runtime benchmark results for fused programs, and the result size of fused programs.

\section{Benchmarks}

To benchmark our algorithm, we have implemented the fusion algorithm described in \cref{chapter:process:processes}, in a library called \emph{Folderol}\footnote{\url{https://github.com/amosr/folderol}}.
% Folderol is a Haskell implementation of the fusion algorithm described in the previous chapter (\cref{chapter:process:processes}).
This fusion system uses the topology of the entire process network to perform fusion.
When fusing one producer with multiple consumers, the fusion algorithm must coordinate between all the consumers to reach consensus on when to pull the next value.
This coordination between all consumers means the fusion algorithm requires global knowledge of the process network.
In contrast, shortcut fusion systems such as \cite{gill1993short} use rewrite rules to remove intermediate buffers and require only local knowledge, but cannot coordinate between multiple consumers.
In cases where shortcut fusion cannot fuse it fails silently, leaving the programmer unaware of the failure.
This silence is also due to the local nature of rewrite rules: if we wish to know whether all the processes have been fused, we need to know about all the processes.
To fuse the entire process network at compile-time, as well as to inform the programmer when fusion fails, Folderol uses Template Haskell, a form of metaprogramming.

% For the benchmarks, We use the \emph{Folderol} Template Haskell implementation. % described in \cref{chapter:process:implementation}.
Benchmarks are available at \url{https://github.com/amosr/folderol/tree/master/bench}.
As well as the ``gold-panning'' example from \cref{taxonomy/gold-panning}, we present one spatial algorithm, two file-based benchmarks, and one audio signal processing benchmark.
In the benchmarks, we compare Folderol variously against hand-fused implementations, the array library `Vector'\footnote{\url{http://hackage.haskell.org/package/vector}}, as well as streaming libraries `Conduit'\footnote{\url{https://hackage.haskell.org/package/conduit}}, `Pipes'\footnote{\url{https://hackage.haskell.org/package/pipes}} and `Streaming'\footnote{\url{https://hackage.haskell.org/package/streaming}}.

The `Vector' library provides high-performance boxed and unboxed arrays with pull-based shortcut fusion.
It is the \emph{de facto} standard for array programming in Haskell, and implements a shortcut fusion system called \emph{stream fusion}, introduced by \citet{coutts2007stream}.
Array operations are implemented by converting the input array to a stream, then performing the corresponding stream operation, and converting back to an array.
The shortcut fusion rule removes the superfluous conversion when a stream is converted to an intermediate array and immediately back to a stream.
Just as pull streams only support a single consumer, this rule can only remove intermediate arrays which have a single consumer.
If the same intermediate array is used multiple times, it acts as a \emph{fusion barrier}, forcing the stream to be manifested into a real, memory-backed array.
% While the fusion is pull-based, splits can still be expressed in the program by mentioning the same array multiple times.
% These splits act as fusion barriers, requiring streams to be manifested as in-memory arrays.
In practice, Vector provides no guarantees about whether fusion will occur, and the easiest way to tell whether fusion has occurred is to look at the generated code to count the loops and arrays.
We could benchmark the program to see whether it is ``fast enough'', but if we do not have an optimal baseline to compare against, it is hard to know how fast the program \emph{should} be.
After inspecting the generated code, if we discover that fusion has not occurred, it may be necessary to rewrite the program and hand-fuse it.
% it is a different thing to convince the compiler to somehow fuse it: perhaps the program must be rewritten and hand-fused, or perhaps strictness annotations are required.

% \TODO{Move elsewhere.}
% The Stream part of Vector is split into two types: the fully general stream, which is parameterised over a monad, and pure streams, essentially specialised to the identity monad.
% Vector has a minor problem with monadic actions, but this is an implementation detail that could probably be ignored.
% Converting from monadic streams back to vectors goes via linked-lists: this is because constructing a vector requires \Hs@ST@ or \Hs@IO@, but we cannot easily interleave the stream's monadic actions (which may be an arbitrary monad) with effects from \Hs@IO@.
% Instead, the current implementation runs all of the stream's actions, collecting the result in a list, and only constructs the vector after the stream has finished.
% This way, the effects do not need to be interleaved.
% It is possible to write this using several unsafe operations, but one must be \emph{very} careful for this, taking into account that laziness must be conquered to ensure the correct interleaving.


The first two Haskell streaming libraries, `Conduit' and `Pipes', both have limited APIs that ensure that any computation can be run with bounded buffers.
These streaming libraries are pull-based, and do not naturally support multiple queries: when a stream is shared among multiple consumers, part of the program must be hand-fused, or somehow rewritten as a straight-line computation.
These libraries also have a monadic interface, which allows the structure of the dataflow graph to depend on the values.
This expressiveness has a price: if the dataflow graph can change dynamically, we cannot statically fuse it.
% This price is paid even when the dataflow graph is static, as the same monadic structure is used: because repetition is expressed as an unfolding dynamic graph, even computations that would be static in other systems must be expressed dynamically, reducing the possibility for fusion.
If the function that computes the next step of the stream is hidden inside a monadic computation, it is hard for the compiler to pull it out and construct it.
% Why is this? Is this an inherent problem with monadic computations, because the structure of the monad depends dynamically on the input values?
Because the structure --- the function that computes the rest of the stream --- is dynamic, it is hard for the compiler to reason statically about the structure.
In Vector's stream fusion, great effort is taken to make the step function non-recursive and statically known, by splitting the stream into a static step function and a dynamic state.
Vector's static step function can be optimised at compile-time, while the state can change dynamically.
In contrast, in Conduit and Pipes, the step function and state are mixed together: the remainder of the stream is only available dynamically as a function closure, hidden inside the continuation for a monadic operation.

The final streaming library, `Streaming', is similar to the previous two libraries, in that it is a monadic streaming library with limited static fusion.
Unlike the previous two, Streaming supports explicit duplication of streams, in a similar way to polarised streams (\cref{taxonomy/polarised}).
To duplicate the stream, the \Hs@store@ function takes a computation to perform on the copy of the stream, while the original stream is passed through.
This operation is analogous to the \Hs/dup_ioi/ combinator, which duplicates a pull stream into a push stream and returns a new pull stream; however, the implementation encodes stream polarity with a monad transformer stack rather than directly using pull and push streams.



The benchmark results presented in this chapter were all run on a MacBook Pro, with a 2GHz Intel Core i7, and 16GB of RAM.
The operating system is OS X El Capitan.
To run the benchmarks, we used the Criterion\footnote{\url{http://hackage.haskell.org/package/criterion}} library.
Criterion offers a fairly reliable way to evaluate runtime performance, as it runs each benchmark multiple times to compute the mean runtime, and warns if the variance is too high or if there are outliers.
It can also collect statistics on allocations, which can be a more stable performance indicator than runtime, as it is less affected by the underlying operating system's scheduling decisions.

We have implemented each benchmark program multiple times across the different backends.
These different implementations generally follow the same structure.
All the implementations are available in \cref{app:Benchmarks}; in this chapter, we focus on the Folderol versions.
% Showing five different implementations of the same program is not that interesting.
% For the most part, we show only the implementations of the Folderol versions; the other versions are available in \cref{app:Benchmarks}.

\subsection{Gold-panning}

Our first benchmark is the \Hs@priceAnalyses@ example from \cref{taxonomy/gold-panning}.
This example computes statistical analyses of a particular stock's price over time, as well as how the stock compares to a market index.

\begin{haskell}[float,caption=Folderol implementation of \Hs/priceAnalyses/,label=l:bench:priceAnalysesFolderol]
priceAnalysesFolderol :: (FilePath,FilePath) -> IO (Double,Double)
priceAnalysesFolderol (fpStock, fpMarket) = do
  (pot,(pom,())) <- scalarIO $ \snkPOT -> scalarIO $ \snkPOM ->
    $$(fuse $ do
      stock  <- source [|sourceRecords fpStock|]
      market <- source [|sourceRecords fpMarket|]
      pot' <- priceOverTime   stock
      pom' <- priceOverMarket stock market
      sink pot' [|snkPOT|]
      sink pom' [|snkPOM|])
  return (pot,pom)

priceOverTime stock = do
  tp <- map [|\s -> (daysSinceEpoch (time s), price s)|] stock
  Stats.regressionCorrelation tp

priceOverMarket stock market = do
  j  <- joinBy [|\s m -> time s `compare` time m|] stock market
  pp <- map    [|\(s,m) -> (price s, price m)|] j
  Stats.regressionCorrelation pp
\end{haskell}

\Cref{l:bench:priceAnalysesFolderol} shows the Folderol implementation of \Hs@priceAnalyses@.
The calls to \Hs@scalarIO@ indicate that the query returns two scalar variables that we wish to capture.
The \Hs@fuse@ function converts the process network into executable code.
The dollar-sign syntax around \Hs@fuse@ denotes a Template Haskell splice, which means that the call to \Hs@fuse@ is evaluated at compile-time, and returns code to execute at runtime.
The \Hs@fuse@ function takes a process network constructed at compile-time, fuses the network into a single process, and generates code for the resulting process.
In the call to \Hs@source@, the piped brackets around the argument \Hs@[|sourceRecords fpStock|]@ indicate a Template Haskell quasiquote; this is used to delay execution of code from compile-time to runtime.

We use the same Folderol implementation to benchmark against concurrent execution of a Kahn process network.
We disable fusion by replacing the call to \Hs@fuse@ with a function called \Hs@fuseWith@, which takes a set of fusion and code generation options.
These options dictate whether to perform fusion, whether to automatically insert communication channels between each process, and if so what chunk size to use for channels.
After inserting the communication channels, the concurrent version uses the same code generation backend as the fused version.

For Pipes, which does not naturally support multiple queries, we implement a two-pass version which computes \Hs/priceOverTime/ and \Hs/priceOverMarket/ in separate loops over the input.
The implementation is available in \cref{l:a:bench:priceAnalysesPipes}.

For Streaming, we duplicate the stream explicitly to compute \Hs@priceOverTime@ as a push stream.
The implementation is available in \cref{l:a:bench:priceAnalysesStreaming}.

\input{figs/runtime/gold-panning.tex}

\Cref{fig:runtime:gold-panning} shows the results of benchmarking the different implementations of \Hs/priceAnalyses/ over $10^6$ elements.
We ran with different sizes of data ranging from $10^3$ to $10^8$ but do not show these additional values; the results for this size are representative of the relationship between the different implementations.
We compare the concurrent execution of Kahn process networks (KPN) with one, two, four, and eight processors.
For the concurrent executions, we vary the chunk size along the x axis, to try to find the best trade-off between memory usage and communication overhead.
For the sequential implementations --- Folderol, Pipes, and Streaming --- the inter-process chunk size does not affect execution.

Interpreting the results in \cref{fig:runtime:gold-panning}, we see that the fused Folderol implementation is the fastest.
For multiple processors, a chunk size of one hundred appears to be ideal.
For this example with only a few queries, there is a significant improvement between one and two processors, but little further improvement as more processors are added.
With the ideal chunk size, the concurrent execution is 1.5 times faster than the Pipes implementation, and $3.7$ times slower than the fused version.


\subsection{Quickhull}

Quickhull is a divide-and-conquer spatial algorithm to find the smallest convex hull containing all points.
At its core is the \Hs@filterMax@ operation which takes a line and an array of points, and finds the farthest point above the line, as well as all points above the line.
The streaming libraries, including Folderol, are parameterised over the monad for effects. For simplicity we use an \Hs@IO@ monad.

\begin{lstlisting}[float,label=l:bench:filterMaxFolderol,caption=Folderol implementation of filterMax]
filterMaxFolderol :: Line -> Vector Point -> IO (Point, Vector Point)
filterMaxFolderol l ps = do
  (maxim,(above,())) <- scalar          $ \snkMaxim ->
                        vectorSize   ps $ \snkAbove ->
   $$(fuse $ do
      ins    <- source [|sourceOfVector ps      |]
      annot  <- map    [|\p -> (p, distance p l)|] ins
      above  <- filter [|\(_,d) -> d > 0        |] annot
      above' <- map    [|fst                    |] above
      maxim  <- maxBy  [|compare `on` snd       |] annot
      sink maxim       [|snkMaxim               |]
      sink above'      [|snkAbove               |]$$)
  return (fst maxim, above)
\end{lstlisting}

\Cref{l:bench:filterMaxFolderol} shows the Folderol implementation of \Hs@filterMax@.
The Folderol implementation starts by constructing a sink for the maximum point to be pushed into (\Hs@snkMaxim@), and a sink for the vector of points above the line (\Hs@snkAbove@).
As we know the output vector of points is not going to be any longer than the input vector, we use \Hs@vectorSize@ as a size hint to specify the upper bound of the vector's size.
% remove the need to dynamically grow the vector. (\cref{s:implementation:sizehints}).
The Template Haskell splice calls \Hs@fuse@ to convert the process network into executable code.
The process network starts by converting the input vector \Hs@pts@ to a stream \Hs@ins@.
We use \Hs@source@ to create an input stream, which takes a Template Haskell expression denoting how to construct a source at runtime.
We then annotate each point of \Hs@ins@ with the distance between each point and the line with \Hs@map@, calling this stream \Hs@annot@.
The \Hs@annot@ stream is filtered to only those above the line (\Hs@above@), then the annotations thrown away (\Hs@above'@).
The maximum is computed by comparing the second half of each annotated point --- the distance --- and stored in \Hs@maxim@.
Finally, \Hs@maxim@ is pushed into the scalar output sink, and all \Hs@above'@ points are pushed into the vector output sink.


In the Folderol implementation, the \Hs/annot/ stream is used twice.
If we rewrite this to use Vector, as in \cref{l:bench:filterMaxVectorShare}, the corresponding \Hs/annot/ vector cannot be fused with both of its consumers, and the program requires multiple loops.
We call this the `shared distances' version, as the \Hs/annot/ vector containing the distances is made manifest and re-used by the two loops computing the maximum and the filter.

% The shortcut fusion system cannot fuse both operations into a single loop, and both operations require the distances between the line and each point.
% This means a choice must be made: either compute the distances upfront and share them, or recompute the distances in each operation.
% We compare both implementations.

\begin{lstlisting}[float,label=l:bench:filterMaxVectorShare,caption=Vector / share implementation of filterMax]
filterMaxVectorShare l ps
 = let annot = map (\p -> (p, distance p l)) ps
       point = fst
             $ maximumBy (compare `on` snd) annot
       above = map fst
             $ filter ((>0) . snd) annot
   in return (point, above)
\end{lstlisting}

% The shared distances implementation is shown in \cref{l:bench:filterMaxVectorShare}.
% First, compute the distance between each point and the line, and store this in a vector called \Hs@annot@.
% Then \Hs@point@ computes the maximum with \Hs@maximumBy@, comparing each distance.
% Next, \Hs@above@ filters each point.

Instead of manifesting the distances vector and sharing it among the two consumer loops, we can also recompute the distances for each consumer.
The recomputed distances implementation is available in \cref{l:a:bench:filterMaxVectorRecompute}.
To recompute the vectors, we modify the shared version, duplicating the binding for the \Hs@annot@ vector to \Hs@annot1@ and \Hs@annot2@. The maximum now uses \Hs@annot1@, and filter uses \Hs@annot2@.
This way, both maps to compute the distance can be fused into their consumers.

In the results below, recomputing the distances is faster and allocates less than the shared distances version.
For this benchmark we used only two-dimensional points, but it is possible that at higher dimensions the cost of recomputing distances may outweigh the cost of allocation.
The choice to recompute the distances requires intimate knowledge of how shortcut fusion works and might be surprising to the naive user: duplicating work does not usually \emph{improve} performance.

For Conduit, we also have two versions: the first uses two passes over the data (\cref{l:a:bench:filterMaxConduitTwoPass}), while the second is hand-fused into a single pass (\cref{l:a:bench:filterMaxConduitOnePass}).
The two-pass version defines two `conduits', or stream computations; \Hs@cabove@ for the filtered vector and \Hs@cmaxim@ for the maximum.
Computation \Hs@cabove@ converts the vector to a conduit with \Hs@sourceVector@, then annotates with the distances, filters according to the distances, removes the annotations, and converts back to a vector.
As with Folderol, we use size hints when converting back to a vector to remove any overhead with growing and copying the vector.
Computation \Hs@cmaxim@ also converts the vector to a conduit, then annotates with the distances, and computes the maximum.
The hand-fused Conduit version is more complicated, and loses some of the abstraction benefits from using a high-level streaming library.

For Pipes, we only have a hand-fused version (\cref{l:a:bench:filterMaxPipes}), which follows much the same structure as the Conduit hand-fused version.

Finally, the Streaming version (\cref{l:a:bench:filterMaxStreaming}) executes in a single pass, with the stream explicitly duplicated into both consumers.
We start by converting the vector to a stream with \Hs@sourceVector@, and then use \Hs@map@ to annotate each point with the distance from the line.
To duplicate the stream we use the \Hs@store@ combinator, which takes a computation to perform on the copy of the stream --- in this case computing the maximum with \Hs@maximumBy@.
The original stream is also passed through, which is filtered and then the annotations discarded.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.21s &   100\% & 1.2e9 & 100\% \\
Hand     &          & 0.14s &    66\% & 4.8e8 &  40\% \\
&&&\\
Vector & store      & 0.40s &   190\% & 1.8e9 & 150\%\\
       & recompute  & 0.34s &   161\% & 8.0e8 & 66\%\\
&&&\\
Conduit & 2-pass    & 10.0s & 4,762\% & 6.2e10& 5,167\% \\
       & hand       &  7.9s & 3,762\% & 4.4e10& 3,667\% \\
&&&\\
Pipes  & hand       &  4.9s & 1,876\% & 3.9e10& 3,250\% \\
Streaming &         &  4.4s & 2,095\% & 3.0e10& 2,500\% \\
\end{tabular}
\end{center}
\caption[Quickhull benchmark results]{Quickhull benchmark results}
\label{table:bench:quickhull}
\end{table}

\Cref{table:bench:quickhull} shows the runtimes for Quickhull over $10^7$ points, which corresponds to around 150MB in memory.
The hand-fused version is the fastest, followed by Folderol.
The Vector versions are somewhat slower because they require two iterations over the data, but still generate high-quality loops.
Intimate knowledge of shortcut fusion was required to find `recompute', the faster of the two vector benchmarks, and there is no indication in the source program, or from the compiler, that the two could not be fused together.
The streaming libraries are significantly slower, spending most of the time allocating closures and forcing thunks.
On the plus side, the limited APIs and linearity constraints for Conduit and Pipes made it more obvious that they would not be fused into a single loop, but even with partial hand-fusion, are still significantly slower.
The `Streaming' program allows explicit sharing, and was the simplest of the three streaming libraries, requiring no hand fusion.
Surprisingly, the simpler `Streaming' program is faster than the hand-fused `Conduit' and `Pipes' programs, but is still significantly slower than Folderol.

% It would be nice to know why the Folderol version is slower than the hand-fused version.
% Inspecting the assembly, the code is very similar.
% The difference between hand-fused and Folderol appears to be that the Folderol implementation is spilling sixteen more bytes to the stack every iteration.

These streaming libraries are not usually used for array computations because of this overhead.
In practice, one tends to `chunk' the data so that each element is an array of multiple elements: this reduces overhead paid for streaming.
This chunking must be implemented manually and complicates the program further, reducing the level of abstraction the programmer can work at.
Even with chunking, we may reduce the streaming overhead from once per element to once per thousand elements, but we can never remove it entirely.
With Folderol there is no streaming overhead to reduce because it is statically fused away.

\begin{figure}
\center
\begin{dot2tex}[dot]
digraph G {
  node [shape="none"];
  input [texlbl="\Hs/input/"];

  graph [style="rounded corners"];

  subgraph cluster_level  {
    lblstyle="left,xshift=1.0cm";
    label="(level detector)";
    squares [texlbl="\Hs/square/"];
    avg [texlbl="\Hs/average/"];
    sqrt [texlbl="\Hs/root/"];

    squares -> avg -> sqrt;
  };

  subgraph cluster_gain  {
    lblstyle="left";
    label="(gain)";
    transfer [texlbl="\Hs/transfer/"];
    multiply [texlbl="\Hs/multiply/"];
    transfer -> multiply;
  };

  input -> squares;
  input -> multiply;
  sqrt -> transfer;
  output [texlbl="\Hs/output/"];
  multiply -> output;
}
\end{dot2tex}
\caption{Dependency graph for audio compressor}
\label{figs/dep/compressor}
\end{figure}

\subsection{Dynamic range compression}
Dynamic range compression is an audio signal processing algorithm to reduce the volume of loud sounds, leaving quiet sounds unchanged.
To implement this, we take an audio signal, and at each point approximate the signal's current volume with a rolling average.
We pass the current volume to a \emph{transfer function} to compute the \emph{gain multiplier}, which denotes how much to adjust the volume by.
If the volume of the input signal is not above the volume threshold, the transfer function sets the gain multiplier to one, indicating no change to the input signal.
If the volume is louder than the threshold, the transfer function decreases the volume by setting the gain multiplier to a value below one.

\Cref{figs/dep/compressor} shows the dependency graph for the audio compressor.
The input stream is used twice, once by the level detector subgraph, and again by the gain subgraph.
The level detector subgraph approximates the signal volume by computing the root mean square with an exponential moving average.
The gain subgraph uses the output of the level detector to compute the gain multiplier, and then uses this to adjust the volume of the input signal.

\begin{lstlisting}[float,label=l:bench:compressorFolderol,caption=Folderol implementation of \Hs/compressor/]
compressor :: Vector Double -> IO (Vector Double)
compressor vecIns = do
  (vecOut,()) <- vectorSize vecIns $ \snkOut -> do
    $$(fuse $ do
        input   <- source    [|sourceOfVector vecIns|]
        squares <- square    input
        means   <- average   squares
        roots   <- root      means
        gains   <- transfer  roots
        output  <- multiply  input    gains
        sink output          [|snkOut|]$$)
  return vecOut

-- Stream transformers
square   = map       [|\x -> x * x|]
average  = postscanl [|average'   |] [|0|]
root     = map       [|sqrt       |]
transfer = map       [|transfer'  |]
multiply = zipWith   [|(*)        |]

-- Worker functions
average' acc sample
 = acc * 0.9 + sample * 0.1

transfer' mean
 | mean > 1  = 1 / mean
 | otherwise = 1
\end{lstlisting}

The Folderol implementation is shown in \cref{l:bench:compressorFolderol}.
% To implement this in Folderol, we start by creating a sink for our output vector, calling the sink \Hs@snkOut@ and the vector \Hs@vecOut@.
% This will be the same size as the input vector, so we use \Hs@vectorSize@.
% We take the input array \Hs@vecIns@ and convert it to a stream \Hs@ins@.
% We are going to compute the volume of the input stream using the \emph{root mean square}, so first we need to square each element.
% This is the \Hs@squares@ stream.
% We need to compute the average of the squares, so we will use an exponential moving average --- which is equivalent to as a low-pass filter.
The stream transformers \Hs/square/, \Hs/root/ and \Hs/transfer/ are computed by applying a single function to each stream element.
To compute the moving average, we use \Hs@postscanl@, which is like a fold over the data except it returns the stream of accumulators rather than just the final result.
The difference between \Hs@postscanl@ and regular \Hs@scanl@ is that the output stream for \Hs@scanl@ contains the original zero accumulator for the fold, which \Hs@postscanl@ omits.
% Now that we have our average, \Hs@avg@, we \Hs@clip@ each element by taking the square root, and clipping it to one. This is our multiplier, \Hs@mul@.
% Finally, we zip the original stream with the multiplier, and multiply them together.

The Vector implementation (\cref{l:a:bench:compressorVector}) is similar to the Folderol implementation, except it does not require the Template Haskell fusion directives or explicit conversion between arrays and streams.
Vector's shortcut fusion is able to fuse this program into a single loop, except the two occurrences of the input vector mean that the vector will be indexed twice.

\Cref{table:bench:compressor} shows the runtimes for the dynamic range compressor over $10^8$ samples, which corresponds to around 750MB in memory.
The runtime difference between the hand-fused implementation and the Folderol implementation are very slight. 
The Vector implementation is slower because its generated loop indexes the input array twice.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.93s &   100\% & 8.0e8 & 100\% \\
Hand     &          & 0.98s &   105\% & 8.0e8 & 100\% \\
&&&\\
Vector &            & 1.78s &   191\% & 8.0e8 & 100\%\\
\end{tabular}
\end{center}
\caption[Compressor benchmark results]{Compressor benchmark results}
\label{table:bench:compressor}
\end{table}

\subsection{Dynamic range compression with low-pass}
Before applying dynamic range compression to an audio signal, we may wish to perform some pre-processing on it.
For this benchmark, we want to apply a low-pass filter to the input signal.

\begin{lstlisting}[float,label=l:bench:compressorLopFolderol,caption=Folderol implementation of compressor with low-pass]
compressorLop :: Vector Double -> IO (Vector Double)
compressorLop vecIns = do
  (vecOut,()) <- vectorSize vecIns $ \snkOut -> do
    $$(fuse $ do
        input   <- source    [|sourceOfVector vecIns|]
        lopass  <- postscanl [|lop20k|] [|0|] input
        squares <- square    lopass
        means   <- average   squares
        roots   <- root      means
        gains   <- transfer  roots
        output  <- multiply  input    gains
        sink out             [|snkOut|]$$)
  return vecOut
\end{lstlisting}

The Folderol implementation of a dynamic range compressor with a low-pass is shown in \cref{l:bench:compressorLopFolderol}.
The low-pass filter is itself a kind of moving average, and is defined in much the same way as the \Hs/average/ stream transformer.
We use \Hs@postscanl@ to create a low-pass filter to remove frequencies above 20kHz, and call the result stream \Hs@lopass@.
The \Hs@lopass@ stream is then used as the input to the level detector subgraph and the gain subgraph.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 1.16s &   100\% & 8.0e8 & 100\% \\
Hand     &          & 1.18s &   101\% & 8.0e8 & 100\% \\
&&&\\
Vector &            & 2.21s &   190\% & 1.6e9 & 200\%\\
\end{tabular}
\end{center}
\caption[Compressor with low-pass benchmark results]{Compressor with low-pass benchmark results}
\label{table:bench:compressorlop}
\end{table}

\Cref{table:bench:compressorlop} shows the runtimes for compressor with low-pass.
For the Vector implementation in the previous example, the two occurrences of the input vector meant that the input array was indexed twice.
However, in the modified version with low-pass (\cref{l:a:bench:compressorLopVector}), it is the \Hs@lopass@ vector which is used twice, rather than the input vector.
Using the \Hs@lopass@ vector twice means that it must be reified into a manifest vector before it can be reused.
The Vector version then has two loops, as well as an extra manifest array.





\subsection{File operations}
For the file benchmarks, we compare against a hand-fused implementation, as well as the three previously mentioned Haskell streaming libraries: `Conduit', `Pipes', and `Streaming'.
We do not compare against `Vector'.
% The first of these two streaming libraries are pull-based, and do not naturally support multiple outputs: the split in the dataflow graph must be hand-fused, or somehow rewritten as a straight-line computation.
% These libraries also have a monadic interface, which allows the structure of the dataflow graph to depend on the values. This expressiveness has a price: if the dataflow graph can change dynamically, we cannot statically fuse it.
% This price is paid even when the dataflow graph is static, as the same monadic structure is used: because repetition is expressed as an unfolding dynamic graph, even computations that would be static in other systems must be expressed dynamically, reducing the possibility for fusion.

\begin{lstlisting}[float,label=l:bench:append2Folderol,caption=Folderol implementation of append2]
append2 :: FilePath -> FilePath -> FilePath -> IO Int
append2 fpIn1 fpIn2 fpOut = do
  (count,()) <- scalarIO $ \snkCount ->
    $$(fuse $ do
      in1   <- source [|sourceLinesOfFile fpIn1|]
      in2   <- source [|sourceLinesOfFile fpIn2|]
      aps   <- append in1 in2
      count <- fold   [|\c _ -> c + 1|] [|0|] aps

      sink count      [|snkCount               |]
      sink aps        [|sinkFileOfLines fpOut  |]$$)
  return count
\end{lstlisting}
\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.29s &   100\% & 8.6e8 & 100\% \\
Hand     &          & 0.29s &   100\% & 8.6e8 & 100\% \\
Conduit &           & 0.93s &   320\% & 3.3e9 & 383\% \\
Pipes  &            & 0.94s &   324\% & 3.4e9 & 395\% \\
Streaming &         & 0.57s &   196\% & 2.6e9 & 302\% \\
\end{tabular}
\end{center}
\caption[Append2 benchmark results]{Append2 benchmark results}
\label{table:bench:append2}
\end{table}


The first file benchmark appends two files while counting the lines.
\Cref{l:bench:append2Folderol} shows the Folderol implementation.
In Conduit (\cref{l:a:bench:append2Conduit}) and Pipes (\cref{l:a:bench:append2Pipes}), counting the lines is implemented as a stream transformer which counts each line before passing it along.
\Cref{table:bench:append2} shows the runtimes for appending two text files each with half a million lines, around 6.5MB in total.



\begin{lstlisting}[float,label=l:bench:part2Folderol,caption=Folderol implementation of part2]
part2 :: FilePath -> FilePath -> FilePath -> IO (Int, Int)
part2 fpIn1 fpOut1 fpOut2 = do
  (c1,(c2,())) <- scalarIO $ \snkC1 -> scalarIO $ \snkC2 ->
    $$(fuse defaultFuseOptions $ do
      in1       <- source    [|sourceLinesOfFile fpIn1|]
      (o1s,o2s) <- partition [|\l -> length l `mod` 2 == 0|] in1

      c1        <- fold      [|\c _ -> c + 1|] [|0|] o1s
      c2        <- fold      [|\c _ -> c + 1|] [|0|] o2s

      sink c1                [|snkC1                 |]
      sink c2                [|snkC2                 |]
      sink o1s               [|sinkFileOfLines fpOut1|]
      sink o2s               [|sinkFileOfLines fpOut2|]$$)
  return (c1, c2)
\end{lstlisting}
\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.30s &   100\% & 8.6e8 & 100\% \\
Hand     &          & 0.30s &   100\% & 8.6e8 & 100\% \\
Conduit & hand      & 0.66s &   220\% & 2.4e9 & 279\% \\
Pipes  & hand       & 0.55s &   183\% & 2.0e9 & 232\% \\
Streaming &         & 1.21s &   403\% & 6.1e9 & 709\% \\
\end{tabular}
\end{center}
\caption[Part2 benchmark results]{Part2 benchmark results}
\label{table:bench:part2}
\end{table}

The second file benchmark takes a file and partitions it into two files: one with even-length lines, and one with odd-length lines.
The output lines are also counted.
\Cref{l:bench:part2Folderol} shows the Folderol implementation.
Even with partial hand-fusion because of the multiple outputs, the Pipes (\cref{l:a:bench:part2Pipes}) and Conduit (\cref{l:a:bench:part2Conduit}) programs are slower than ours, as well as losing the abstraction benefits from using a high-level library.
The Streaming implementation (\cref{l:a:bench:part2Streaming}) allows streams to be shared in a fairly straightforward way and does not require hand-fusion, but is also the slowest in this benchmark.
\Cref{table:bench:part2} shows the runtimes for partitioning a file with one million lines, around 6.5MB.

\subsection{Partition / append}

The program in \cref{l:bench:partitionAppendFail} partitions an input vector into a vector containing the even-valued elements, and a vector containing the odd-valued elements.
The two result vectors are then appended together.
If we try to compile this code with Folderol, we get a compile-time warning:

% Process network after fusion:
% ()    ->{Folderol.Source.sourceOfVector xs_6989586621679047426}-> $0
% 
% $0    ->------------------------(dup2)--------------------------> $4 $5
%       ->L0
%         L0 (dup2_L0_0): pull $0 L1 L4
%         L1 (dup2_L1_0): push $4 L2
%         L2 (dup2_L2_0): push $5 L3
%         L3 (dup2_L3_0): drop $0 L0
%         L4 (dup2_L4_0): done
% $4 $5 ->--------------(append / filter / filter)----------------> $3
%       ->L0
%         L0 (append_L0_0_filter_L0_0_n_0_filter_L0_0_n_0): pull $4 L1 L4
%         L1 (append_L0_0_filter_L1_0_n_0_filter_L0_0_n_0): bool L3 L2
%         L2 (append_L0_0_filter_L3_0_n_0_filter_L0_0_n_0): drop $4 L0
%         L3 (append_L0_0_filter_L3_0_h_0_filter_L0_0_n_0): jump L5
%         L4 (append_L3_0_filter_L4_0_c_0_filter_L0_0_n_0): pull $5 L6 L9
%         L5 (append_L1_0_filter_L3_0_h_0_filter_L0_0_n_0): push $3 L2
%         L6 (append_L3_0_filter_L4_0_c_0_filter_L1_0_n_0): bool L8 L7
%         L7 (append_L3_0_filter_L4_0_c_0_filter_L3_0_n_0): drop $5 L4
%         L8 (append_L3_0_filter_L4_0_c_0_filter_L3_0_h_0): jump L10
%         L9 (append_L6_0_filter_L4_0_c_0_filter_L4_0_c_0): done
%         L10 (append_L4_0_filter_L4_0_c_0_filter_L3_0_h_0): push $3 L7
% $3    ->--------------{snkYs_6989586621679047427}---------------> ()

\begin{code}
bench/Bench/PartitionAppend/Folderol.hs:18:8: warning:
  Maximum process count exceeded:
    after fusion there are 2 processes, but maximum is 1.
  Falling back to unbounded communication channels.
\end{code}

This warning indicates that the program could not be fused into a single loop without introducing unbounded buffering.
Some sort of buffering is inevitable, because \Hs@append@ requires all the \Hs@evens@ first: all the even-valued elements need to be read from the input stream \Hs@x0@ before the odd-valued elements can be read by \Hs@append@.
To implement this, we would need to read from the input stream \Hs@x0@, and if we see an odd-valued element, we would need to hold on to it until we were sure there were no even-valued elements left.

\begin{lstlisting}[float,label=l:bench:partitionAppendFail,caption=Partition / append fusion failure]
partitionAppendFailure :: Vector Int -> IO (Vector Int)
partitionAppendFailure xs = do
  (apps,()) <- vectorSize xs $ \snkApps ->
    $$(fuse $ do
        x0    <- source [|sourceOfVector xs|]
        evens <- filter [|even             |] x0
        odds  <- filter [|odd              |] x0
        -- Failure: cannot fuse append with both filters
        apps  <- append evens odds
        sink apps       [|snkApps          |]$$)
  return apps
\end{lstlisting}


In this case, the input stream is backed by a real vector, which means we can, in fact, implement it without introducing an unbounded buffer: we already have the buffer.
To execute the program, we explicitly introduce another source to read from the vector, as in \cref{l:bench:partitionAppend2Source}.
The two filters now apply to different input sources, and even though they are backed by the same vector, there is no need to coordinate the two uses.
All of \Hs@evens@ can be read by \Hs@append@, independently of \Hs@odds@.

\begin{lstlisting}[float,label=l:bench:partitionAppend2Source,caption=Partition / append with two sources]
partitionAppend2Source xs = do
  (apps,()) <- vectorSize xs $ \snkApps ->
    $$(fuse $ do
        x0    <- source [|sourceOfVector xs|]
        x1    <- source [|sourceOfVector xs|]
        evens <- filter [|even             |] x0
        odds  <- filter [|odd              |] x1
        apps  <- append evens odds
        sink apps       [|snkApps          |]$$)
  return apps
\end{lstlisting}

We can implement this program another way, if we are willing to introduce two intermediate arrays.
In this version we introduce two loops.
In the first loop, we partition the input into two intermediate arrays.
Then in the second loop, we append the two intermediate arrays.
This implementation is shown in \cref{l:bench:partitionAppend2Loop}.

\begin{lstlisting}[float,label=l:bench:partitionAppend2Loop,caption=Partition / append with two loops]
partitionAppend2Loop xs = do
  (evens,(odds,())) <- vectorSize xs $ \snkEvens ->
                       vectorSize xs $ \snkOdds ->
    $$(fuse $ do
        x0      <- source    [|sourceOfVector xs|]
        (evens,odds)
                <- partition [|even             |] x0
        sink evens           [|snkEvens         |]
        sink odds            [|snkOdds          |]$$)
  (apps,()) <- vectorSize xs $ \snkApps ->
    $$(fuse $ do
        evens' <- source      [|sourceOfVector evens|]
        odds'  <- source      [|sourceOfVector odds|]
        apps   <- append evens' odds'
        sink apps             [|snkApps          |]$$)
  return apps
\end{lstlisting}

The Vector implementations (\cref{l:a:bench:partitionAppendVector}) follow the same structure: the two-loop version uses an intermediate array to store the filtered arrays in before appending them; the other indexes the input array twice, thus computing the predicate twice.
The two-loop Vector version uses the \Hs@partition@ primitive, which is itself a hand-fused implementation of two filters.
The Vector \Hs@partition@ cannot fuse with any consumers and always constructs manifest output vectors.
The fact that it constructs a manifest vector does allow an optimisation that Folderol cannot perform: if the size is known, the output can be filled at both ends, with the elements that satisfy the predicate filling the start of the array, and the elements that do not satisfy the predicate filling from the back of the array.
Once the end of the input is reached, the second half, which was filled back-to-front, can be reversed in-place.
Unfortunately, in this example, the Vector program still splits this array in two before appending it back together.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol & 2-loop   & 0.17s &   100\% & 2.4e8 & 100\% \\
         & 2-source & 0.28s &   165\% & 8.0e7 &  33\% \\
Vector   & 2-loop   & 0.15s &    88\% & 1.6e8 &  67\% \\
         & 2-source & 0.26s &   153\% & 1.6e8 &  67\% \\
\end{tabular}
\end{center}
\caption[PartitionAppend2 benchmark results]{PartitionAppend2 benchmark results}
\label{table:bench:part2app2}
\end{table}

\Cref{table:bench:part2app2} shows the runtimes for partitioning and appending an input vector with $10^7$ elements.
In the results table, the Vector version with two loops is the fastest, but the difference between it and Folderol with two loops is fairly small.
The Folderol version with two sources uses the least memory - a third of the two-loop Folderol, and a half of the Vector versions.
Sometimes, we are willing to sacrifice lower throughput for lower memory usage, and this decision depends on the context.
By giving a compile-time warning when fusion fails, we are able to maintain a high level of abstraction, while empowering the author to make these scheduling decisions.
% The author must explicitly choose when is the right time to reuse an array, when is the right time to introduce a new loop, and when is the right time to introduce intermediate buffers.
% This way, the result of fusion should not be magic or surprising.
% The author can be confident that if it compiles, it will have the desired streaming behaviour.

% -----------------------------------------------------------------------------
\section{Result Size}
% \TODO{Update with Folderol sizes}

As with any practical fusion system, we must be careful that the size of the result code does not become too large when more and more processes are fused together.
\Cref{fig:bench:outputsize} shows the maximum number of output states in the result when a particular number of processes are fused together in a pipelined-manner.
To produce this graph, we programmatically generated dataflow networks for \emph{all possible} pipelined combinations of the \Hs@map@, \Hs@filter@, \Hs@scan@, \Hs@group@ and \Hs@join@ combinators, and tried all possible fusion orders consiting of adjacent pairs of processes.
The \Hs@join@ combinator itself has two inputs, so only works at the very start of the pipeline --- we present result for pipelines with and without a \Hs@join@ at the start.
The same graph shows the number of states in the result when the various combinations of combinators are fused in parallel, for example, we might have a \Hs@map@ and a \Hs@filter@ processing the same input stream.
In both cases, the number of states in the result process grows linearly with the number of processes. In all combinations, with up to 7 processes there are fewer than 100 states in the result process. 


\input{copy/03-body/02-process/figures/process-size.tex}

The size of the result process is roughly what one would get when inlining the definitions of each of the original source processes. This is common with other systems based on inlining and/or template meta-programming, and is not prohibitive.

On the other hand, \cref{fig:bench:exponential} shows the results for a pathological case where the size of the output program is exponential in the number of input processes. The source dataflow networks consists of N join processes, N+1 input streams, and a single output stream. The output of each join process is the input of the next, forming a chain of joins. In source notation the network for N = 3 is (\Hs@sOut = join sIn1 (join sIn2 (join sIn3 sIn4))@).

When fusing two processes, the fusion algorithm essentially compares every state in the first process with every state in the second, computing a cross product. During the fusion transform, as states in the result process are generated they are added to a finite map --- the \Hs@instrs@ field of the process definition. The use of the finite map ensures that identical states are always combined, but genuinely different states always make it into the result. 

In the worst case, fusion of two processes produces O($n*m$) different states, where $n$ and $m$ are the number of states in each. If we assume the two processes have about the same number of states then this is O($n^2$). Fusing the next process into this result yields O($n^3$), so overall the worst case number of states in the result will be O($n^k$), where $k$ is the number of processes fused. 

In the particular case of \Hs@join@, the implementation has two occurrences of the \Hs@push@ instruction. During fusion, the states for the consuming process are inlined at each occurrence of \Hs@push@. These states are legitimately different because at each occurence of \Hs@push@ the input channels of the join process are in different channel states, and these channel states are included in the overall process state.


\section{Conclusion}
In the benchmarks, we saw that Folderol was always competitive with the hand-fused program, and in all but a few cases, faster than the other programs.
For array computations there is some extra code required to convert between vectors and streams.
There is also some extra wrapping with Template Haskell splices and quasiquotes, but this is fairly minor, and the required changes are more or less mechanical and type-driven.

Another benefit, aside from performance, is not having to read through Core programs to know whether whether fusion has worked or failed.
If fusion succeeds, we know it has succeeded.
If fusion fails, we know not just that it has failed, but \emph{where} it has failed.
These explicit fusion failures make it a lot easier to track down performance issues due to non-fusion.

