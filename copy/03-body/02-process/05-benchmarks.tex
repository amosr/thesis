\chapter{Evaluation}
\label{s:Benchmarks}
Stream fusion is ultimately performed for practical reasons: we want the fused result program to run faster than the original unfused program.
We also need to be sure that the fused program is not too large, otherwise compilation and fusion would consume too much memory.
This chapter shows benchmark results for fused programs and the result size of fused programs.

\section{Benchmarks}

For the benchmarks, I use the Folderol Template Haskell implementation described in \autoref{chapter:process:implementation}.
Benchmarks are available at \url{https://github.com/amosr/folderol/tree/master/bench}.
I present one spatial algorithm, two file-based benchmarks, and one audio signal processing benchmark.
The benchmarks are comparing with the array library `Vector', as well as streaming libraries `Conduit', `Pipes' and `Streaming'.

The `Vector' library provides high-performance boxed and unboxed arrays with pull-based shortcut fusion.
It is the \emph{de facto} standard for array programming in Haskell.
While the fusion is pull-based, splits can still be expressed in the program.
When splits are present, they act as fusion barriers, requiring streams to be manifested as in-memory arrays.
Vector also provides no guarantees about whether fusion will occur: it makes its best effort, but the easiest way to tell whether fusion has occurred is to look at the generated program.
We could benchmark the program to see whether it is ``fast enough'', but if we do not have an optimal baseline to compare against, it is hard to know how fast the program \emph{should} be.
After benchmarking, if we discover that fusion has not occurred, it is a completely other thing to convince the compiler to somehow fuse it.
Perhaps the program must be rewritten, hand-fused, or perhaps strictness annotations are required.

Move elsewhere.
The Stream part of Vector is split into two types: the fully general stream, which is parameterised over a monad, and pure streams, essentially specialised to the identity monad.
Vector has a minor problem with monadic actions, but this is an implementation detail that could probably be ignored.
Converting from monadic streams back to vectors goes via linked-lists: this is because constructing a vector requires @ST@ or @IO@, but we cannot easily interleave the stream's monadic actions (which may be an arbitrary monad) with effects from @IO@.
Instead, the current implementation runs all of the stream's actions, collecting the result in a list, and only constructs the vector after the stream has finished.
This way, the effects do not need to be interleaved.
It is possible to write this using several unsafe operations, but one must be \emph{very} careful for this, taking into account that laziness must be conquered to ensure the correct interleaving.


The three Haskell streaming libraries, `Conduit', `Pipes', and `Streaming', have a limited API that ensures that any computation can be run with only bounded buffers.
These streaming libraries are pull-based, and do not naturally support multiple outputs: the split in the dataflow graph must be hand-fused, or somehow rewritten as a straight-line computation.
These libraries also have a monadic interface, which allows the structure of the dataflow graph to depend on the values. This expressiveness has a price: if the dataflow graph can change dynamically, we cannot statically fuse it.
% This price is paid even when the dataflow graph is static, as the same monadic structure is used: because repetition is expressed as an unfolding dynamic graph, even computations that would be static in other systems must be expressed dynamically, reducing the possibility for fusion.
If the function that computes the next step of the stream is hidden inside a monadic computation, it is hard for the compiler to pull it out and construct it.
Why is this? Is this an inherent problem with monadic computations, because the structure of the monad depends dynamically on the input values?
Because the structure --- the function that computes the rest of the stream --- is dynamic, it is hard for the compiler to reason statically about the structure.
In Vector/stream fusion, great effort is taken to make the step function non-recursive and statically known, by splitting the stream into both step function and state.
The step function is completely static, and so can be optimised etc, but the state changes dynamically.
In contrast, in Pipes/Conduit, the step function and state are mixed together: the remainder of the stream is only available dynamically as a function closure, hidden inside a constructor inside a monadic operation.



The benchmark results presented below were all run on a MacBook Pro, with a 2GHz Intel Core i7, and 16GB of RAM.
The operating system is OS X El Capitan.
To run the benchmarks, I used the Criterion\footnote{\url{http://hackage.haskell.org/package/criterion}} library.
Criterion offers a fairly reliable way to evaluate runtime performance, as it runs each benchmark multiple times to compute the mean runtime, and warns if the variance is too high or if there are outliers.
It can also collect statistics on allocations, which is sometimes (but not always) a more stable performance indicator than runtime, as it is less affected by the whims of the underlying operating system.

\subsection{Quickhull}

Quickhull is a divide-and-conquer spatial algorithm to find the smallest convex hull containing all points.
At its core is the @filterMax@ operation which takes a line and an array of points, and finds the farthest point above the line, as well as all points above the line.
The streaming libraries, including Folderol, are parameterised over the monad for effects. For simplicity we use an @IO@ monad.

\begin{lstlisting}[float,label=l:bench:filterMaxFolderol,caption=Folderol implementation of filterMax]
filterMaxFolderol :: Line -> Vector Point -> IO (Point, Vector Point)
filterMaxFolderol l ps = do
  (maxim,(above,())) <- scalar          $ \snkMaxim ->
                        vectorSize   ps $ \snkAbove ->
   $$(fuse $ do
      ins    <- source [|sourceOfVector ps      |]
      annot  <- map    [|\p -> (p, distance p l)|] ins
      above  <- filter [|\(_,d) -> d > 0        |] annot
      above' <- map    [|fst                    |] above
      maxim  <- maxBy  [|compare `on` snd       |] annot
      sink maxim       [|snkMaxim               |]
      sink above'      [|snkAbove               |]$$)
  return (fst maxim, above)
\end{lstlisting}

\autoref{l:bench:filterMaxFolderol} shows the Folderol implementation of @filterMax@.
The Folderol implementation starts by constructing a sink for the maximum point to be pushed into (@snkMaxim@), and a sink for the vector of points above the line (@snkAbove@).
As we know the output vector of points is not going to be any longer than the input vector, we use @vectorSize@ as a size hint to remove the need to dynamically grow the vector (\autoref{s:implementation:sizehints}).
The Template Haskell splice calls @fuse@ to convert the process network into executable code.
The process network starts by converting the input vector @pts@ to a stream @ins@.
We use @source@ to create an input stream, which takes a Template Haskell expression denoting how to construct a source at runtime.
We then annotate each point of @ins@ with the distance between each point and the line with @map@, calling this stream @annot@.
The @annot@ stream is filtered to only those above the line (@above@), then the annotations thrown away (@above'@).
The maximum is computed by comparing the second half of each annotated point --- the distance --- and stored in @maxim@.
Finally, @maxim@ is pushed into the scalar output sink, and all @above'@ points are pushed into the vector output sink.


Let us now look at two versions of @filterMax@ using Vector.
The shortcut fusion system cannot fuse both operations into a single loop, and both operations require the distances between the line and each point.
This means a choice must be made: either compute the distances upfront and share them, or recompute the distances in each operation.
I compare both implementations.

\begin{lstlisting}[float,label=l:bench:filterMaxVectorShare,caption=Vector / share implementation of filterMax]
filterMaxVectorShare l ps
 = let annot = Unbox.map (\p -> (p, distance p l)) ps
       point = fst
             $ Unbox.maximumBy (compare `on` snd) annot
       above = Unbox.map fst
             $ Unbox.filter ((>0) . snd) annot
   in return (point, above)
\end{lstlisting}

The shared distances implementation is shown in \autoref{l:bench:filterMaxVectorShare}.
First, compute the distance between each point and the line, and store this in a vector called @annot@.
Then @point@ computes the maximum with @maximumBy@, comparing each distance.
Next, @above@ filters each point.

The recomputed distances implementation is available in \autoref{app:Benchmarks}, \autoref{l:a:bench:filterMaxVectorRecompute}.
To recompute the vectors, we start from the shared version and duplicate the @annot@ vector to @annot1@ and @annot2@. The maximum now uses @annot1@, and filter uses @annot2@.
This way, both maps to compute the distance can be fused into their consumers.

In the results below, recomputing the distances is faster than storing the distances.
For this benchmark we used only two-dimensional points, but it is possible that at higher dimensions the cost of recomputing distances may outweigh the cost of allocation.
Also, the choice to recompute the distances requires intimate knowledge of how shortcut fusion works and might be surprising to the naive user: duplicating work does not usually \emph{improve} performance.

For Conduit, we also have two versions: the first uses two passes over the data, while the second is hand-fused into a single loop.
The two-pass version defines two `conduits', or stream computations; @cabove@ for the filtered vector and @cmaxim@ for the maximum.
Computation @cabove@ converts the vector to a conduit with @sourceVector@, then annotates with the distances, filters according to the distances, removes the annotations, and converts back to a vector.
As with Folderol, we use size hints when converting back to a vector to remove any overhead with growing and copying the vector.
Computation @cmaxim@ also converts the vector to a conduit, then annotates with the distances, and computes the maximum.
The hand-fused Conduit version is more complicated, and loses the abstraction benefits from using a high-level streaming library.

For Pipes, we only have a hand-fused version, which follows much the same structure as the Conduit hand-fused version.

Finally, the Streaming version is much closer to the Vector version, except that the stream must be explicitly split into both consumers.
We start by converting the vector to a stream with @sourceVector@, and then use @map@ to annotate each point with the distance from the line.
To split the stream, we use @store@ which takes a computation to perform on the copy of the stream --- in this case computing the maximum with @maximumBy@.
The original stream is also passed through, which is filtered and then the annotations discarded.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.21s &   100\% & 1.2e9 & 100\% \\
Hand     &          & 0.14s &    66\% & 4.8e8 &  40\% \\
&&&\\
Vector & store      & 0.40s &   190\% & 1.8e9 & 150\%\\
       & recompute  & 0.34s &   161\% & 8.0e8 & 66\%\\
&&&\\
Conduit & 2-pass    & 10.0s & 4,762\% & 6.2e10& 5,167\% \\
       & hand       &  7.9s & 3,762\% & 4.4e10& 3,667\% \\
&&&\\
Pipes  & hand       &  4.9s & 1,876\% & 3.9e10& 3,250\% \\
Streaming &         &  4.4s & 2,095\% & 3.0e10& 2,500\% \\
\end{tabular}
\end{center}
\caption[Quickhull benchmark results]{Quickhull benchmark results}
\label{table:bench:quickhull}
\end{table}

\autoref{table:bench:quickhull} shows the runtimes for Quickhull over $10^7$ points, which corresponds to around 150MB in memory.
Different sizes of data produce similar results.
The hand-fused version is the fastest, followed by Folderol.
The Vector versions are somewhat slower because they require two iterations over the data, but still generate high-quality loops.
Intimate knowledge of shortcut fusion was required to find `recompute', the faster of the two vector benchmarks, and there is no indication in the source program, or from the compiler, that the two could not be fused together.
The streaming libraries are significantly slower, spending most of the time allocating closures and forcing thunks.
Why are they so much slower? This would be better in the motivation.
On the plus side, the limited APIs and linearity constraints for Conduit and Pipes made it more obvious that they would not be fused into a single loop, but even with partial hand-fusion, are still significantly slower.
The `Streaming' program allows explicit sharing, and was the simplest of the three streaming libraries, requiring no hand fusion.
Surprisingly, the simpler `Streaming' program is faster than the hand-fused `Conduit' program, but is still significantly slower than Folderol.

It would be nice to know why the Folderol version is slower than the hand-fused version.
Inspecting the assembly, the code is very similar.
The difference between hand-fused and Folderol appears to be that the Folderol implementation is spilling sixteen more bytes to the stack every iteration.

These streaming libraries are not usually used for array computations because of this overhead.
In practice, one tends to `chunk' the data so that each element is an array of multiple elements: this reduces overhead paid for streaming.
This chunking complicates the program further, again reducing the level of abstraction the programmer can work at.
Even with chunking, we may reduce the streaming overhead from once per element to once per thousand elements, but we can never remove it entirely.
With Folderol there is no streaming overhead to reduce because it is statically fused away.

\subsection{Dynamic range compression}
Dynamic range compression is an audio signal processing algorithm to reduce the volume of loud sounds, leaving quiet sounds unchanged.
To implement this, we take an audio signal, and at each point compute a rolling average of the volume.
We clip the volume at the upper bound, one, and divide the clipped volume by the original volume.
This will be our multiplier, which we multiply the original signal by at each step.
This way, if the volume is less than the limit, our multiplier will be one and the signal is unchanged. 
If the volume is above the limit, our multiplier will be the reciprocal of volume: whatever is required to get the volume back down to one.

\begin{lstlisting}[float,label=l:bench:compressorFolderol,caption=Folderol implementation of compressor]
compressor :: Vector Double -> IO (Vector Double)
compressor vecIns = do
  (vecOut,()) <- vectorSize vecIns $ \snkOut -> do
    $$(fuse $ do
        ins     <- source    [|sourceOfVector vecIns|]
        squares <- map       [|\x -> x * x|]       ins
        avg     <- postscanl [|lop        |] [|0|] squares
        mul     <- map       [|clip       |]       avg
        out     <- zipWith   [|(*)        |] mul   ins
        sink out             [|snkOut     |]$$)
  return vecOut
 where
  lop acc sample = acc * 0.9 + sample * 0.1

  clip mean
   = let root    = sqrt mean
     in  min 1.0 root / root
\end{lstlisting}

The Folderol version is shown in \autoref{l:bench:compressorFolderol}.
To implement this in Folderol, we start by creating a sink for our output vector, calling the sink @snkOut@ and the vector @vecOut@.
This will be the same size as the input vector, so we use @vectorSize@.
We take the input array @vecIns@ and convert it to a stream @ins@.
We are going to compute the volume of the input stream using the \emph{root mean square}, so first we need to square each element.
This is the @squares@ stream.
We need to compute the average of the squares, so we will use an exponential moving average --- which is equivalent to as a low-pass filter.
To compute the moving average, we use @postscanl@, which is like a fold over the data, except it returns the stream of accumulators rather than just the final result.
The difference between @postscanl@ and regular @scanl@ is that the output stream for @scanl@ contains the original zero accumulator for the fold, while @postscanl@ omits this.
Now that we have our average, @avg@, we @clip@ each element by taking the square root, and clipping it to one. This is our multiplier, @mul@.
Finally, we zip the original stream with the multiplier, and multiply them together.

The Vector version is very similar to the Folderol version, except without the Template Haskell boilerplate.
The shortcut fusion \emph{is} able to fuse this into a single loop, except the two occurrences of the input vector mean that it will be indexed twice.

\autoref{table:bench:compressor} shows the runtimes for the dynamic range compressor over $10^8$ samples, which corresponds to around 750MB in memory.
The difference between the hand-fused version and the Folderol version are very slight, and may be due to operating system scheduling overhead. 
The Vector version is slower because its generated loop indexes the input array twice.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.93s &   100\% & 8.0e8 & 100\% \\
Hand     &          & 0.98s &   105\% & 8.0e8 & 100\% \\
&&&\\
Vector &            & 1.78s &   191\% & 8.0e8 & 100\%\\
\end{tabular}
\end{center}
\caption[Compressor benchmark results]{Compressor benchmark results}
\label{table:bench:compressor}
\end{table}

\subsection{Dynamic range compression with low-pass}
Before compressing an audio signal, we may wish to perform some pre-processing on it.
For this example, suppose we want to apply a low-pass filter to the input signal.

\begin{lstlisting}[float,label=l:bench:compressorLopFolderol,caption=Folderol implementation of compressor]
compressorLop :: Vector Double -> IO (Vector Double)
compressorLop vecIns = do
  (vecOut,()) <- vectorSize vecIns $ \snkOut -> do
    $$(fuse $ do
        ins     <- source    [|sourceOfVector vecIns|]
        lopped  <- postscanl [|lop20k     |] [|0|] ins
        squares <- map       [|\x -> x * x|]       lopped
        avg     <- postscanl [|lop        |] [|0|] squares
        mul     <- map       [|clip       |]       avg
        out     <- zipWith   [|(*)        |] mul   lopped
        sink out             [|snkOut     |]$$)
  return vecOut
\end{lstlisting}

The code for compressor with a low-pass is shown in \autoref{l:bench:compressorLopFolderol}.
Here we again use @postscanl@ to create another low-pass filter, calling the result @lopped@.
Then @lopped@ is used as the input to both @squares@ and @out@.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 1.16s &   100\% & 8.0e8 & 100\% \\
Hand     &          & 1.18s &   101\% & 8.0e8 & 100\% \\
&&&\\
Vector &            & 2.21s &   190\% & 1.6e9 & 200\%\\
\end{tabular}
\end{center}
\caption[Compressor with low-pass benchmark results]{Compressor with low-pass benchmark results}
\label{table:bench:compressorlop}
\end{table}

\autoref{table:bench:compressorlop} shows the results for compressor with low-pass.
For the Vector implementation, in the previous example the two occurrences of @ins@ just meant that the input array was indexed twice.
However, in the modified version it is not an input vector being used twice, but the result of @postscanl@.
This means that the result vector for @lopped@ must be reified into a manifest vector, before it can be reused.
The Vector version then has two loops, as well as an extra manifest array.





\subsection{File operations}
For the file benchmarks, we compare against three Haskell streaming libraries: `Conduit', `Pipes', and `Streaming'.
These streaming libraries are pull-based, and do not naturally support multiple outputs: the split in the dataflow graph must be hand-fused, or somehow rewritten as a straight-line computation.
These libraries also have a monadic interface, which allows the structure of the dataflow graph to depend on the values. This expressiveness has a price: if the dataflow graph can change dynamically, we cannot statically fuse it.
% This price is paid even when the dataflow graph is static, as the same monadic structure is used: because repetition is expressed as an unfolding dynamic graph, even computations that would be static in other systems must be expressed dynamically, reducing the possibility for fusion.

\begin{lstlisting}[float,label=l:bench:append2Folderol,caption=Folderol implementation of append2]
append2 :: FilePath -> FilePath -> FilePath -> IO Int
append2 fpIn1 fpIn2 fpOut = do
  (count,()) <- scalarIO $ \snkCount ->
    $$(fuse $ do
      in1   <- source [|sourceLinesOfFile fpIn1|]
      in2   <- source [|sourceLinesOfFile fpIn2|]
      aps   <- append in1 in2
      count <- fold   [|\c _ -> c + 1|] [|0|] aps

      sink count      [|snkCount               |]
      sink aps        [|sinkFileOfLines fpOut  |]$$)
  return count
\end{lstlisting}

\begin{lstlisting}[float,label=l:bench:part2Folderol,caption=Folderol implementation of part2]
part2 :: FilePath -> FilePath -> FilePath -> IO (Int, Int)
part2 fpIn1 fpOut1 fpOut2 = do
  (c1,(c2,())) <- scalarIO $ \snkC1 -> scalarIO $ \snkC2 ->
    $$(fuse defaultFuseOptions $ do
      in1       <- source    [|sourceLinesOfFile fpIn1|]
      (o1s,o2s) <- partition [|\l -> length l `mod` 2 == 0|] in1

      c1        <- fold      [|\c _ -> c + 1|] [|0|] o1s
      c2        <- fold      [|\c _ -> c + 1|] [|0|] o2s

      sink c1                [|snkC1                 |]
      sink c2                [|snkC2                 |]
      sink o1s               [|sinkFileOfLines fpOut1|]
      sink o2s               [|sinkFileOfLines fpOut2|]$$)
  return (c1, c2)
\end{lstlisting}

The first file benchmark simply appends two files while counting the lines.
In Pipes and Conduit, counting the lines is implemented as a pipe which counts each line before passing it along.
\autoref{table:bench:append2} shows the runtimes for appending two text files each with half a million lines, around 6.5MB in total.



\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.29s &   100\% & 8.6e8 & 100\% \\
Hand     &          & 0.29s &   100\% & 8.6e8 & 100\% \\
Conduit &           & 0.93s &   320\% & 3.3e9 & 383\% \\
Pipes  &            & 0.94s &   324\% & 3.4e9 & 395\% \\
Streaming &         & 0.57s &   196\% & 2.6e9 & 302\% \\
\end{tabular}
\end{center}
\caption[Append2 benchmark results]{Append2 benchmark results}
\label{table:bench:append2}
\end{table}

The second file benchmark takes a file and partitions it into two files: one with even-length lines, and one with odd-length lines.
The output lines are also counted.
Even with partial hand-fusion because of the multiple outputs, the Pipes and Conduit programs are slower than ours, as well as losing the abstraction benefits from using a high-level library.
The `Streaming' library allows streams to be shared in a fairly straightforward way and does not require hand-fusion, but is also the slowest in this benchmark.
\autoref{table:bench:part2} shows the runtimes for partitioning a file with one million lines, around 6.5MB.

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol &          & 0.30s &   100\% & 8.6e8 & 100\% \\
Hand     &          & 0.30s &   100\% & 8.6e8 & 100\% \\
Conduit & hand      & 0.66s &   220\% & 2.4e9 & 279\% \\
Pipes  & hand       & 0.55s &   183\% & 2.0e9 & 232\% \\
Streaming &         & 1.21s &   403\% & 6.1e9 & 709\% \\
\end{tabular}
\end{center}
\caption[Part2 benchmark results]{Part2 benchmark results}
\label{table:bench:part2}
\end{table}

\subsection{Partition / append}

\begin{lstlisting}[float,label=l:bench:partitionAppendFail,caption=Partition / append fusion failure]
partitionAppend :: Vector Int -> IO (Vector Int)
partitionAppend !xs = do
  (ys,()) <- vectorSize xs $ \snkYs ->
    $$(fuse $ do
        x0    <- source [|sourceOfVector xs|]
        as    <- filter [|even             |] x0
        bs    <- filter [|odd              |] x0
        asbs  <- append as bs
        sink asbs       [|snkYs               |]$$)
  return ys
\end{lstlisting}

If we try to compile the code in \autoref{l:bench:partitionAppendFail}, we get a compilation error:
\begin{code}
Maximum process count exceeded:
 after fusion there are 2 processes, but maximum is 1
\end{code}
This indicates that the program could not be fused into a single loop, without introducing unbounded buffering.
This is because @append@ requires all the @as@ first: all the even-valued elements need to be read from the stream, before the odd-valued elements can be read by append.
But this means that when reading from the stream, if we see an odd-valued element, we would need to hold on to it until we were sure there were no even-valued elements left.

In this case, the input stream is not really a stream, but is backed by a real vector.
So we can actually implement it without introducing an unbounded buffer: we already have the buffer.
To execute the program, we \emph{explicitly} introduce another source to read from the vector. 

\begin{lstlisting}[float,label=l:bench:partitionAppend2Source,caption=Partition / append with two sources]
partitionAppend2Source :: Vector Int -> IO (Vector Int)
partitionAppend2Source !xs = do
  (ys,()) <- vectorSize xs $ \snkYs ->
    $$(fuse $ do
        x0    <- source [|sourceOfVector xs|]
        x1    <- source [|sourceOfVector xs|]
        as    <- filter [|even             |] x0
        bs    <- filter [|odd              |] x1
        asbs  <- append as bs
        sink asbs       [|snkYs               |]$$)
  return ys
\end{lstlisting}

But we can actually implement it another way, if we are willing to introduce two intermediate arrays.
In this version we introduce two loops.
In the first loop, we partition the input into two intermediate arrays.
Then in the second loop, we append the two intermediate arrays.

\begin{lstlisting}[float,label=l:bench:partitionAppend2Loop,caption=Partition / append with two loops]
runPartApp2Loop :: Unbox.Vector Int -> IO (Unbox.Vector Int)
runPartApp2Loop !xs = do
  (as,(bs,())) <- vectorSize xs $ \snkAs ->
                  vectorSize xs $ \snkBs ->
    $$(fuse $ do
        x0      <- source    [|sourceOfVector xs|]
        (as,bs) <- partition [|even             |] x0
        sink as              [|snkAs            |]
        sink bs              [|snkBs            |]$$)
  (ys,()) <- vectorSize xs $ \snkYs ->
    $$(fuse $ do
        as'   <- source      [|sourceOfVector as|]
        bs'   <- source      [|sourceOfVector bs|]
        asbs  <- append as' bs'
        sink asbs            [|snkYs               |]$$)
  return ys
\end{lstlisting}

\begin{table}
\begin{center}
\begin{tabular}{ll|rrrr}
& & Runtime (s)  & Runtime (\%) & Allocation (b) & Allocation (\%) \\
\hline
Folderol & 2-loop   & 0.17s &   100\% & 2.4e8 & 100\% \\
         & 2-source & 0.28s &   165\% & 8.0e7 &  33\% \\
Vector   & 2-loop   & 0.15s &    88\% & 1.6e8 &  67\% \\
         & 2-source & 0.26s &   153\% & 1.6e8 &  67\% \\
\end{tabular}
\end{center}
\caption[Part2 benchmark results]{Part2 benchmark results}
\label{table:bench:part2}
\end{table}


In the table, the Vector version with two loops is the fastest, but the difference between it and Folderol with two loops is fairly small.
The Folderol version with two sources uses the least memory - a third of the two loop Folderol, and a half of the Vector versions.
Sometimes, we are willing to sacrifice lower throughput for lower memory usage, and this decision really depends on the context.
So by giving a compilation when fusion fails, we are able to maintain a high level of abstraction, while at the same time forcing the author to make these scheduling decisions.
The author must explicitly choose when is the right time to reuse an array, when is the right time to introduce a new loop, and when is the right time to introduce intermediate buffers.
This way, the result of fusion should not be magic or surprising.
The author can be confident that if it compiles, it will have the desired streaming behaviour.


\section{Program size}
\label{s:Evaluation}

Stream fusion is ultimately performed for practical reasons. We want the fused result program to run faster than unfused source program. Fused result programs like the one in Fig. \ref{fig:Process:Fused} can be lowered directly to a target language, such as Haskell (maybe using Template Haskell), C or LLVM abstract assembly. The details of such lowering surely affect the performance of the final program, but as they are largely unrelated to the fusion algorithm itself, we focus on the form of the fused program expressed directly in the process language.


% -----------------------------------------------------------------------------
\section{Fusibility}
\label{s:FusionOrder}
When we fuse a pair of processes we commit to a particular interleaving of instructions from each process. When we have at least three processes to fuse, the choice of which two to handle first can determine whether this fused result can then be fused with the third process. Consider our @alternatives@ example from \S\ref{s:EvaluationOrder}, here it is again:
\begin{lstlisting}[float,label=l:bench:alternates,caption=Alternates]
  alternates : S Nat -> S Nat -> S Nat -> S (Nat, Nat)
  alternates sInA sInB sInC
   = let  s1   = alt2 sInA sInB
          s2   = alt2 sInB sInC
          sOut = zip s1 s2
     in   sOut
\end{lstlisting}

In our current system, if we fuse the two @alt2@ processes together first, then try to fuse this result process with the downstream @zip@ process, then this final fusion transform fails. This happens because the first fusion transform commits to a sequential instruction interleaving where two output values \emph{must} be pushed to stream @s1@ first, before then pushing values to @s2@. As we discussed in \S\ref{s:EvaluationOrder}, the downstream @zip@ process needs to pull single values from @s1@ and @s2@ alternatively.

Dynamically, if we were to try to execute the result process and the downstream @zip@ process concurrently, then the execution would deadlock. Statically, when we try to fuse the result process with the downstream @zip@ process then the deadlock is discovered and fusion fails. Deadlock happens when neither process can advance to the next instruction, and in the fusion algorithm this will manifest as the failure of the $tryStepPair$ function from Fig.\ref{fig:Fusion:Def:StepPair}. The $tryStepPair$ function determines which of the instructions from either process can be executed next, and when execution is deadlocked there are none.

On the upside, fusion failure is easy to detect. It is also easy to provide a report to the client programmer that describes why two particular processes could not be fused. The report is phrased in terms of the process definitions visible to the client programmer, instead of partially fused intermediate code. The joint labels used in the fusion algorithm represent which states each of the original processes would be in during a concurrent execution, and we provide the corresponding instructions as well as the abstract states of all the input channels. This reporting ability is \emph{significantly better} than that of prior fusion systems such as Repa~\cite{lippmeier2012guiding}, as well as the co-recursive stream fusion of \cite{coutts2007stream}, and many other systems based on general purpose program transformations. In such systems it is usually not clear whether the fusion transformation even succeeded, and debugging why it might not have succeeded involves spelunking\footnote{def. spelunking: Exploration of caves, especially as a hobby. Usually not a science.} through many pages (sometimes hundreds of pages) of compiler intermediate representations.

In practice, the likelihood of fusion suceeding depends on the particular dataflow network being used, as well as the form of the processes in that network. For fusion of pipelines of standard combinators such as @map@, @fold@, @filter@, @scan@ and so on, fusion always succeeds. The process implementations of each of these combinators only pull one element at a time from their source streams, before pushing the result to the output stream, so there is no possiblity of deadlock. Deadlock can only happen when multiple streams fan-in to a process with multiple inputs, such as with @merge@. When the dataflow network has a single output stream then we use the method of starting from the process closest to the output stream, walking to towards the input streams, and fusing in successive processes as they occur. This allows the interleaving of the intermediate fused process to be dominated by the consumers, rather than producers, as consumers are more likely to have multiple input channels which need to be synchronized. In the worst case the fallback approach is to try all possible orderings of processes to fuse, assuming the client programmer is willing to wait for the search to complete. 

% We discuss other possible approaches in \S\ref{s:Future:FusionOrder}. 


% When the two @alt2@ processes are fused we end up with a sequential result process. Trying to then fuse that result into the downstream @zip@ process fails. Alternately, if we were to try to concurrently evaluate the fused result process with the downstream @zip@ process would deadlock because we would reach a state where neither process could advance. To state this again: fusion fails when concurrent evaluation of the two source processes would deadlock.

% The main fusion algorithm here works on pairs of processes.
% When there are more than two processes, there are multiple orders in which the pairs of processes can be fused. The order in which pairs of processes are fused does not affect the output values, but it does affect the access pattern: the order in which outputs are produced and inputs read. Importantly, the access pattern also affects whether fusion succeeds or fails to produce a process. In other words, while evaluating multiple processes is non-deterministic, the act of fusing two processes \emph{commits} to a particular deterministic interleaving of the two processes. The simplest example of this has two input streams, a function applied to both, then zipped together. 



% -----------------------------------------------------------------------------
\section{Result Size}

As with any fusion system, we must be careful that the size of the result code does not become too large when more and more processes are fused together. The left of Fig.~\ref{fig:bench:outputsize} shows the maximum number of output states in the result when a particular number of processes are fused together in a pipelined-manner. To produce this graph we programmatically generated dataflow networks for \emph{all possible} pipelined combinations of the @map@, @filter@, @scan@, @group@ and @merge@ combinators, and tried all possible fusion orders consiting of adjacent pairs of processes. The @merge@ combinator itself has two inputs, so only works at the very start of the pipeline --- we present result for pipelines with and without a @merge@ at the start. The right of Fig.~\ref{fig:bench:outputsize} shows the number of states in the result when the various combinations of combinators are fused in parallel, for example, we might have a @map@ and a @filter@ processing the same input stream. In both cases the number of states in the result process grows linearly with the number of processes. In all combinations, with up to 7 processes there are less than 100 states in the result process. 
%%% AR: Note that the split version also has only one merge?

The size of the result process is roughly what one would get when inlining the definitions of each of the original source processes. This is common with other systems based on inlining and/or template meta-programming, and is not prohibitive.

On the other hand, Fig.~\ref{fig:bench:exponential} shows the results for a pathological case where the size of the output program is exponential in the number of input processes. The source dataflow networks consists of N merge processes, N+1 input streams, and a single output stream. The output of each merge process is the input of the next, forming a chain of merges. In source notation the network for N = 3 is @sOut = merge sIn1 (merge sIn2 (merge sIn3 sIn4))@.

When fusing two processes the fusion algorithm essentially compares every state in the first process with every state in the second, computing a cross product. During the fusion transform, as states in the result process are generated they are added to a finite map --- the @instrs@ field of the process definition. The use of the finite map ensures that identical states are always combined, but genuinely different states always make it into the result. 

In the worst case, fusion of two processes produces O($n*m$) different states, where $n$ and $m$ are the number of states in each. If we assume the two processes have about the same number of states then this is O($n^2$). Fusing the next process into this result yields O($n^3$), so overall the worst case number of states in the result will be O($n^k$), where $k$ is the number of processes fused. 

In the particular case of @merge@, the implementation has two occurrences of the @push@ instruction. During fusion, the states for the consuming process are inlined at each occurrence of @push@. These states are legitimately different because at each occurence of @push@ the input channels of the merge process are in different channel states, and these channel states are included in the overall process state.



\input{copy/03-body/02-process/figures/bench/process-size.tex}
