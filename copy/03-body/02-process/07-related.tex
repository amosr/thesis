%!TEX root = ../Main.tex
\chapter{Related work}
\label{related}

This chapter discusses related work on streaming and fusion.
Some of these points have been touched on previously; we now expand upon them.

\section{Fusion and streams for functional programs}
\label{related/fusion}
\label{related/stream-fusion}

This thesis aims to address the limitations of combinator-based stream fusion systems to execute multiple queries concurrently.
As explained in \cref{taxonomy}, neither pull-based or push-based streams are sufficient to execute multiple queries.
To execute multiple queries, we need to be able to share streams among multiple consumers, which push streams support, but pull streams do not.
However, for queries containing combinators with multiple inputs such as \Hs/zip/ and \Hs/join/, we can use pull streams, but not push streams.
% Some combinators are inherently push-based, particularly those with multiple outputs such as @unzip@; while others are inherently pull-based, such as @zip@.

The listlessness transform, an early form of fusion described by \citet{wadler1984listlessness}, can execute multiple queries concurrently under some circumstances; however, the transform is not guaranteed to terminate, and is known to diverge on relatively simple programs \citep{caspi1996synchronous}.
Deforestation \citep{wadler1990deforestation}, an extension of listlessness to support arbitrary recursive data types, addressed the problem of divergence by requiring the program to use each input data structure only once.
This linearity constraint equates to disallowing sharing of streams.

Shortcut fusion is an attractive idea, as it allows fusion systems to be specified by a single rewrite rule.
However, shortcut fusion relies on local inlining which, like pull-based streams, only occurs when there is a single consumer.
Thus, shortcut fusion is inherently biased towards pull fusion.
Push-based shortcut fusion systems \emph{do} exist \cite{gill1993short}, but support neither @zip@ nor @unzip@ \cite{svenningsson2002shortcut,lippmeier2013data}.

Recent work on stream fusion by \citet{kiselyov2016stream} uses staged computation in a pull-based system to ensure all combinators are inlined, but when streams are used multiple times this causes excessive inlining, which duplicates work.
For effectful inputs such as reading from the network, duplicating work changes the semantics.
% I could write more about this eg only supporting a single output, but the other points probably apply to push streams in general

Our previous work on data flow fusion~\cite{lippmeier2013data} is neither pull-based nor push-based, and supports stream sharing and combinators with multiple inputs.
It supports standard combinators such as @map@, @filter@ and @fold@, and converts each stream to a series with explicit rate types, similar to the clock types of Lucid Synchrone \cite{benveniste2003synchronous}.
These rate types ensure that well-typed programs can be fused without introducing unbounded buffers.
Unfusable programs trigger a compile-time error.
However, it only supports a limited set of combinators, and adding more combinators requires changing the fusion system itself.

One way to address the difference between pull and push streams is to explicitly support both separately using the polarised streams we saw in \cref{taxonomy/polarised}, as described by \citet{bernardy2015duality} and \citet{lippmeier2016polarized}.
% Here, pull streams have the type @Source@ and represent a source that is always available to be pulled from, while push streams have the type @Sink@ and represent a sink that can always accept input.
Both systems rely on stream bindings being used linearly to ensure correctness, including boundedness of buffers.
% Operations over pull streams are expressed fairly naturally compared to list operations, for example the @zip@ combinator has the type @Source a -> Source b -> Source (a,b)@.
% Sinks, however, are co-variant, and operations must be performed somewhat backwards, so that the @unzip@ combinator takes the two output sinks to push into and returns a new sink that pushes into these.
% It has the type @Sink a -> Sink b -> Sink (a,b)@.
% These systems require the streaming computation to be manually split into sources and sinks, and be joined together by a loop that `drains' values from the source and pushes them into the sink.
These systems require manual polarity analysis of the entire dependency graph, and require complex control flow because of the switching between pulling and pushing.

% The duality between pull and push arrays has also been explored in Obsidian by \citep{claessen2012expressive} and later in \citep{svensson2014defunctionalizing}.
% Here the distinction is made for the purpose of code generation for GPUs rather than fusion, as operations such as appending pull arrays require conditionals inside the loop, whereas using push arrays moves these conditionals outside the loop.

Streaming IO libraries have blossomed in the Haskell ecosystem, generally based on Iteratees \cite{kiselyov2012iteratees}.
Libraries such as Conduit \cite{hackage:conduit}, Enumerator \cite{hackage:enumerator}, Machines \cite{hackage:machines}, Pipes \cite{hackage:pipes} and Streaming \cite{hackage:streaming} are all designed to write stream computations with bounded buffers.
However, while these libraries provide boundedness guarantees, they provide no fusion guarantees, and as such programs tend to be written over chunks of data to make up for the communication overhead.
For the most part they support only straight-line computations with only limited forms of branching, while Streaming supports explicit duplication in a similar way to polarised streams.
We compared the runtime performance of some of these libraries earlier, in \cref{s:Benchmarks}.

The benefits of fusion have been known about for a long time, since at least the 1970s.
Jackson Structured Programming \citep{jackson2002jsp} is a design methodology where the structure of a program is derived using the structure of the input files to process.
Here, a method called ``program inversion'' performs a similar role to fusion, by removing intermediate results.
This method is similar to converting a pull computation into a push computation.
While these methods were generally performed by hand, the concept is similar to mechanised fusion.

\section{Tupling}
\label{related/tupling}
% This process of combining two folds into one is a simple instance of a transform known as \emph{tupling}.
% Transforms such as \cite{hu1997tupling,hu2005program,chiba2010program} can automatically perform tupling for some programs, but do not support combinators with multiple input streams such as \Hs/join/ or \Hs/append/.
% We discuss tupling further in \cref{related/tupling}.

\emph{Tupling} combines multiple traversals over a data structure into a single traversal.
Tupling is more general than stream fusion: it supports simplifying traversals of trees and other data structures, rather than just streams.
Two types of tupling are \emph{fold/unfold tupling} and \emph{hylomorphism-based tupling}.

Fold/unfold tupling, such as \citet{chiba2010program}, works by repeatedly unfolding or inlining a definition into its use site, performing some local rewrite-based optimisations, then re-folding the definition.
The unfolding may expose some simplification opportunities, which the local rewrite rules simplify away.
However, because the definitions to be unfolded are recursive, significant effort must be taken to ensure only \emph{finite} unfoldings are generated; for this reason, \citet{hu1997tupling} declare fold/unfold tupling to be impractical.

Hylomorphism-based tupling, such as \citet{hu1996cheap}, works by expressing traversals of the data structure as a \emph{hylomorphism}.
A hylomorphism describes how to generate some intermediate structure based on the input structure, as well as describing how to fold over the intermediate structure to compute the result.
The hylomorphism allows us to compute the result without generating the intermediate structure in full.
If two traversals of an input data structure can be expressed as folds over the same intermediate strucutre, both traversals can be computed together.
Automatic tupling algorithms attempt to automatically derive a hylomorphism for a given input data structure and traversal function, but these algorithms only work for a limited set of functions.
The algorithm in \citet{launchbury1995warm} is not total and cannot fuse a @zip@ combinator with both of its consumers.
The language in \citet{hu1996deriving} is restricted to ensure totality of the algorithm, but cannot express the data-dependent access pattern of the @join@ combinator.

% [talk about tupling: \cite{hu1996deriving,hu1996cheap,hu1996extension,bransen2014exploiting,launchbury1995warm}]

\section{Neumann push model}
\label{related/push-model}

The push streams described in \cref{taxonomy} are different from the push model used for database execution, as introduced in \citet{neumann2011efficiently}.
In an attempt to avoid confusion, we call this the \emph{Neumann push model}.
In the Neumann push model, a stream producer is represented as a continuation which takes a sink, or push function, to push values into:

\begin{haskell}
data PushModel a = PushModel ((a -> IO ()) -> IO ())
\end{haskell}

The consumer provides a sink by calling the continuation, then the producer repeatedly pushes all its values to the provided sink.
In this model, the consumer tells the producer when to start producing the entire stream: this is in contrast with pull streams, where the consumer asks for a single element at a time, and push streams, where the producer provides a single element at a time.
\citet{neumann2011efficiently} originally claimed that the Neumann push model was inherently more efficient than the pull model, but this claim used an unfair comparison between a compiled Neumann push model and an un-optimised pull model \cite{shaikhha2018push}.

The control-flow for the Neumann push model is the same as that of \emph{push arrays}, as described in \citet{claessen2012expressive} and \citet{svensson2014defunctionalizing}.
Here, push arrays are used as a code generation technique, with the main advantage of generating \emph{branchless} code to append two arrays.
The branchless version of append executes as two loops, one to read from each array, rather than one loop with a conditional branch inside to choose which input array to read from.

The control-flow is also the same as push-based shortcut fusion \cite{gill1993short}, as the consumer initiates the production loop.
Just as push-based shortcut fusion supports neither @zip@ nor @unzip@ \cite{svenningsson2002shortcut}; neither does the Neumann push model support combinators with multiple inputs except append; nor does it support executing multiple queries concurrently.

\citet{biboudis2017expressive} describes the advantage of this model when targeting the Java just-in-time (JIT) compiler, as it allows the producer to be implemented as a simple for-loop repeatedly calling the consumer function, which makes the JIT optimiser more likely to inline the consumer.


% The control-flow for the Neumann push model is the same as for \emph{push arrays}, as described in \citet{claessen2012expressive}.
% Like pull streams, the Neumann push model does not support executing multiple queries concurrently; unlike pull streams, the Neumann push model does not support combinators with multiple inputs except append.
% For now, we are interested in executing multiple queries; we defer further discussion of the Neumann push model to \cref{related/push-model}.


\section{Synchronised product and process calculi}
\label{related/synchronised-product}

In relation to process calculi, synchronised product has been suggested as a method for fusing Kahn process networks together~\cite{fradet2004network}, but there is no evidence that this has been implemented or evaluated.
The synchronised product of two processes allows either process to take independent or local steps at any time, but shared actions, such as when both processes communicate on the same channel, must be taken in both processes at the same time.
This fusion method is much simpler than ours, but is also much stricter.
When two processes share multiple channels, synchronised product will fail unless both processes read the channels in exactly the same order.
Our system can be seen as an extension of synchronised product that allows some leeway in when processes must take shared steps: they do not have to take shared steps at the same time, but if one process lags behind the other, it must catch up before the other one gets too far ahead.

It may be possible, in future work, to simplify our fusion system by preprocessing input processes to automatically insert some leeway for shared channels, before using synchronised product for fusion.
We believe that this leeway can, in fact, be inserted using synchronised product itself, but our experiments in this direction have been limited.

Like synchronised product and our process fusion, \emph{filter fusion}~\citep{proebsting1996filter} also statically interleaves the code of producer and consumer processes.
In filter fusion, each process can have at most a single input and output channel; common operators like \Hs@zip@, \Hs@unzip@, \Hs@append@, \Hs@partition@ and so on are not supported.
Given an adjacent producer and consumer pair, filter fusion alternately assigns control to the code of each.
When the consumer needs input, control is passed to the producer; when the producer produces its value, control is passed back to the consumer.
This simple scheduling algorithm works only for straight line pipelines of processes.
Both synchronised product and process fusion provide a finer-grained interleaving of code, which is necessary to support combinators with multiple input streams and multiple output streams.

\section{Synchronous languages}
\label{related/synchronous-languages}

Synchronous languages such as {\sc Lustre}~\cite{halbwachs1991synchronous}, Lucy-n~\cite{mandel2010lucy} and SIGNAL~\cite{le2003polychrony} all use some form of clock calculus and causality analysis to ensure that programs can be statically scheduled with bounded buffers.
These languages describe \emph{passive} processes where values are fed in to streams from outside environments, such as data coming from sensors.
In this case, the passive process has no control over the rate of input coming in, and if they support multiple input streams, they must accept values from them in any order.
In contrast, the processes we describe are \emph{active} processes that have control over the input that is coming in.
This is necessary for combinators such as mergesort-style @merge@, as well as @append@.
Note that in the synchronous language literature, it is common to refer to a different merge operation, also known as @default@, which computes a stream that is defined whenever either input is defined.

\section{Synchronous dataflow}
\label{related/synchronous-dataflow}

Synchronous dataflow (not to be confused with synchronous languages above) is a dataflow graph model of computation where each dataflow actor has constant, statically known input and output rates.
The main advantage of synchronous dataflow is that it is simple enough for static scheduling to be decidable, but this comes at a cost of expressivity.
StreamIt~\cite{thies2002streamit} uses synchronous dataflow for scheduling when possible, otherwise falling back to dynamic scheduling~\cite{soule2013dynamic}.
Boolean dataflow and integer dataflow~\cite{buck1993scheduling,buck1994static} extend synchronous dataflow with boolean and integer valued control ports, and attempt to recover the structure of ifs and loops from select and switch actors.
These systems allow some dynamic structures to be scheduled statically, but are very rigid and only support limited control flow structures: it is unclear how @merge@ or @append@ could be scheduled by this system.
Finite state machine-based scenario aware dataflow (FSM-SADF)~\cite{stuijk2011scenario,van2015scenario} is still quite expressive compared to boolean and integer dataflow, while still ensuring static scheduling.
A finite state machine is constructed, where each node of the FSM denotes its own synchronous dataflow graph.
The FSM transitions from one dataflow graph to another based on control outputs of the currently executing dataflow graph.
For example, a filter is represented with two nodes in the FSM.
The dataflow graph for the initial state executes the predicate, and the value of the predicate is used to determine which transition the FSM takes: either the predicate is false and the FSM stays where it is, or the predicate is true and moves to the next state.
The dataflow graph for the next state emits the value, and moves back to the first state.
This does appear to be able to express value-dependent operations such as @merge@, but lacks the composability --- and familiarity --- of combinators.

% StreamIt:
% Only allows limited splits and joins: round robin and duplication for splits, round robin and combination for joins. 
% Does not support fully general graphs - instead using combinators to introduce a (split/join) and a combinator for a feedback loop.
% 
% Parameterized dataflow (PDF),  \cite{bhattacharya2001parameterized}
% Schedulable parametric dataflow (SPDF),  \cite{fradet2012spdf}

% Recent work on stream fusion by \citet{kiselyov2016stream} uses staged computation to ensure all combinators are inlined, but for splits this causes excessive inlining which duplicates work, due to values of the source arrays being read multiple times.

