%!TEX root = ../Main.tex
% TODO: move to 02-process/???

% -----------------------------------------------------------------------------
\chapter{Future work}
\label{s:FutureWork}

%%% AR: Can we fit a few sentences here just to conclude, say we've described a fusion algorithm, with promising future work too

\section{Case analysis}
\label{s:FullyAbstractCase}

The fusion algorithm treats all @case@ conditions as fully abstract, by exploring all possible combinations for both processes.
This can cause issues for processes that dynamically require only a bounded buffer, but the fusion algorithm statically tries every combination and wrongly asserts that unbounded buffering is required.

For example, suppose two processes have the same case condition (@x > 0@).
The fusion algorithm will generate all four possibilities, including the contradictory ones such as where the first process has (@x > 0 = true@) and the second has (@x > 0 = false@).
If any require an unbounded buffer, the fusion algorithm will fail.

A possible extension is to somehow cull these contradictory states so that if it is statically known that a state is unreachable, it does not matter if it requires unbounded buffers.
It may be possible to achieve this with relatively little change to the fusion algorithm itself, by having it emit some kind of failure instruction rather than failing to produce a process.
A separate postprocessing step could then perform analyses and remove statically unreachable process states.
After postprocessing, if any failure instructions are reachable, fusion fails as before.


% -----------------------------------------------------------------------------
\section{Finite streams}
\label{s:Finite}

The processes we have seen so far deal with infinite streams, but in practice most streams are finite. Certain combinators such as @fold@ and @append@ only make sense on finite streams, and others like @take@ produce inherently finite output. We have focussed on the infinite stream version because it is simpler to explain and prove, but the extensions required to support finite streams do not require substantial conceptual changes.

Unlike infinite streams, pulling from a finite stream can fail, meaning the stream is finished. We therefore modify the @pull@ instruction to have two output labels: one to execute when a value is pulled, and the other to execute when the stream is finished. On the pushing end, we also need some way of finishing streams, so we add a new instruction to close an output stream.

During evaluation we need some way of knowing whether a stream is closed, which can be added as an extra constructor in the \InputState~ type. The same constructor is added to the static input state used by fusion. In this way, for any changes made to evaluation, the analogous static change must be made in the fusion transform.

For a combinator such as @take@ which only needs a finite prefix of the input stream, it may be useful to add an instruction that allows a pulling process to explicitly disconnect from an input channel. After disconnecting, the process no longer needs to be coordinated with the producer, leaving the producer free to push as often as the other consumers allow. We have implemented an early prototype that supports finite streams, our mechanized formalization does not cover it.

It is also possible to encode finite streams as infinite streams with an explicit end-of-stream marker (EOF) and case statements. However, this requires the fusion transform to analyse and reason about case statements and their predicates. It seems obvious that if two consumers read the same value and check if it is EOF, they are both going to give the same result, but this is of course undecidable in general. By making the structure of finite streams explicit and constraining processes to use finite streams in particular ways, we may be able to give more guarantees than by relying on heuristics for deciding equality of predicates. 

% We now describe the extensions required to support finite streams.
% We add a new @closed@ constructor to the \InputState~ to encode the end of the stream.
% Once an input stream is in the closed state, it can never change to another state: it remains closed thereafter.
% 
% We modify the @pull@ instruction so that it has two output labels (like @case@).
% The first label, the read branch, is executed as before when the pull succeeds and a value is read from the stream.
% The second label, the close branch, is executed when the stream is closed, and no more values will ever be available.
% After a pull takes the close branch, any subsequent pulls from that stream will also take the close branch.
% 
% We add two new instructions for closing output streams and disconnecting from input streams.
% Closing an output stream $(@close@~\Chan~\Next)$ is similar to pushing an end-of-file marker to all readers.
% As with @push@, the evaluation semantics of @close@ can only proceed if all readers are in a position to accept the end-of-file, but instead of setting the new \InputState~ to @pending@ with a value, the \InputState~ is set to @closed@.
% After a stream has been closed, no further values can be pushed.
% 
% Disconnecting from input streams $(@disconnect@~\Chan~\Next)$ signals that a process is no longer interested in the values of a stream.
% This can be used when a process requires the first values of a stream, but does not require the whole stream.
% If a process read the first values of a stream and then stopped pulling, its \InputState~ buffer would fill up and never be cleared, so no other process would be able to continue pulling from that stream.
% Disconnecting the stream allows other processes to use the stream without the disconnected process getting in the way of computation.
% The evaluation semantics for @disconnect@ remove the channel from the inputs of the process.
% After removing the channel from the inputs, when a writing process tries to inject values, this process will just be ignored rather than inserting into the \InputState~ buffer and potentially causing writing to block.
% After a process disconnects from an input channel, it can no longer pull from that channel.
% 
% We also add an instruction for terminating the process (@done@).
% After all input streams have been read to completion or disconnected and output streams closed, the process may execute @done@ to signal that processing is complete.
% 
% The fusion definition must be extended to deal with these new instructions.
% The static input state has a @closed@ constructor added and disconnection is encoded by removal from the input state, and the \ti{tryStep} changes more or less follow the evaluation changes.
% Shared and connected pulls now deal with two more possibilities in the input state: the input may be closed in which case the close branch of the pull is taken; or the other process may have disconnected in which case the pull is executed as in the non-shared non-connected case.
% Connected pushes must also deal with when the other process has disconnected in which case the push is executed as if it were non-connected.
% For @in1@ and @out1@ channels, the new @close@ and @disconnect@ instructions are used as normal with no coordination required.
% For connected @close@, as with @push@, the receiving process must have @none@ and the next step performs the @close@ and sets the input state to @closed@.
% For shared @disconnect@, the @disconnect@ is only performed after both processes have disconnected; otherwise the entry is just removed from the input state.
% For connected @disconnect@, the @disconnect@ is not performed and the entry is removed from the input state.
% 
% Finally, \ti{tryStepPair} is modified so that @done@ is performed when both machines are @done@.
% 
% These modifications allow our system to fuse finite streams as well as infinite.


% -----------------------------------------------------------------------------
% \section{Non-determinism and fusion order}
% \label{s:FusionOrder}
% The main fusion algorithm here works on pairs of processes.
% When there are more than two processes, there are multiple orders in which the pairs of processes can be fused. The order in which pairs of processes are fused does not affect the output values, but it does affect the access pattern: the order in which outputs are produced and inputs read. Importantly, the access pattern also affects whether fusion succeeds or fails to produce a process. In other words, while evaluating multiple processes is non-deterministic, the act of fusing two processes \emph{commits} to a particular deterministic interleaving of the two processes. The simplest example of this has two input streams, a function applied to both, then zipped together. 
% \begin{code}
% zipMap as bs =
%   let as' = map (+1) as
%       bs' = map (+1) bs
%       abs = zip as' bs'
%   in  abs
% \end{code}
% There are three combinators here, so after converting each combinator to its process there are three orders we can fuse. The two main options are to fuse the two maps together and then add the zip, or to fuse the zip with one of the maps, then add the other map. If we start by fusing the zip with one of its maps, the zip ensures that its inputs are produced in lock-step pairs, and then adding the other map will succeed. However if we try to fuse the two maps together, there are many possible interleavings: the fused program could read all of @as'@ first; it could read all of @bs'@ first; it read the two in lock-step pairs; or any combination of these. When the zip is added, fusion will fail if the wrong interleaving was chosen.

% The example above can be solved by fusing connected processes first, but it is possible to construct a connected process that still relies on the order of fusion.
% 
% \begin{code}
% zipApps as bs cs =
%   let as' = as ++ bs
%       bs' = as ++ cs
%       abs = zip as' bs'
%   in  abs
% \end{code}

% \section{Non-determinism and fusion order}
% \label{s:Future:FusionOrder}
% Our current solution to this is to try all permutations of fusing processes and use the first one that succeeds. A more principled solution may be to allow non-determinism in a single process by adding a non-deterministic choice instruction. Then when fusing two processes together, if both processes are pulling from unrelated streams, the result would be a non-deterministic choice between pulling from the first process and executing the first, or pulling from the second process and executing the second. In this way we could defer committing to a particular evaluation order until the required order is known. This may produce larger intermediate programs, but the same deterministic program could be extracted at the end.

\section{Early termination and resource cleanup}
What happens if a process does not care about the entire input stream, and only needs a prefix of the elements?
This is the case with @take 10@, which only cares about the first ten elements of the stream.
Another case is zipping two streams together --- where if the two streams are not the same length, the remainder of the longest is not required.
However, in the current formulation, the producer --- that is, the pushing process --- has no way of knowing whether any of its consumers are still interested.
As such, it must keep producing even when no consumers are still interested.

\begin{code}
takeAndDrop as = do
  asPrefix <- take 10 as
  perform print asPrefix
\end{code}

This program, which requires only the first ten (@as@), will keep looping until the entire input stream is depleted. Here is the fused process:

\begin{code}
takeAndDropFused as = process (l0 0).
 l0 i   = pull as (l1 i) l4
 l1 i a = case (i < 10) (l2 i a) (l3 i)
 l2 i a = lift (print a) (l3 i)
 l3 i   = drop as (l0 (i + 1))
 l4     = done
\end{code} 

Notice that label (@l1@) checks if the take has finished, and simply continues to loop over the input, draining it.
It is tempting to just stop executing the producer once all its consumers have disconnected, leaving it indefinitely blocked on a push.

\begin{code}
takeAndDropFusedStop as = process (l0 0).
 l0 i   = pull as (l1 i) l4
 l1 i a = case (i < 10) (l2 i a) l4
 l2 i a = lift (print a) (l3 i)
 l3 i   = drop as (l0 (i + 1))
 l4     = done
\end{code} 

In our modified process, once the (@take@) has finished with its input, it simply finishes processing by jumping to (@l4@).
This works for processes with a single output channel such as our example, but is not as straightforward when processes have multiple output channels.
Even when the original processes only have a single output channel, the intermediate fused process may have multiple output channels.

Let us consider an example with multiple outputs. Here, we take an input stream of pairs (@abs@) and @unzip@ them into two streams (@as@) and (@bs@).
For (@as@) we @take@ the first ten elements, and print them.
For (@bs@) we print all of the elements.

\begin{code}
takeAndRun abs = do
  (as, bs) <- unzip abs
  asPrefix <- take 10 as
  perform print asPrefix
  perform print bs
\end{code}

Here, once (@take@) has finished with (@as@), the pushing process (@unzip@) needs to keep pushing to (@bs@).
This would not work if (@unzip@) were blocked pushing to (@as@).

Another possibility is to modify the (@push@) instruction to have two output labels: the first is taken when pushing succeeds, and another for when pushing fails, as when there are no more consumers.

Show an example of a process using this. What is the best example process? Unzip would need three copies of the loop.
Also talk about the problem that disconnecting happens before you try to push, but you only learn about it once you push.
This means if you had a long pipeline and disconnected at the bottom, the next push from the second-bottom would fail and disconnect from its input.
Its producer would, in turn, only find out about the disconnect once it tried to push.
This sort of bubbling up means it would take many elements before the disconnect actually finishes: if the pipeline is five long, it would take five elements from the time the bottom-most consumer decides to disconnect, before the top producer learns about the disconnection.
This would be even worse if there are filters involved, as not every input element would be pushed.
It might also blow up the size of the program, since every case of disconnection needs to be handled differently.

More research and experience is needed to know whether this is a problem.
How often do these disconnections actually occur in practice?

\section{Size inference}
\label{s:Future:SizeInference}
\autoref{s:implementation:sizehints}
Talk about @vectorSizeIO@, how we might implement size inference to get rid of it.

