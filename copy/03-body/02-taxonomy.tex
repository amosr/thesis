\chapter{A brief taxonomy of streaming models}
\label{taxonomy}

In this thesis, we write queries as \emph{streaming programs} so that we may query large datasets without running out of memory.
Streaming programs consume data from their input streams element by element, processing the elements in sequential order, and need only store a limited number of elements at a time as local state.
A streaming program cannot rewind an input stream to reread previous elements, or perform random access to read from an arbitrary element in the stream.
These restrictions mean that a streaming program cannot, for example, sort all the input data in a single pass over the input stream, because single-pass sorting requires storing all the elements in memory.
The upside of these restrictions is that if we can write our queries as streaming programs, we can be confident that they will run in constant space --- no matter how large the input stream is.
In general, input streams may be infinite, though in this thesis we focus on large but finite streams.

% With just this definition one could go and write a streaming program in more or less any language with input-output facilities.
% This is how a C programmer may implement a streaming program.
% Take, for example, the standard unix tool \Hs/wc/.
% Conceptually, this program does four things: it reads a file, counts the lines, counts the words, and counts the characters.
% One might expect these four distinct concepts to be clearly distinct in the code, but because
% Writing streaming programs in this manner can be error-prone and tedious however, as all aspects of the program must be interspersed.

Streaming, as described above, is a rather general concept.
This definition tells us what a streaming program is, but it does not offer any guidance how to write streaming programs.
In fact, there are many ways to write streaming programs; in this thesis we restrict our attention to streaming programs written in a \emph{functional style}.
The functional style of writing streaming programs involves using small stream transformers that are composed to create larger programs.
The benefit of this style is that each stream transformer can be reasoned about and tested in isolation with no hidden dependencies between them.

There are numerous \emph{streaming models} to choose from, and we must commit to a particular model before we can start writing programs.
Choosing a streaming model requires making a trade-off between the performance overhead, which operations are supported, and the amount of extra bookkeeping the programmer must perform to write their program compared to a non-streaming implementation.
% The streaming model determines which stream operations are supported including how streams can be connected together, and the overhead incurred when connecting streams together.
% \ED{A pedantic note on terminology: the distinction between \emph{model} and \emph{representation} is that the model includes information about how the overhead is removed.}
We must compare different streaming models to make an informed decision.
We start our initial comparison by focussing on two low-overhead streaming models to illustrate how they support different operations, and to motivate the use of Kahn process networks as a streaming model.
In \cref{s:Benchmarks}, we will compare with some more expressive but less efficient streaming models.

\section{Gold panning}
\label{taxonomy/gold-panning}
% Even with the bounded memory and sequential access restrictions of streaming, we can still write interesting queries as streaming programs.
Let us start by describing a situation in which we would like to execute many queries over the same dataset.
To avoid mixing up the details of streaming with the details of the example, we initially assume that the dataset fits in memory as a list and ignore details about efficiency.
Throughout the thesis, we will refer back to this example as \emph{gold panning}.

Suppose we have a file containing the historical prices for a particular corporate stock.
The file contains many records; each record contains a date and the average price for that day, and all the records in the file are sorted chronologically.
The records are stored on-disk in comma-separated values (CSV) format, and are represented in memory by the following Haskell datatype:

\begin{haskell}
data Record = Record
 { time  :: Time
 , price :: Double }
\end{haskell}

We wish to appraise this stock to see whether it was, historically, a worthy investment.
One quality of a good investment is that its price increases over time; we can quantify any increase by computing the linear regression of the price over time, using the coefficient of the line to approximate increase or decrease over time.
It is very convenient to be able to summarise growth with one number, but stock prices rarely act as lines.
While a line might be a good approximation for a stable stock price with few dips and bumps, it is a poor approximation for an unstable stock.
Fortunately, we can use a statistical tool called the \emph{Pearson correlation coefficient} to determine how linear the relationship is, and therefore how good the approximation is --- which may be valuable information about the stock price in itself as well as denoting the confidence of our analyses.
The Pearson correlation coefficient is defined as the covariance of price with time, divided by the product of the standard deviation of price and the standard deviation of time.
We can also define the Pearson correlation coefficient geometrically: it is the cosine of the angle between the regression line of price over time, and the regression line of time over price.

\input{figs/stock-a.tex}

\begin{haskell}[float,label=figs/impl/correlation-multi,caption=Multiple-pass correlation implementation]
correlation :: [(Double,Double)] -> Double
correlation xys = covariance xys / stddev (map fst xys) * stddev (map snd xys)

stddev :: [Double] -> Double
stddev xs = sqrt (mean (map (^2) xs) / mean xs * mean xs)

covariance :: [(Double,Double)] -> Double
covariance xys =
 let xy = map (\(x,y) -> x * y) xys
 in mean xy - mean (map fst xys) * mean (map snd xys)

mean :: [Double] -> Double
mean xs = sum xs / fromIntegral (length xs)
\end{haskell}

\begin{haskell}[float,label=figs/impl/correlation,caption=One-pass correlation implementation]
type State = (Double, Double, Double, Double, Double, Double)
correlation_z :: State
correlation_z = (0,0,0,0,0,0)

correlation_k :: State -> (Double,Double) -> State
correlation_k (mx, my, sd, sdX, sdY, n) (x,y) =
 let n'   = n   + 1
     dx   = x   - mx
     dy   = y   - my
     mx'  = mx  + (dx / n')
     my'  = my  + (dy / n')
     dx'  = x   - my'
     dy'  = y   - my'
     sd'  = sd  + dx * dy'
     sdX' = sdX + dx * dx'
     sdY' = sdY + dy * dy'
 in (mx',my',sd',sdX',sdY',n')

correlation_x :: State -> Double
correlation_x (mx, my, sd, sdX, sdY, n) =
  let varianceX  = sdX / n
      varianceY  = sdY / n
      covariance = sd  / n
      stddevX = sqrt varianceX
      stddevY = sqrt varianceY
  in covariance / (stddevX * stddevY)
\end{haskell}


\Cref{fig:stock:it:stock-over-time} shows the fluctuations of the example stock's price over a year, along with the regression line of price over time in red, and the regression line of time over price in blue.
The stock price is far from a perfect line, but does show a clear upwards trend.
% The correlation coefficient is a scalar ranging from negative one to positive one.
In this graph, the correlation is represented by the angle between the red and blue regression lines; the smaller the angle between the two regression lines, the more closely correlated the two are, and the `straighter' the relationship is.
The angle here corresponds to a correlation of $0.94$, in a range from negative one to positive one.

% We can roughly quantify whether or not price increases over time by computing the Pearson correlation coefficient of the two.
% The Pearson correlation coefficient represents how well the relationship can be approximated by a linear function and ranges from negative one to one; a correlation of one denotes that price grows linearly with time, while a correlation of negative one denotes that price decreases linearly with time.
% When the relationship is non-linear but does still tend to increase over time, the correlation will be a positive number between zero and one.

\cref{figs/impl/correlation-multi} shows a direct implementation of the correlation coefficient.
This implementation performs multiple passes over the input list, and is not numerically stable.
We can implement a more stable one-pass correlation algorithm using the covariance algorithm specified in \citet{welford1962note}.
Although the details are quite complicated, we can express this algorithm as a fold over a list.
The fold uses an initial state, \Hs/correlation_z/, and for each element updates the state with a worker function \Hs/correlation_k/.
The one-pass correlation algorithm keeps track of the running means and standard deviations of both axes, which are used to compute the correlation.
As such, the fold state contains more than just the correlation.
After the fold has completed, we perform an \emph{extraction} function, \Hs/correlation_x/, to extract the correlation from the state.

\Cref{figs/impl/correlation} contains the implementations of the fold worker functions for computing the correlation: \Hs/correlation_x/, \Hs/correlation_k/ and \Hs/correlation_z/.
With these worker functions, we can compute the correlation as follows:

\begin{haskell}
correlation :: [(Double,Double)] -> Double
correlation = correlation_x (foldl correlation_k correlation_z)
\end{haskell}


We can implement a function to compute the regression similarly; we omit the definitions of the regression worker functions.
The definition of the fold uses the fold worker functions \Hs/regression_x/ for extracting the final result, \Hs/regression_z/ for the initial state, and \Hs/regression_k/ to update the state for every input element:

\begin{haskell}
regression :: [(Double,Double)] -> Line
regression = regression_x (foldl regression_k regression_z)
\end{haskell}

Now that we have functions to compute the linear regression and the correlation, we can compute both at the same time.
The following program returns a pair containing the correlation and regression:

\begin{haskell}
priceOverTime :: [Record] -> (Line, Double)
priceOverTime stock =
  let timeprices = map (\r -> (daysSinceEpoch (time r), price r)) stock
  in (regression timeprices, correlation timeprices)
\end{haskell}

Both \Hs/regression/ and \Hs/correlation/ functions take a list of pairs of numbers, so we first convert the \Hs/Record/ values to pairs of numbers using \Hs/map/.
Although this is a single program, it computes two values.
Whether we think of this program as one query or two is inconsequential; the important part is that this program, as it is written, requires two traversals over the \Hs/timeprices/ list.
List programs can traverse the same list many times; in \cref{taxonomy/pull} we shall see how multiple traversals is a problem for streaming programs.

Stock prices rarely follow linear functions of time; even the best stocks go down once in a while, and sometimes the market as a whole can go down.
Furthermore, even though this stock appears to be doing quite well if we consider it in isolation, we do not know whether it is an exceptional stock or an exceptional market.
We are interested in comparing against the rest of the market as well.

To compare against the rest of the market, we have another file of records containing the average price of a representative subset of stocks.
This representative subset is called a \emph{market index}.
We want to compare each day's price for our stock against the average price for the corresponding day in the index.

\input{figs/stock-bc.tex}

\Cref{fig:stock:it:market-over-time} shows the linear regression and correlation of the market index price over time, while \cref{fig:stock:it:stock-over-market} shows the linear regression and correlation of the stock price from \cref{fig:stock:it:stock-over-time} compared to the market index price.
The comparison of stock price to market index price compares each day's stock price against the corresponding day's market index price.
Each day is visualised as the percentage difference between the day's stock price and the mean stock price for the period for the y axis, compared to the percentage difference between the day's market index price and the mean market index price for the period for the x axis.
Both regression lines are slightly steeper than forty-five degrees, indicating that the stock price has grown faster than the market index price.
The correlation, represented by the angle between the two regression lines, indicates a relatively strong linear relationship between the stock price and the market index price.

We can compute the comparison of stock price over market index price with the following program:

\begin{haskell}
priceOverMarket :: [Record] -> [Record] -> (Line, Double)
priceOverMarket stock index =
  let joined = join (\s i   -> time s `compare` time i) stock index
      prices = map  (\(s,i) -> (price s, price i))      joined
  in (regression prices, correlation prices)
\end{haskell}

The \Hs/join/ operator matches each stock day against the corresponding market index day, and discards the data for any days missing from either input.
We then extract both prices from the joined result and compute the regression and correlation.
As with \Hs/priceOverTime/, this function requires two traversals of the \Hs/prices/ list.

Since the analyses \Hs/priceOverTime/ and \Hs/priceOverMarket/ both provide useful information, we will perform both.
It is just as easy to combine these queries together as it was to compute both the correlation and the regression.
The following program computes both:

\begin{haskell}
priceAnalyses :: [Record] -> [Record] -> ((Line, Double), (Line, Double))
priceAnalyses stock index =
  let pot = priceOverTime   stock
      pom = priceOverMarket stock index
  in (pot, pom)
\end{haskell}

% \input{figs/priceOverTime-priceOverMarket.tex}
\FigurePdfLabel{figs/depgraphs/priceOverTime-priceOverMarket}{figs/procs/priceOverTime-priceOverMarket}{Dependency graph for queries \Hs/priceOverTime/ and \Hs/priceOverMarket/}

\Cref{figs/procs/priceOverTime-priceOverMarket} shows the dependency graph for both queries.
The nodes in this graph are the two input lists \Hs/stock/ and \Hs/index/, and the list operators in each query.
The two input lists are drawn inside boxes to distinguish them from the operator nodes.
% each intermediate list, and the \Hs/correlation/ and \Hs/regression/ functions which summarise the list values.
The edges are dependencies from one value to another; the \Hs/join/ operator uses both the \Hs/stock/ and \Hs/index/ lists, so there are arrows from both \Hs/stock/ and \Hs/index/ to the \Hs/join/ node.
Edges that denote intermediate lists between two operators are labelled with the variable name used in the list program.
Below the bottom of the graph, not shown, the results of the four \Hs/regression/ and \Hs/correlation/ operators are paired together to construct the return value.
The large boxes bisecting most of the nodes denote which nodes are defined inside the \Hs/priceOverTime/ function and which are defined in \Hs/priceOverMarket/.
This dependency graph is a directed acyclic graph: the nodes \Hs/stock/, \Hs/timeprices/, and \Hs/prices/ have multiple children; \Hs/joined/ has multiple parents.
Having multiple children means a list is mentioned multiple times, which generally corresponds to requiring multiple traversals of the list in a sequential evaluation.

Although a sequential evaluation of this list program requires multiple traversals of the input, we \emph{can} rewrite it to be a single-pass streaming program.
Our choice of streaming model dictates how difficult this rewrite will be.

\section{Pull streams}
\label{taxonomy/pull}

The first streaming model we look at are \emph{pull streams}, which are also sometimes called iterators or cursors.
The essence of a pull stream is that a consumer can \emph{pull} on it to gain the next value.
We represent a pull stream as a function with no parameters which either returns \Hs/Just/ a value, or returns \Hs/Nothing/ when the stream is finished.
Since the function may need to read from a file or update some local state, it is expressed as an \Hs/IO/ computation:

\begin{haskell}
data Pull a = Pull (IO (Maybe a))
\end{haskell}

With this stream representation, we can implement analogues of the list combinators used in the example queries.
We can map a function over a pull stream like so:

\begin{haskell}
map :: (a -> b) -> Pull a -> Pull b
map a_to_b (Pull pull_a) = Pull pull_b
 where
  pull_b = do
    maybe_a <- pull_a
    return (case maybe_a of
             Nothing -> Nothing
             Just a  -> Just (a_to_b a))
\end{haskell}

Between unwrapping and wrapping the \Hs/Pull/ constructor, the \Hs/map/ function takes a function \Hs/pull_a/ to compute the input stream values, and returns a function \Hs/pull_b/ to compute the transformed stream values.
Whenever the consumer of \Hs/map/ calls \Hs/pull_b/ to ask for the next value, \Hs/pull_b/ in turn calls \Hs/pull_a/ asking for the next value.
When the stream is not finished, we apply the transform function \Hs/a_to_b/ to the pulled element and return the transformed element.
In pull streams, consumers ask producers for the next value, and control flow bubbles up from consumer to producer.

We can also implement \Hs/foldl/.
Because pull streams can perform effects such as reading from a file, the result type for \Hs/foldl/ is now wrapped in \Hs/IO/:

\begin{haskell}
foldl :: (b -> a -> b) -> b -> Pull a -> IO b
foldl k z (Pull pull_a) = loop z
 where
  loop state = do
    maybe_a <- pull_a
    case maybe_a of
      Nothing -> return state
      Just a -> loop (k state a)
\end{haskell}

This implementation of \Hs/foldl/ calls the local function \Hs/loop/ with the initial state of \Hs/z/.
The \Hs/loop/ function repeatedly pulls from the pull function, \Hs/pull_a/, updating the state for every element.

Consuming a stream is an effectful operation.
Every time we call the pull function we get the next element, which means the pull function must somehow keep track of which value it is up to.
For example, a pull function which reads from a file holds a file-handle, which in turn references some mutable state about the file offset.
Every time we read from the file, the file offset is incremented.
If two consumers were to ask the same pull function for the next input one after another, they would get different elements of the stream.


\begin{haskell}[float,caption=Pull stream combinators,label=figs/impl/pull/combinator]
correlation :: Pull (Double,Double) -> IO Double
regression  :: Pull (Double,Double) -> IO Line

join        :: (a -> b -> Ordering) -> Pull a -> Pull b -> Pull (a,b)
join comparekey (Pull pull_a) (Pull pull_b) = Pull (do
   a <- pull_a
   b <- pull_b
   go a b)
 where
  go (Just a) (Just b)
   = case comparekey a b of
      EQ -> return (Just (a,b))
      LT -> do
        a' <- pull_a
        go a' b
      GT -> do
        b' <- pull_b
        go a b'
  go _ _ = return Nothing
\end{haskell}


\Cref{figs/impl/pull/combinator} shows the type signatures of the pull stream versions of \Hs/regression/ and \Hs/correlation/, as well as the implementation of the \Hs/join/ combinator.
The \Hs/correlation/ and \Hs/regression/ functions can be implemented much like their list versions, using the pull implementation of \Hs/foldl/.

The \Hs/join/ function executes by reading a value from each input stream and comparing the values using the given comparison function.
Both input streams are sorted by some key, which the comparison function extracts and compares.
If the keys are equal, \Hs/join/ returns the pair.
Otherwise, \Hs/join/ pulls again from the input stream with the smaller key: since both streams are sorted by the key, if one stream has a higher key than the other, it means the stream with the higher key does not have a corresponding value for the smaller key.
% if the other stream has a higher key it means the other stream does not have a corresponding value for the smaller key.
In our \Hs@priceOverMarket@ example, the files are sorted by date and the comparison function compares the dates.
We can join the two files in a streaming manner because both input files are already sorted by date; if the files were not sorted by date, we would need to perform a non-streaming join, for example a hash-join, which stores the entirety of one input in a hashtable in memory.
% eg keep all of one input in memory and perform a hash-join, or read the file multiple times.
% The join function takes the two input streams, as well as functions to extract the key from each input record.

We cannot naively translate the list version of \Hs/priceOverTime/ to use these streaming combinators, because the list version required multiple traversals.
The following program will not compute the correct result because it uses the \Hs/timeprices/ stream twice:

\begin{haskell}
priceOverTime_pull_bad :: Pull Record -> IO (Line, Double)
priceOverTime_pull_bad stock = do
  let timeprices = Pull.map (\r -> (daysSinceEpoch (time r), price r)) stock
  r <- Pull.regression  timeprices
  c <- Pull.correlation timeprices
  return (r, c)
\end{haskell}

Computing the regression pulls all the values from the \Hs/timeprices/ stream and folds over them until the stream is exhausted.
After computing the regression, the program computes the correlation of the same input stream.
When the correlation tries to read the \Hs/timeprices/ stream again, the stream has already been exhausted.
For this reason, we say that pull streams cannot be used multiple times.
This single-use restriction also exists in other streaming models, such as Java 8 Streams \citep{reese2014java} and Strymonas \citep{biboudis2017expressive}.
Some streaming models, such as Iteratees \citep{kiselyov2012iteratees}, do not explicitly state any single-use restriction, but exhibit the same potentially surprising behaviour when multiple consumers compete for elements from the same stream.
In \cref{taxonomy/polarised} we shall see a streaming model that is more strict and requires that streams be used exactly once and cannot be discarded (\emph{linearly}).

Since our input streams are sourced from files stored on disk, we could imagine \emph{rewinding} the input streams and re-reading the files again from the start.
If we were to rewind the stream, all the effects and all the work that went into computing the stream the first time would have to be done a second time.
Rewinding would allow this program to compute the correct result, but it is only feasible for \emph{persistent} input streams.
Real-time inputs such as sensor data may accumulate too quickly to be stored indefinitely, and for network-backed storage, the extra communication time may make performing multiple reads impractical.

Fortunately, because \Hs/regression/ and \Hs/correlation/ are both computed by folds, we can combine the two into a single fold.
In the following program, the fold worker function \Hs/both_k/ and seed \Hs/both_z/ compute both regression and correlation at the same time:

\begin{haskell}
regressionCorrelation_pull :: Pull (Double,Double) -> IO (Line, Double)
regressionCorrelation_pull stream = do
  (r,c) <- Pull.foldl both_k both_z stream
  return (regression_x r, correlation_x c)
 where
  both_k (r,c) v = (regression_k r v, correlation_k c v)
  both_z         = (regression_z,     correlation_z)

priceOverTime_pull :: Pull Record -> IO (Line, Double)
priceOverTime_pull stock = do
  let timeprices = Pull.map (\r -> (daysSinceEpoch (time r), price r)) stock
  regressionCorrelation_pull timeprices
\end{haskell}

This program computes the correct value.
% The semantic meaning of this program is no more complicated than the list version, but the complexity is not as well hidden as in the list version.
To write this version, we have had to manually look inside the definitions of \Hs/correlation/ and \Hs/regression/ and duplicate them.
This was relatively easy because both use-sites were folds.
This process of combining two folds into one is a simple instance of a transform known as \emph{tupling}.
Transforms such as \cite{hu1997tupling,hu2005program,chiba2010program} can automatically perform tupling for some programs, but do not support combinators with multiple input streams such as \Hs/join/ or \Hs/append/.
We discuss tupling further in \cref{related/tupling}.


% If the input data is stored in a file, we have the option or re-reading the input file again, but this also requires performing any parsing again --- duplicating work.
% We could also store the values in a buffer and traversing the buffer multiple times, but this requires knowing how large the stream will be.
% In general, if we want to re-use the same stream multiple times we have the choice between duplicating work or buffering the values somewhere.

Let us turn our attention to the second query, \Hs/priceOverMarket/.
We can use the same function \Hs/regressionCorrelation_pull/ that we used above, like so:

\begin{haskell}
priceOverMarket_pull :: Pull Record -> Pull Record -> (Line, Double)
priceOverMarket_pull stock index =
  let joined = Pull.join (\s i   -> time s `compare` time i) stock index
  let prices = Pull.map  (\(s,i) -> (price s, price i))      joined
  regressionCorrelation_pull prices
\end{haskell}

We now have pull stream implementations of both \Hs/priceOverTime/ and \Hs/priceOverMarket/, but when we wish to compute both at the same time, we cannot simply pair them together as we did in the list implementation of \Hs/priceAnalyses/ --- this time because the \Hs/stock/ stream is mentioned multiple times.

When we implemented the pull stream version of \Hs/priceOverTime/, we had to look at the two occurences where the \Hs/timeprices/ stream had been used.
We had to inline both places where the stream was used and manually write a new function to do the work of both.
Both were fairly simple folds.
Doing the same for \Hs/priceAnalyses/ is more complicated: we would need to implement a special version of the \Hs/join/ combinator used inside \Hs/priceOverMarket/, which not only joins the two input streams together, but also computes the regression and correlation of its stock stream at the same time.

It might appear that, since the \Hs/joined/ stream contains pairs from both \Hs/stock/ and \Hs/index/, we could use this to compute the correlation and regression of the the \Hs/stock/ component alone.
Such a query would be easier to combine with \Hs/priceOverMarket/, but this query would compute a different result, since the \Hs/joined/ stream only contains elements from \Hs/stock/ for which corresponding days exist in the \Hs/index/ stream.

Pull streams are not helping us execute multiple queries at a time.
If we wish to execute multiple queries in a single-pass, we need to be able to mention streams multiple times.
To execute these shared streams, each time we read from a shared stream, we need some way to distribute this element among all of the shared stream's consumers.

\subsection{Streaming overhead}
\label{taxonomy/pull/streaming-overhead}

The pull stream representation can incur some overhead due to the need to wrap elements in \Hs@Maybe@ constructors.
For simple combinators such as map, however, the overhead can be optimised away.
Consider the following function, which applies two worker functions to the elements in a stream:

\begin{haskell}
map2 :: (a -> b) -> (b -> c) -> Pull a -> Pull c
map2 f g stream_a
 = let stream_b = Pull.map f stream_a
       stream_c = Pull.map g stream_b
   in  stream_c
\end{haskell}

We could write this program in an equivalent way by composing the two functions together and performing a single map: \Hs/Pull.map (g . f) stream_a/.
Fortunately, after some optimisation, both programs incur the same amount of overhead.
To demonstrate concretely the overhead of composing stream transformers, we take the definition of \Hs/Pull.map/ and inline it into the use-sites in \Hs@stream_b@ and \Hs@stream_c@ above.
After removing some wrapping and unwrapping of \Hs/Pull/ constructors, we have the following function:

\begin{haskell}
map2 f g (Pull pull_a) = Pull pull_c
 where
  pull_b = do
    a <- pull_a
    return (case a of
             Nothing -> Nothing
             Just a' -> (Just (f a')))
  pull_c = do
    b <- pull_b
    return (case b of
             Nothing -> Nothing
             Just b' -> (Just (g a')))
\end{haskell}

When we pull from \Hs@pull_c@, it asks \Hs@pull_b@ for the next element, which in turn asks \Hs@pull_a@.
When there is a stream element to process, \Hs@pull_a@ constructs a \Hs@Just@ containing the value and returns it to \Hs@pull_b@.
This \Hs@Just@ is then destructed by \Hs@pull_b@ so the function \Hs@f@ can be applied to the element, before wrapping the result in a new \Hs@Just@ which is returned to \Hs@pull_c@.
Now, \Hs@pull_c@ must perform the same unwrapping and wrapping on the returned value, even though we statically know that when \Hs@pull_a@ returns a \Hs@Just@, \Hs@pull_b@ also returns a \Hs@Just@.

To take advantage of this knowledge and remove the superfluous wrapping and unwrapping, we first transform the program by inlining \Hs@pull_b@ into where it is called in \Hs@pull_c@.
Then, using the monad laws, we can rewrite the \Hs@return@ statement containing the case expression from \Hs@pull_b@, nesting this case expression inside the scrutinee of the other case expression:

\begin{haskell}
map2 f g (Pull pull_a) = Pull pull_c
 where
  pull_c = do
    a <- pull_a
    return (case (case a of
                   Nothing -> Nothing
                   Just a' -> (Just (f a')))
             Nothing -> Nothing
             Just b' -> (Just (g b')))
\end{haskell}

The nested case expression returns statically-known constructors of \Hs@Nothing@ or \Hs@Just@, which the outer case expression immediately matches on.
We remove the intermediate step using the \emph{case-of-case} transform \cite{jones1998transformation}, which converts these nested case expressions to a single case expression:

\begin{haskell}
map2 f g (Pull pull_a) = Pull pull_c
 where
  pull_c = do
    a <- pull_a
    return (case a of
             Nothing -> Nothing
             Just a' -> (Just (g (f b))))
\end{haskell}

By applying some standard program transformations, the two maps are combined into one, removing the overhead of additional constructors.
Optimising compilers perform similar transforms as part of their suite of general purpose optimisations \citep{jones1996compiling}.

For other operations, the streaming overhead can be harder to remove.
\Cref{figs/impl/pull/filter} shows the implementation of the \Hs/filter/ combinator.
The function \Hs/pull_a'/ pulls an element and checks whether it satisfies the given predicate.
If the element does not satisfy the predicate, \Hs/pull_a'/ performs a recursive loop to check the next element.
The recursion in this function makes it harder for the compiler to inline and optimise, and the overhead may not be removed. % : for example, inlining recursive functions must be done carefully to avoid non-termination.
Stream fusion \citep{coutts2007stream}, which uses a more sophisticated pull stream representation, allows pull functions to return a \emph{skip} constructor in place of filtered-out elements, instructing the consumer to pull again.
The stream fusion implementation of \Hs/filter/ instructs the consumer to pull the next element when the predicate fails, instead of recursively looping.
Compilers that use an intermediate language based on sequent calculus may be able to inline and optimise recursive functions directly, without needing to convert stream operations to non-recursive functions \citep{maurer2017compiling}.
We discuss other representations of pull streams in \cref{related/fusion}.
These other representations of pull streams, as well as the representation used by stream fusion, do not afford extra expressivity over the pull streams described above; the same set of combinators can be implemented with the same asymptotic space and time behaviour and improved constant factors.

\begin{haskell}[float,caption=Pull implementation of \Hs/filter/,label=figs/impl/pull/filter]
filter :: (a -> Bool) -> Pull a -> Pull a
filter predicate (Pull pull_a) = Pull pull_a'
 where
  pull_a' = do
    a <- pull_a
    case a of
     Nothing -> return Nothing
     Just a'
      | predicate a' -> return (Just a')
      | otherwise    -> pull_a'
\end{haskell}

% more sophisticated representations of pull streams, such as Stream Fusion~\cite{coutts2007stream}, that can remove this overhead.


\section{Push streams}
\label{taxonomy/push}

\emph{Push streams} are the conceptual dual of pull streams: rather than the consumer pulling elements from the producer, in push streams the producer pushes elements to the consumer.
As we shall see, the advantage of push streams is that they enable stream elements to be shared among multiple consumers: a producer can push the same value to multiple consumers.
This sharing of elements makes it easier to perform multiple queries over the same input stream.

A push stream is a function which accepts a (\Hs/Maybe a/) and performs some \Hs/IO/ effect, for example writing to a file, or writing to some mutable state.
This could be represented by the type (\Hs/Maybe a -> IO ()/), which is the dual of the pull stream (\Hs/IO (Maybe a)/).
However, this representation provides no direct way to retrieve a result from a consumer: for example, the return value of our correlation or regression.
This is a common enough use-case that it justifies a departure from the conceptual clarity of using the exact dual.
We instead use the following representation:

\begin{haskell}
data Push a r = Push
  { push :: a -> IO ()
  , done :: IO r }
\end{haskell}

We augment the definition with an extra type parameter, \Hs/r/, for the result type.
Since the result only becomes available at the end of the stream, we separate the two cases of the (\Hs/Maybe a/) argument into two functions, \Hs/push/ and \Hs/done/.
When we have a value we call \Hs/push/.
When the stream is finished we call \Hs/done/ to retrieve the result.

% Most consumers also have some initialisation that needs to be performed, for example opening an output file or constructing a mutable reference in which to accumulate values.
% In the \Hs/Push/ type, the \Hs/init/ function performs initialisation and returns a \Hs/PushRep/.
% \begin{haskell}
% data Push a r = Push
%   { init :: IO (PushRep a r) }
% \end{haskell}



In this representation, it is the consumers that are values of type (\Hs/Push a r/): they are sinks into which we can push values of type \Hs/a/, and eventually get an \Hs/r/ back.
This inversion of control for pull streams leads to a fundamental difference in how we program with push streams, and what we can express with push streams.

The push streams described here are analogous to the \emph{sinks} described in \citet{bernardy2015duality} and \citet{lippmeier2016polarized}, albeit with a slightly different representation.
In this thesis, we use the push/pull terminology of \citet{kay2009you}.
However, the `push' in `push streams' is different from the `push' in the `push model' used for database execution, as described in \citet{neumann2011efficiently}.
In the \emph{Neumann push model}, a stream producer is represented as a continuation which takes a sink to push values into.
Once the consumer provides a sink, the producer repeatedly pushes all its values to the provided sink.
The control-flow for the Neumann push model is the same as for \emph{push arrays}, as described in \citet{claessen2012expressive}.
% Push arrays can be represented as a continuation which receives a push stream as an argument, and are useful for optimising code-generation for append operations.
Like pull streams, the Neumann push model does not support executing multiple queries concurrently; unlike pull streams, the Neumann push model does not support combinators with multiple inputs except append.
We discuss the Neumann push model further in \cref{related/push-model}.


We cannot map a function over the elements in push streams in the way that we would with lists or pull streams, because the definition of (\Hs/Push a r/) uses the stream element type \Hs/a/ as the input to a function.
Instead, we implement contravariant-map, or \Hs/contramap/, like so:

\begin{haskell}
contramap :: (a -> b) -> Push b r -> Push a r
contramap a_to_b bs = Push push_a done_a
 where
  push_a a = push bs (a_to_b a)
  done_a   = done bs
\end{haskell}

The \Hs/contramap/ function takes a function to convert values of type \Hs/a/ to values of type \Hs/b/ and a sink to push values of type \Hs/b/ to, returning a sink which can receive values of type \Hs/a/.
When a producer tries to push an input value into the returned stream, the \Hs@push_a@ function converts this to a value of type \Hs@b@ and pushes it further on to the consumer of \Hs@b@.
Unlike with pull streams, a push consumer has no way of choosing among multiple inputs.
The producer is in control while the consumer passively waits for its next input value.

We \emph{do} in fact have a regular (covariant) map function for push streams, but this transforms the stream result rather than the input elements:

\begin{haskell}
map_result :: (r -> r') -> Push a r -> Push a r'
map_result r_to_r' push_a = Push (push push_a) done_a'
 where
  done_a' = do
    r <- done push_a
    return (r_to_r' r)
\end{haskell}

The type of \Hs/foldl/ for push streams is similar to pull streams, except instead of taking the pull stream to read from, it returns a push stream which will eventually return the result.
The return value is in \Hs/IO/ because we use a mutable reference to store the current state, which must be allocated before returning the stream.
As values are pushed into the sink, the mutable reference containing the fold state is updated with the current result of the fold:

\begin{haskell}
foldl :: (b -> a -> b) -> b -> IO (Push a b)
foldl k z = do
  ref <- newIORef z
  let push_a a = do
       state <- readIORef ref
       writeIORef ref (k state a)
  let done_a = readIORef ref
  return (Push push_a done_a)
\end{haskell}

As before, we can use this \Hs@foldl@ function to implement \Hs@correlation@ and \Hs@regression@.

In order to share a stream between multiple consumers, we need some way to broadcast messages and push each element to many consumers.
We can broadcast to two consumers by combining two consumers into one before connecting it to a producer.
The following function, \Hs/dup_ooo/, duplicates a stream among two consumers, and returns a pair containing both results.
We call this operation \Hs@dup_ooo@ because it \emph{dup}licates elements into two \emph{o}utput sinks (push streams), returning a new \emph{o}utput sink; the reason for this name will become apparent when we see other ways to duplicate streams in \cref{taxonomy/polarised}.

\begin{haskell}
dup_ooo :: Push a r -> Push a r' -> Push a (r,r')
dup_ooo a1 a2 = Push push_a done_a
 where
  push_a a = do
    push a1 a
    push a2 a

  done_a = do
    r  <- done a1
    r' <- done a2
    return (r, r')
\end{haskell}

We could also use the applicative functor \citep{mcbride2008applicative} instance for push streams to combine consumers together, specifying how to transform and combine the results.
The applicative functor implementation is similar to the \Hs/dup_ooo/ function specified above.
This \Hs/dup_ooo/ function could then be written equivalently as (\Hs/dup_ooo a1 a2 = (,) <$> a1 <*> a2/).

We can use \Hs/dup_ooo/ and \Hs/contramap/ to implement \Hs/unzip/, which deconstructs a stream of pairs into a pair of streams:

\begin{haskell}
unzip :: Push a r -> Push b r' -> Push (a,b) (r,r')
unzip push_a push_b = dup_ooo (contramap fst push_a) (contramap snd push_b)
\end{haskell}

Pairs of \Hs@a@ and \Hs@b@ flow from the returned push stream into the argument streams; when there are no more input pairs, the stream results are paired together and flow from the argument streams to the returned stream.
This inverted control flow is because the stream representation forces operations on the elements to be contravariant, and operations on the stream results to be covariant.

With these combinators, we can write the \Hs/priceOverTime/ query using push streams:

\begin{haskell}
priceOverTime_push :: IO (Push Record (Line,Double))
priceOverTime_push = do
  reg   <- Push.regression
  cor   <- Push.correlation
  let cm = Push.contramap
    (\r -> (daysSinceEpoch (time r), price r))
    (Push.dup_ooo reg cor)
  return cm
\end{haskell}

This program computes both correlation and regression in a streaming fashion.
In comparison to the list version of \Hs/priceOverTime/, we have explicitly combined both consumers and reversed the control flow.
We shall see more examples of push programs in \cref{part:icicle}.

We cannot implement \Hs/priceOverMarket/ in constant space with push streams alone, because it requires joining two input streams by date.
Recall the \Hs/join/ combinator, which takes two input streams and retrieves a value from each.
At every step the combinator chooses which stream to pull from, pulling on the stream with the smaller value.
With push streams, a consumer cannot choose which input stream to pull from, or when: the consumer is a function waiting to be called with its input, always ready to accept values as they come.

This inability to join two streams by date is a symptom of a more general limitation of push streams.
Push streams also cannot implement \Hs@zip@, which pairs two inputs together, because the consumer needs to control the computation to alternate between each input.
Except for one special case, push streams do not support combinators with multiple inputs.
The special case is that a push stream can react to multiple inputs in the order they are received.
As a list program, this is similar to taking two lists and at each step non-deterministically choosing which list to pull an element from.
In certain circumstances we can control the push order and use this merge to append two streams.
Because the push order is controlled outside of the merge, appending two streams in this way separates the append logic from the merge combinator which defines the appended stream.

% Pull and push streams are the two most fundamental stream types.
\section{Polarised streams}
\label{taxonomy/polarised}

Stream sharing allows push streams to support multiple queries by broadcasting the elements to multiple consumers, but they do not support streaming operators with multiple inputs, except for non-deterministic merge; pull streams support operators with multiple inputs, but they do not support multiple queries~\citep{kay2009you}.
Combining pull and push streams in the form of \emph{polarised streams} allows us to support multiple inputs and multiple queries~\citep{lippmeier2016polarized}.


Although we cannot share the elements of a pull stream among multiple pull consumers, we can share the elements of a pull stream among one push consumer and one pull consumer.
We call this operation \Hs@dup_ioi@ because it \emph{dup}licates an \emph{i}nput source (pull) into an \emph{o}utput sink (push), returning a new \emph{i}nput source (pull):

\begin{haskell}
dup_ioi_ignore_result :: Pull a -> Push a r -> Pull a
dup_ioi_ignore_result (Pull pull_a) push_b = Pull pull_a'
 where
  pull_a' = do
    v <- pull_a
    case v of
     Nothing -> do
      _ <- done push_b
      return Nothing
     Just a -> do
      push push_b a
      return (Just a)
\end{haskell}

We achieve this duplication by constructing a pull stream which, when pulled on, pulls from its source \Hs@pull_a@, pushes the value to sink \Hs@push_b@, and returns the value to the caller.
The result of the push stream is ignored because the pull stream representation has no way to return a result at the end of the stream.

Encoding the result of a stream inside the stream itself is not important for single-consumer pull streams, because it is usually the consumer of the stream that computes the result.
When mixing stream representations to allow multiple consumers, however, we need to be able to capture the result of all the consumers.
We extend the pull stream representation so that instead of returning a \Hs@Maybe@ with \Hs@Nothing@ to signal the end of the stream, streams now return an \Hs@Either@ with (@Left a@) to signal an element and (@Right r@) to signal the result at the end of the stream:

\begin{haskell}
data PullResult a r = PullResult (IO (Either a r))
\end{haskell}

With this extended pull stream representation, we can implement a version of \Hs@dup_ioi@ that keeps the results of the input stream and the output stream, and pairs them together:

\begin{haskell}
dup_ioi :: PullResult a r -> Push a r' -> PullResult a (r,r')
dup_ioi (PullResult pull_a) push_b = PullResult pull_a'
 where
  pull_a' = do
    v <- pull_a
    case v of
     Right r -> do
      r' <- done push_b
      return (Right (r,r'))
     Left a -> do
      push push_b a
      return (Left a)
\end{haskell}

% There is very little change between the two versions of \Hs@dup_ioi@
Modifying \Hs@dup_ioi_ignore_result@ to work on the new representation only required changing the constructors for the stream and adding the return value; other combinators are modified similarly.
We use the same naming convention for suffixes, for example \Hs@map_i@ for mapping pull streams, and \Hs@map_o@ for contravariantly mapping push streams.
When consuming a pull stream by folding over it, we return the fold result as well as the stream result.
The type signature for \Hs@foldl_i@ changes to include the stream result; the implementation change is similar to the change for \Hs@dup_ioi@:

\begin{haskell}
foldl_i :: (b -> a -> b) -> b -> PullResult a r -> IO (b,r)
\end{haskell}

We can also copy elements from a pull stream to a push stream: we call this operation \emph{draining} the pull stream.
To drain a stream, we loop over all the values in the pull stream and push each one into the push stream.
At the end, we return a pair of the results of both streams:

\begin{haskell}
drain_io :: Pull a r -> Push a r' -> IO (r, r')
drain_io (Pull pull_a) push_a = loop
 where
  loop = do
    v <- pull_a
    case v of
     Left a -> do
      push push_a a
     Right r -> do
      r' <- done push_a
      return (r, r')
\end{haskell}

We can combine \Hs@drain_io@ and \Hs@dup_ooo@ together to duplicate a pull stream into two push streams, which we call \Hs@dup_ioo@:

\begin{haskell}
dup_ioo :: Pull a r -> Push a r' -> Push a r'' -> IO (r,(r',r''))
dup_ioo pull0 push1 push2 = drain_io pull0 (dup_ooo push1 push2)
\end{haskell}

We can also implement \Hs@dup_oii@ by flipping the arguments of \Hs@dup_ioi@, and compose the various \Hs@dup@ functions together to duplicate an arbitrary number of outputs.
With \Hs@dup_ioi@, \Hs@dup_oii@, \Hs@dup_ioo@ and \Hs@dup_ooo@, we can duplicate a stream when there is no more than one pull consumer.
Joining multiple input streams together, as in the \Hs@join@ combinator, is the dual: there can be no more than one push producer.
Recall that the \Hs@join@ combinator required both inputs to be pull streams, and could not be implemented with push streams alone.
With the polarised naming convection, this version of \Hs@join@ is called \Hs@join_iii@.
Because the input streams have result values, we ensure that the joined stream's result contains the results of both inputs.
To compute both results, when one stream ends before the other we drain the unfinished stream until we reach the result.
Other than this draining, the implementation of \Hs@join_iii@ shown in \cref{figs/impl/polar/join_iii} follows the implementation of \Hs@join@.

\begin{haskell}[float,caption=Polarised implementation of \Hs/join_iii/,label=figs/impl/polar/join_iii]
join_iii :: (a -> b -> Ordering) -> PullResult a r
         -> PullResult b r'      -> PullResult (a,b) (r,r')
join_iii comparekey (PullResult pull_a) (PullResult pull_b) = PullResult (do
   a <- pull_a
   b <- pull_b
   go a b)
 where
  go (Left a) (Left b)
   = case comparekey a b of
      EQ -> return (Left (a,b))
      LT -> do
        a' <- pull_a
        go a' b
      GT -> do
        b' <- pull_b
        go a b'
  go (Right a) (Right b) = return (Right (a,b))
  go (Left _) (Right b) = do
    a' <- pull_a
    go a' (Right b)
  go (Right a) (Left _) = do
    b' <- pull_b
    go (Right a) b'
\end{haskell}

\begin{haskell}[float,caption=Polarised implementation of \Hs/join_ioo/,label=figs/impl/polar/join_ioo]
join_ioo :: (a -> b -> Ordering) -> PullResult a r
         -> Push (a,b) r'        -> Push b (r,r')
join_ioo comparekey (PullResult pull_a) push_ab = Push push_b done_b
 where
  push_b b = do
    a <- pull_a
    case a of
     Left a' -> case comparekey a b of
      EQ     -> push push_ab (a,b)
      LT     -> push_b b
      GT     -> return ()
     Right _ -> return () 
  done_b = do
    a <- pull_a
    case a of
     Left _   -> done_b
     Right a' -> do
      b <- done push_ab
      return (a,b)
\end{haskell}


We can also join two streams when one is a pull stream and the other is a push stream: this is called \Hs@join_ioo@.
Conceptually, this combinator has an input pull stream of type \Hs@a@ and an input push stream of type \Hs@b@, with an output push stream of pairs of \Hs@a@ and \Hs@b@.
The definition for \Hs@join_ioo@ is given in \cref{figs/impl/polar/join_ioo}.
The output push stream is given as an argument while the input push stream is the return value.

In the implementation of \Hs/join_ioo/, the returned push stream accepts values of type \Hs@b@.
When a new value is pushed, it repeatedly reads values from the input pull stream until the pulled value is equal to or greater than the pushed value using the given ordering function to compare the keys.
When the ordering function says the two keys are equal, it pushes the pair to the output stream.
When the pull stream ends before the push stream, this implementation reads the end of the pull stream multiple times; the pull stream always returns the stream result after the end of the stream.
Other multiple-input combinators can be inverted similarly to support pull-push-push (\Hs@_ioo@) and push-pull-push (\Hs@_oio@) versions.

We can implement the two \Hs@priceAnalyses@ queries, \Hs@priceOverTime@ and \Hs@priceOverMarket@, by mixing pull and push streams.
We assign a polarity of push or pull to all streams in both queries, starting with the input streams.
\Cref{figs/polar/priceOverTime-priceOverMarket} shows the dependency graph with polarised combinators and explicit duplications.
The polarity of each stream is depicted as a filled or unfilled circle.
Filled circles $\bullet$ represent pull streams because they always contain the next value or the result.
Unfilled circles $\circ$ represent push streams because they are a hole which values can be pushed into.

\input{figs/polarity-pot-pom.tex}

We begin by classifying both input streams as pull streams: we can copy elements from pull streams into push streams, but not vice versa.
The \Hs@stock@ input stream is used twice, so at least one of the use-sites must be push.
Since we were able to express \Hs@priceOverTime@ entirely as a push stream, we duplicate \Hs@stock@ into a push stream for \Hs@priceOverTime@ and a pull stream for \Hs@priceOverMarket@.
For \Hs@priceOverMarket@, we can join both pull streams and map over it.
To compute both regression and correlation, one of the folds must be a push; in this case either consumer can be pull or push.
The decision is inconsequential.
There are many ways to assign polarities to this program, but they all compute the same result.

\Cref{figs/impl/polar/priceOverTime-priceOverMarket} shows the implementation of this polarised dependency graph for \Hs/priceOverTime/ and \Hs/priceOverMarket/.
This streaming single-pass implementation requires more complex control-flow than the list version.
Stream elements flow ``backwards'' from function result to argument in the places where push streams are used, and flow ``forwards'' where pull streams are used.
When a pull stream is duplicated into a push stream, the stream results for the push stream are nested inside the resulting pull stream; recovering these results requires pattern-matching on the nested tuple.

\begin{haskell}[float,caption=Polarised implementation of \Hs/priceOverTime/ and \Hs/priceOverMarket/,label=figs/impl/polar/priceOverTime-priceOverMarket]
priceOverTime_o :: IO (Push Record (Line,Double))
priceOverTime_o = do
  pot_regres    <- regression_o
  pot_correl    <- correlation_o
  let folds      = Push.dup_ooo reg cor
  let timeprices = map_o (\r -> (daysSinceEpoch (time r), price r)) folds
  return timeprices

priceOverMarket_ii :: PullResult Record r -> PullResult Record r'
                   -> IO (Line,(Double,(r,r')))
priceOverMarket_ii stock index = do
  let joined  = join_iii (\s i   -> time s `compare` time i) stock index
  let prices  = map_i    (\(s,i) -> (price s, price i))      joined
  pom_regres <- regression_o
  let prices' = dup_ioi prices pom_regres
  correlation_i prices'

priceAnalyses_ii :: PullResult Record r -> PullResult Record r'
                 -> IO ((Line,Double), (Line, Double))
priceAnalyses_ii stock index = do
  pot        <- priceOverTime_o
  let stock'  = dup_ioi stock pot
  result     <- priceOverMarket_ii stock' index
  case result of 
    (potC,(potR,((r,(pomC,pomR)),r'))) ->
      return ((potC,potR), (pomC,pomR))
\end{haskell}

Assigning polarities is a global analysis, in that we need to inspect the dependency graph containing all queries, rather than looking at each query or each combinator in isolation.
If we add a new query to \Hs/priceAnalyses/, we need to consider the existing polarities when assigning polarities to the new query.
Suppose we have an \emph{industry index} which, like the market index, contains average prices of a representative subset of stocks.
For lists, we can reuse the \Hs@priceOverMarket@ query to compute how closely our stock follows the industry.
For polarised streams, we cannot reuse \Hs@priceOverMarket_ii@ in \Hs@priceAnalyses_ii@ to compare the stock against both indices, because this would require duplicating the stock stream into two pull consumers.
We need to implement another version of the same query with different polarities: \Hs@priceOverMarket_oi@.
Polarised streams are not composible and can require code duplication.

\subsection{Diamonds and cycles}

Recall that the definition of \Hs/correlation/ takes a list of pairs of doubles and performs a fold over them.
We could have also written \Hs/correlation/ to take two lists of doubles and pairing the elements together before folding them.
When correlating values from the same input list, the list-of-pairs version requires one fewer intermediate list; when correlating values from different lists, the pair-of-lists version is slightly more convenient.
Which version is preferable depends on the situation, but the difference is usually minor.

\begin{figure}
\center
\begin{dot2tex}[dot]
digraph G {
  node [shape="none"];
  stock;
  stock [texlbl="\Hs/stock/"];
  stock -> pot_dup[style="*-*"];

  graph [style="rounded corners"];

  subgraph cluster_priceOverTime  {
    lblstyle="left,xshift=0.12cm";
    label="priceOverTime_c";
    texlbl="\Hs/priceOverTime_c/";
    pot_dup [texlbl="\Hs/dup_ioi/"];
    pot_time [texlbl="\Hs/map_i time/"];
    pot_price [texlbl="\Hs/map_o price/"];
    pot_dup -> pot_time[style="*-*"];
    pot_dup -> pot_price[style="o-o"];

    subgraph cluster_correlation {
      lblstyle="below=4.25cm";
      // lblstyle="above";
      label="correlation_io";
      texlbl="\Hs/correlation_io/";
      potc_zip [texlbl="\Hs/zip_ioo/"];
      pot_cor [texlbl="\Hs/foldl_o/"];
      potc_zip -> pot_cor[style="o-o"];
    };
    pot_time -> potc_zip[style="*-*"];
    pot_price -> potc_zip[style="o-o"];
  };
}
\end{dot2tex}
\caption{Polarised dependency graph with diamond}
\label{figs/polar/correlation-zip}
\end{figure}

\begin{haskell}[float,caption=Polarised implementation of \Hs/zip\_ioo/,label=figs/polar/impl/zip_ioo]
zip_ioo :: PullResult a r -> Push (a,b) r' -> Push b (r,r')
zip_ioo (Pull pull_a) push_ab = Push push_b done_b
 where
  push_b b = do
   a <- pull_a
   case a of
    Left a' -> push push_ab (a',b)
    Right _ -> return ()
  done_b = do
   a <- pull_a
   case a of
    Left _  -> done_b
    Right r -> do
     r' <- done push_ab
     return (r,r')
\end{haskell}


With polarised streams, we cannot execute \Hs@priceOverTime@ with the pair-of-lists version, although we can assign polarities.
\Cref{figs/polar/correlation-zip} shows the polarised graph for a hypothetical version of \Hs@priceOverTime@ that only computes the correlation and uses zip.
The \Hs@zip_ioo@ combinator, implemented in \cref{figs/polar/impl/zip_ioo}, is similar to the \Hs@join_ioo@ combinator; it has an input pull, an input push, and an output push.
When the input push stream receives a value, \Hs@zip_ioo@ reads from the pull stream and sends the pair to the output push stream.

\begin{figure}
\center
\begin{dot2tex}[dot]
digraph G {
  node [shape="none"];
  stock;
  stock [texlbl="\Hs/stock/"];
  stock -> pot_dup;

  graph [style="rounded corners"];

  subgraph cluster_priceOverTime  {
    lblstyle="left,xshift=0.12cm";
    label="priceOverTime_c";
    texlbl="\Hs/priceOverTime_c/";
    pot_dup [texlbl="\Hs/dup_ioi/"];
    pot_time [texlbl="\Hs/map_i time/"];
    pot_price [texlbl="\Hs/map_o price/"];
    pot_dup -> pot_time;
    pot_dup -> pot_price[style="<-"];

    subgraph cluster_correlation {
      lblstyle="below=4.25cm";
      // lblstyle="above";
      label="correlation_io";
      texlbl="\Hs/correlation_io/";
      potc_zip [texlbl="\Hs/zip_ioo/"];
      pot_cor [texlbl="\Hs/foldl_o/"];
      potc_zip -> pot_cor[style="<-"];
    };
    pot_time -> potc_zip;
    pot_price -> potc_zip[style="<-"];
  };
}
\end{dot2tex}
\caption{Control flow graph for polarised diamond}
\label{figs/control/correlation-zip}
\end{figure}

Although it is not obvious from the polarised dependency graph, the combinators have a recursive dependency on each other.
The polarised diagram shows \emph{elements} flowing down, but the \emph{control flow} for push streams is upwards.
\Cref{figs/control/correlation-zip} shows the control flow for the polarised diagram, after replacing the edges corresponding to pull streams with downward arrows and upward arrows for edges corresponding to push streams.
In this graph, the recursive dependency is illustrated by the cycle between \Hs@dup_ioi@, \Hs@zip_ioo@ and the two maps.
This cycle complicates translating the graph to an implementation.

\begin{haskell}[float,caption={Incomplete polarised implementation of \Hs/priceOverTime_c/},label=figs/polar/impl/priceOverTime_c]
priceOverTime_c :: PullResult Record r -> IO (Double, r)
priceOverTime_c stock =
  let stock' :: PullResult Record (Double,r)
              = dup_ioi stock prices
      times  :: PullResult Record (Double,r)
              = map_i time stock'
      prices :: Push Record Double
              = map_o price cor
      cor    :: Push Record Double
              = correlation_io times
  in _ -- incomplete: no way to run computation

correlation_io :: PullResult Double r -> Push Double (Double, r)
correlation_io stream_a = zip_ioo stream_a correlation_o
\end{haskell}

The incomplete implementation in \cref{figs/polar/impl/priceOverTime_c} also demonstrates the recursive dependency between \Hs@stock'@, \Hs@times@, \Hs@prices@ and \Hs@cor@.
The \Hs@priceOverTime_c@ function defines a set of stream transformers, but we have no way to execute this stream transformer and extract the result.
The incomplete implementation constructs a stream transformer, but does not directly compute the stream result.
All the combinators are \emph{passive combinators}, constructing a stream transformer that responds to push or pull requests rather than actively pulling or pushing.
\emph{Active combinators} like \Hs/drain_io/ and \Hs/foldl_i/ actively consume pull streams rather than transforming them, and return an \Hs/IO/ action containing the stream result.
Without an active combinator, the query will not execute.
Active combinators can consume pull streams and output to push streams.
Active combinators cannot actively consume push streams, because the control flow for push streams is driven by the producer.
Similarly, they cannot actively produce pull streams.
None of the combinators in this query can be implemented as active combinators because they all consume push streams or produce pull streams.
Instead we must hand-optimise the program, combining the duplicate, maps and zip into one combinator, as in the original version of \Hs@priceOverTime@.


% 
% 
% 
% \subsection{Polarised streams}
% 
% \section{Other streams}
% \subsection{Stream fusion}
% \subsection{Copull: push as pull}
% \subsection{Push-pull, or enumerator}
% \subsection{Monadic streams}
% 

\section{Kahn process networks}
\label{taxonomy/kpn}

The three streaming models we have seen --- pull, push, and polarised streams --- differ in what drives the computation.
With pull streams, the consumer drives the computation.
With push streams, the producer drives the computation.
With polarised streams, the active combinator, which may be a pull consumer and a push producer, drives the computation.
All these systems have one operator driving the computation.
When we have multiple queries to execute, it can be hard to choose just one combinator to drive the entire computation.

An alternate streaming model, which allows many operators to control the computation and supports executing multiple queries, is a \emph{Kahn process network}.
A Kahn process network is a concurrent process network with restrictions to ensure deterministic execution.
Each combinator inside each query becomes a communicating process in the network.
Processes communicate through input channels which they can pull values from, and output channels which they can push values to.
Each process can have multiple inputs and outputs, and the process chooses the order to pull from its inputs and push to its outputs.

Concurrent programs can be hard to write and debug because the \emph{schedule}, which specifies the interleaving of process execution, depends on environmental factors outside the program itself --- for example, the number of physical processors available, and which other processes are also being executed.
Because the environment is not controlled by the processes themselves, we say the schedule is chosen \emph{non-deterministically}.
Likewise, if a program gives different results for different schedules, we say the result is \emph{non-deterministic}.
Kahn process networks are a restricted form of static process network where the result computed by a particular process is independent of the schedule \cite{kahn1976coroutines}: the schedule may be chosen non-deterministically, but the result is still deterministic.
Kahn process networks ensure deterministic results by imposing restrictions on how processes communicate so that scheduling decisions cannot be observed inside the process.
% The restrictions are: all communication between processes is through first-in-first-out channels; reading from channels is blocking; and channels are written to by a single process, broadcasting each value to all consumers of the channel.
All communication between processes is through first-in-first-out channels.
The Kahn process network model specifically rules out processes with shared mutable state, as such a process could non-deterministically compute different results.
If one process were reading from mutable state while another were writing a new value, then the reading process may get the old value or new value, depending on how the processes were scheduled.
Reading from channels is blocking: processes cannot to \emph{peek} at a channel to see whether there are waiting values, because another process might be waiting to be scheduled and about to push a new value.
Channels are written to by a single process, broadcasting each value to all consumers of the channel.
Only one process is ever allowed to push to a given channel: if two processes were able to push to the same channel at the same time, the scheduler would have to decide the order in which values were received.

With Kahn process networks, we can implement a process which joins sorted streams by pulling from each input channel as in the \Hs@join@ combinator, and we can share streams among multiple consumers because pushed values are broadcast to each consumer.
We can convert operators that use both push streams and pull streams to processes.
We can also introduce a process that copies values from a pull stream into a channel, and a process that copies values from a channel into a push stream.

One version of the Kahn process network model uses bounded channels to ensure that the entire network executes in bounded memory.
Kahn process networks with bounded channels still compute results deterministically, but can introduce \emph{artificial deadlocks} in cases where the computation would succeed with a sufficiently large buffer, but the given bounds are too small.
There are dynamic algorithms to identify artificial deadlocks at runtime and resolve them by increasing buffer sizes \cite{parks1995bounded,geilen2003requirements}.


\begin{haskell}[float,caption=Types and combinators for Kahn process networks,label=figs/kpn/combinators]
data Channel a

data Network a
instance Monad Network

data Result  a
instance Applicative Result

map     :: (a -> b) -> Channel a
        -> Network (Channel b)
join    :: (a -> b -> Ordering) -> Channel a -> Channel b
        -> Network (Channel (a,b))
foldl   :: (a -> b -> a) -> a -> Channel b
        -> Network (Result a)

execute :: Network (Result a) -> IO a
\end{haskell}

\Cref{figs/kpn/combinators} shows the datatypes and type signatures of a Kahn process network implementation.
We leave discussion of the implementation for \cref{chapter:process:processes}, and for now focus solely on this simplified version of the interface.
The \Hs@Channel@ type denotes a communication channel between processes.
The \Hs@Network@ monad describes how to construct a process network; execution is deferred until after the entire network has been constructed.
The \Hs@map@ and \Hs@join@ combinators have type signatures similar to the list versions, with lists replaced by \Hs@Channel@s and the return value inside the \Hs@Network@ monad.

Because execution is deferred, the \Hs@foldl@ combinator cannot return the fold result immediately; the result is wrapped in a \Hs@Result@ type.
The \Hs@Result@ type describes the result of executing a process network; it is a promise that the value will be available after all the processes in the network finish.
The \Hs@Result@ has an applicative functor instance, allowing multiple results to be combined together.
The \Hs@execute@ function takes a process network description containing the result promise, and executes the processes before extracting the result.



\begin{haskell}[float,caption=Implementation of \Hs/priceAnalyses/ queries as a Kahn process network,label=figs/kpn/impl/priceAnalyses]
correlation :: Channel (Double,Double) -> Network (Result Double)
regression  :: Channel (Double,Double) -> Network (Result Line)

priceOverTime :: Channel Record -> Network (Result (Line,Double)
priceOverTime stock = do
  timeprices <- map (\r -> (daysSinceEpoch (time r), price r)) stock
  r          <- regression  timeprices
  c          <- correlation timeprices
  return ((,) <$> r <*> c)

priceOverMarket :: Channel Record -> Channel Record -> Network (Result (Line,Double))
priceOverMarket stock index = do
  joined <- join (\s i   -> time s `compare` time i) stock index
  prices <- map  (\(s,i) -> (price s, price i))      joined
  r      <- regression  prices
  c      <- correlation prices
  return ((,) <$> r <*> c)

priceAnalyses :: Channel Record -> Channel Record
              -> Network (Result ((Line,Double),(Line,Double)))
priceAnalysis stock index = do
  pot <- priceOverTime   stock
  pom <- priceOverMarket stock index
  return ((,) <$> pot <*> pom)
\end{haskell}

\Cref{figs/kpn/impl/priceAnalyses} shows the \Hs@priceAnalyses@ queries implemented as a Kahn process network.
There are some differences from the list version: the process network is constructed inside the \Hs@Network@ monad and the results are paired together using \Hs@Result@ applicative functor instance.
Converting the implementation from the list form to the process network form is almost purely syntactic, in contrast to the polarity analysis required for polarised streams.

Concurrent process networks have the desired high-level semantics for executing concurrent queries, but they do not provide the ideal execution strategy.
In \cref{taxonomy/pull/streaming-overhead}, we saw that communication between pull stream combinators involves allocating \Hs@Maybe@ values, which can sometimes be removed by general purpose compiler optimisations (and sometimes not).
Communication between processes requires more overhead than allocating \Hs@Maybe@ values, and is not removed by general purpose optimisations.
To send a value from one process to another, the sending process may need to lock the communication channel to ensure that it has exclusive access to the channel, before copying the value into a buffer where it can be read by the other process.
Concurrent process network implementations amortise the cost of communication by \emph{chunking} messages together: instead of sending many messages with one value in each, chunked communication sends one message containing an array of values.
Chunking reduces the cost of sending messages, but increases memory and cache pressure.
Chunk size determines how many communications are saved, so larger chunks mean less communication overhead.
However, larger chunks also mean that each chunk array requires more memory and is thus less likely to fit in cache.
Since each channel between a pair of processes requires its own chunk, larger process networks have more chunks in memory at the same time.
The optimal chunk size is a trade-off between communication overhead and memory usage, which is usually found by experimentation.


% consider when one process sends a machine integer to another process.
% If the sending process has performed any computation on the integer, the sending process most likely has the integer stored in a register, where it can be operated on directly by the processor.
% To send this value to the other process, the sending process locks the channel to prevent other writers or premature reads, copies the integer to memory where it can be read by the other process, and unlocks the channel.
% Next, the receiving process performs the inverse operation, locking the channel, copying the integer from memory to a register, and unlocking the channel.
% Sending messages is an expensive way to copy an integer from one register to another register --- perhaps even the same register, if the sending process has been swapped out and the receiving process swapped in.

% Concurrent process network implementations often amortise the cost of communication by \emph{chunking} messages together: instead of sending many messages with an integer in each, chunked communication sends one message containing an array of integers.
% Chunking reduces the cost of sending messages, but cannot eliminate it altogether.
% Chunk size determines how many communications are saved, so larger chunks mean less communication overhead.
% However, larger chunks also mean that each array requires more memory and are less likely to fit in memory or cache.
% Since each channel between processes requires its own chunk, larger process networks tend to have more chunks in memory at the same time.
% Finding the optimal chunk size is a trade-off between communication overhead and memory usage, and finding this optimum usually requires experimentation, with no guarantee that we will be able to find it.

From a functional programming perspective, small, fine-grained processes like those used in our \Hs/priceAnalysis/ example are desirable because they allow us to write a process to implement each combinator and compose them together.
From an execution perspective, however, when fine-grained processes perform more communication than computational work, the overall performance is determined by synchronisation and scheduling overheads~\cite{chen1990impact}.
We can reduce the amount of communication by fusing multiple connected processes together into one larger process.
The fused process performs the task of multiple individual processes, but communicates by local variables instead of channels.
In \cref{chapter:process:processes} we describe an algorithm to fuse processes together to reduce overhead.
For \Hs@priceAnalysis@, our algorithm can automatically fuse all the processes together into a single processes.
A single process executes sequentially, so fusing the entire network into a single process removes any potential speedup from task parallelism, but in our benchmarks in \cref{s:Benchmarks}, the sequential version is faster than the concurrent version even with several processors.
Often, a well-optimised sequential implementation of a program will consume significantly less power and cost less to run than a parallel implementation~\cite{mcsherry2015scalability}.

% \cite{chen1990impact}
% "We find that even though there can be a lot of parallelism at the fine grain level, synchronization and scheduling strategies determine the ultimate performance of the system."
% "The most important characteristic is that, without the near-optimal scheme, performance at the operation level and at the statement level is actually worse than at the loop-iteration level."
% "unless we have a very sophisticated scheduling strategy, exploiting very fine grain parallelism with only a limited amount of resources can yield poorer results than we have anticipated"

% In \cite{vrba2009evaluating} it ... too much parallelism and not enough work.

% Too much parallelism might make the program \emph{scalable}, in that adding more processors will speed the program, but unfortunately it does not ensure absolute performance.
% In fact, the contrary is often true.
% \cite{mcsherry2015scalability} stresses the importance of measuring against a well-optimised single-threaded implementation when absolute performance is the goal.


\section{Summary}
We have seen the relative advantages of various streaming models.
Pull streams support operators with multiple inputs, and can take advantage of an optimising compiler to reduce overhead.
Push streams support multiple concurrent queries, and are written back-to-front with explicit duplication for sharing streams.
Polarised streams support multiple inputs and multiple queries, require polarity analysis of the entire dependency graph, and are written partially back-to-front and partially front-to-back.
Kahn process networks support multiple inputs and multiple queries, and concurrent execution involves communication overhead.

In the next chapter we will see Icicle, a language for specifying push stream queries (\cref{part:icicle}).
Icicle queries are written front-to-back, and streams can be shared without requiring explicit duplication.
Queries are compiled to folds over push streams which can be executed concurrently.
After looking at Icicle, we shall see how Kahn process networks can be executed efficiently by fusing processes together (\cref{chapter:process:processes}).

