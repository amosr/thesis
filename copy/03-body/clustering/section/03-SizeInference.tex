%!TEX root = ../Main.tex
\section{Size inference}
Before performing clustering, we infer the relative sizes of each array in the program.
We now present a simple constraint-based inference algorithm.
Size inference has been previously described in the context of array fusion by Chatterjee~\cite{chatterjee1991size}.
In constrast to our algorithm, \cite{chatterjee1991size} does not support size-changing functions such as filter.
If size inference fails, the programs may still be compiled, but fusion is not performed.

Although our constraint based formulation of size inference is reminiscent of type inference for HM(X)~\cite{odersky1999type}, there are important differences.
Firstly, our type schemes include existential quantifiers, which express the fact that the sizes of arrays produced by filter operations are statically unknown, in general.
The output size of \Hs@generate@ is also statically unknown, as the result size is data-dependent and is not available until runtime.
HM(X) style type inferences use the $\exists$ quantifier to bind local type variables in constraints, and existential quantifiers do not appear in type schemes.
Secondly, our types are first-order only, as program graphs cannot take other program graphs as arguments.
Provided we generate the constraints in the correct form, solving them is straightforward.


% -----------------------------------------------------------------------------
\begin{figure}
\begin{tabbing}
MMMMMMMMM \= MM  \= MM \= MMMMMM \= \kill
\textbf{Size Type}
\> $\tau$   \> @::=@  \> $k$                  \> (size variable)       \\
\>          \> $~|$   \> $\tau \times \tau$   \> (cross product)
\end{tabbing}

\begin{tabbing}
MMMMMMMMM \= MM  \= MM \= MMMMMM \= \kill
\textbf{Size Constraint}
\> $C$      \> @::=@  \> $\true$               \> (trivially true)      \\
\>          \> $~|$   \> $k = \tau$           \> (equality constraint) \\
\>          \> $~|$   \> $C \wedge C$         \> (conjunction)
\end{tabbing}

\begin{tabbing}
MMMMMMMMM \= MM  \= MM \= MMMMMM \= \kill
\textbf{Size Scheme}
\> $\sigma$ \> @::=@  
        \> $\forall \ov{k}.~ \exists \ov{k}.~ (\ov{ x : \tau }) \to (\ov{x : \tau})$
\end{tabbing}

\caption{Sizes, constraints and schemes}
\label{clustering:f:constraints}
\end{figure}


\newcommand{\constr}[1]{\llbracket #1 \rrbracket}


% -----------------------------------------------------------------------------
\subsection{Size types, constraints and schemes}
\label{clustering:s:SizeTypes}
\Cref{clustering:f:constraints} shows the grammar for size types, constraints and schemes. A size scheme is like a type constraint from Hindley-Milner type systems, except that it only mentions the size of each input array, and ignores the element types.

A size may either be a variable $k$ or a cross product of two sizes.
We use the latter to represent the result size of the \Hs@cross@ operator discussed in the previous section.
Constraints may either be trivially $\true$, an equality $k = \tau$, or a conjunction of two constraints $C \wedge C$.
We refer to the trivially true and equality constraints as \emph{atomic constraints}.
Size schemes relate the sizes of each input and output array.
The \Hs@normalize2@ example from \cref{clustering:f:normalize2-clusterings}, which returns two output arrays of the same size as the input, has the following size scheme:
% For example, the size scheme for the \Hs@normalize2@ example from \cref{clustering:f:normalize2-clusterings}, which returns two arrays of the same size as the input, is as follows:
$$
\Hs@normalize2@ ~:_s \forall k.~ (xs : k) \to (ys_1 : k,~ ys_2 : k)
$$

We write $:_s$ to distinguish size schemes from type schemes.

The existential quantifier appears in size schemes when the array produced by a filter, or similar operator, appears in the result.
For example:
\begin{haskell}
filterLeft %\(:\sb{s}\,\forall\,k\sb{1}.\,\exists\,k\sb{2}.\,(xs\,:\,k\sb{1})\;\to\;(ys\sb{1}\,:\,k\sb{1},\,ys\sb{2}\,:\,k\sb{2})\)%
filterLeft xs
  = let ys1 = map (+ 1)   xs
        ys2 = filter even xs
    in (ys1, ys2)
\end{haskell}

The size scheme of \Hs@filterLeft@ shows that it works for input arrays of all sizes.
The first result array has the same size as the input, and the second has some unrelated and unknown size.

Finally, note that size schemes form but one aspect of the type information that would be expressible in a full dependently typed language.
For example, in Coq or Agda we could write something like:
\begin{haskell}
filterLeft : %\(\forall\,k\sb{1}:\,\)Nat\(.~\,\exists\,k\sb{2}:\,\)Nat\(.\)%
  Array %\(k\sb{1}\)% Float -> (Array %\(k\sb{1}\)% Float, Array %\(k\sb{2}\)% Float)
\end{haskell}

However, the type inference systems for fully higher order dependently typed languages typically require quantified types to be provided by the user, and do not perform the type generalization process. In our situation, we need automatic type generalization, but for a first-order language only.


\input{copy/03-body/clustering/figures/Size-constraint-gen.tex}
% -----------------------------------------------------------------------------
\subsection{Constraint generation}
The rules for constraint generation are shown in \cref{clustering:f:ConstraintGeneration}.
The first judgment form is written as ($\SizeL{\Gamma_1}{\textit{lets}}{\Gamma_2}{C}$) and reads: ``under environment $\Gamma_1$, the bindings in $\textit{lets}$ produce the result environment $\Gamma_2$ and size constraints $C$''.
The judgment form ($\SizeB{\Gamma_1}{zs}{b}{\Gamma_2}{C}$) performs constraint generation for a single binding and reads: ``under environment~$\Gamma_1$, array variable $zs$ binds the result of $b$, producing a result environment $\Gamma_2$ and size constraints $C$''.
The environment ($\Gamma$) has the following grammar:
$$
\Gamma~ @::=@ ~~\cdot ~~|~~ \Gamma,~ \Gamma ~~|~~ zs : k ~~|~~ k ~~|~~ \exists k
$$

As usual, ($\cdot$) represents the empty environment and ($\Gamma,~ \Gamma$) represents environment concatenation.
The element ($zs : k$) records the size $k$ of some array variable $zs$.
A plain $k$ indicates that $k$ can be unified with other size types when solving constraints, whereas $\exists k$ indicates a  \emph{rigid} size variable that cannot be unified with other sizes.
We use the $\exists k$ syntax because this variable will also be existentially quantified if it appears in the size scheme of the overall program.

The constraints are generated in a specific form, to facilitate the constraint solving process.
For each array variable in the program, we generate a new size variable, like size $k_{zs}$ for array variable $zs$.
These new size variables always appear on the \emph{left} of atomic equality constraints.
For each array binding, we may also introduce unification or rigid variables; these appear on the \emph{right} of atomic equality constraints.

The final environment and constraints generated for the \Hs@normalize2@ example from \cref{clustering:s:Introduction} are as follows, with the program shown on the right:

\begin{minipage}{0.5\textwidth}
$$
\begin{array}{ll}
   & @xs @ : k_{xs},~
\\ & @gts@ : k_{gts},~ \exists k_1,
\\ & @ys1@ : k_{ys1},~ k_2,
\\ & @ys2@ : k_{ys2},~ k_3
\\
\vdash & \true 
        ~\wedge~  k_{gts} = k_1
\\ \wedge & \true
        ~ \wedge~  k_{xs}  ~= k_2
        ~ \wedge~  k_{ys1}  = k_2 
\\     &~~~~~~~\; 
          \wedge~  k_{xs}   = k_3
        ~ \wedge~  k_{ys2}  = k_3
\end{array}
$$
\end{minipage}
\begin{minipage}{0.5\textwidth}
% normalize2 %\(:_s \forall k_{xs}.~ (xs : k_{xs}) \to (ys_1 : k_{xs},~ ys_2 : k_{xs})\)%
\begin{haskell}
normalize2 xs
 = let sum1 = fold   (+)  0   xs
       gts  = filter (>   0)  xs
       sum2 = fold   (+)  0   gts
       ys1  = map    (/ sum1) xs
       ys2  = map    (/ sum2) xs
   in (ys1, ys2)
\end{haskell}
\end{minipage}

In this example, the input array \Hs/xs/ and its corresponding size type $k_{xs}$ are passed as part of the input environment to the constraint generation judgment.
For each binding, we generate a constraint and add any required array and size bindings to the environment.
The \Hs/sum1/ binding, a \Hs/fold/, does not bind any array variables and works for any input size, so we leave the environment as-is and produce a $\true$ constraint.
For the \Hs/gts/ binding, a \Hs/filter/, the size of the output array is unknown.
We record the size of the output array by introducing a new size-type variable $k_{gts}$, as well as an existential variable $k_1$; we also generate the constraint ($k_{gts} = k_1$).
For the \Hs/ys1/ binding, a \Hs/map/, the size of the output array is the same as the input array.
We introduce a new size variable $k_{ys1}$ to record the size of the output array, and introduce a new unification variable $k_2$.
In the constraints, we require both variables to be equal to the input size variable $k_{xs}$, ensuring that array size variables occur on the left-hand side and unification variables on the right.
Constraint generation for the remaining bindings proceeds similarly.


% -------------------------------------------------------------------
\subsection{Constraint solving and generalization}
\Cref{clustering:f:ConstraintSolving} shows the rule for assigning a size scheme to a program.
The top-level judgment form ($\SizeF{\program}{\sigma}$) assigns size scheme $\sigma$ to $\program$.

\input{copy/03-body/clustering/figures/Size-constraint-solving.tex}

Rule (SProgram) assigns a size scheme to a program by first extracting size constraints, before solving them and generalizing the result.
In the rule, $\Gamma_0$ is used as the input environment to constraint generation, and is constructed by generating a fresh size variable ($k_i$) for each input array ($xs_i$).
The environment and constraints produced by constraint generation are named $\Gamma_1$ and $C_1$; these constraints are then solved using the $\textrm{SOLVE}$ function, which we describe soon.
The constraints, after being solved, are stored in $C_2$, and the environment in $\Gamma_2$.
We use the solved constraints to find the size types of the input arrays ($\ov{s}$), and the size types of the output arrays ($\ov{t}$).
We perform generalization by adding universal quantifiers for the unification variables mentioned by the types of input arrays ($\ov{k_a}$), and adding existential quantifiers for the existential variables mentioned by the types of output arrays ($\ov{k_e}$).
Finally, we require that the types of input arrays do not mention any existential variables; an example of this restriction is shown in \cref{clustering:s:RigidSizes}.

In the rule, the solving process is indicated by $\textrm{SOLVE}$, and takes an environment and a constraint set, and produces a solved environment and constraint set.
As the constraint solving process is both standard and straightforward, we only describe it informally.

During constraint generation in the previous section, we were careful to ensure that all the size variables named after program variables are on the left of atomic equality constraints, while all the unification and existential variables are on the right.
To solve the constraints, we keep finding pairs of atomic equality constraints where the same variable appears on the left, unify the right of both of these constraints, and apply the resulting substitution to both the environment and original constraints.
When there are no more pairs of constraints with the same variable on the left, the constraints are in solved form and we are finished.

During constraint solving, all unification variables occuring in the environment can have other sizes substituted for them.
In contrast, the rigid variables marked by the $\exists$ symbol cannot.
For example, consider the constraints for \Hs@normalize2@ mentioned before:
$$
\begin{array}{ll}
   & @xs @ : k_{xs},~
@gts@ : k_{gts},~ \exists k_1,~
@ys1@ : k_{ys1},~ k_2,~
@ys2@ : k_{ys2},~ k_3
\\
\vdash & \true 
        ~\wedge~  k_{gts} = k_1 ~\wedge~ \true
\\     &~~~~~~~\; 
          \wedge~  \colorbox{green!10}{$k_{xs}  ~= k_2$}
          \wedge~  k_{ys1}  = k_2 
\\     &~~~~~~~\; 
        ~ \wedge~  \colorbox{green!10}{$k_{xs}   = k_3$}
        ~ \wedge~  k_{ys2}  = k_3
\end{array}
$$

In the highlighted constraints, $k_{xs}$ is mentioned twice on the left of an atomic equality constraint, so we can substitute $k_2$ for $k_3$. Eliminating the duplicates, as well as the trivially $\true$ terms then yields:
$$
\begin{array}{ll}
   & @xs @ : k_{xs},~
@gts@ : k_{gts},~ \exists k_1,~
@ys1@ : k_{ys1},~ k_2,~
@ys2@ : k_{ys2},~ k_3
\\
\vdash & k_{gts} = k_1
        ~\wedge~  k_{xs}  ~= k_2
        ~\wedge~  k_{ys1}  = k_2 
        ~\wedge~  k_{ys2}  = k_2
\end{array}
$$

To produce the final size scheme, we look up the sizes of the input and output variables of the original program from the solved constraints and generalize appropriately.
This process is determined by the top-level rule in \cref{clustering:f:ConstraintSolving}.
In the case of \Hs/normalize2/, no rigid size variables appear in the result, so we can universally quantify all size variables to get the following size scheme:
$$\Hs@normalize2@ ~:_s \forall k_2. (xs : k_2) \to (ys_1 : k_2,~ ys_2 : k_2)
$$


Rule~(SProgram) also characterises the programs we accept: a program is \emph{valid} if and only if $\exists \sigma.\ \SizeF{\program}{\sigma}$. 

% -----------------------------------------------------------------------------
\subsection{Rigid sizes}
\label{clustering:s:RigidSizes}
When the environment of our size constraints contains rigid variables (indicated by $\exists k$), we introduce existential quantifiers instead of universal quantifiers into the size scheme.
Consider the \Hs@filterLeft@ program from \cref{clustering:s:SizeTypes}:
\begin{haskell}
filterLeft xs
 = let ys1 = map (+ 1)   xs
       ys2 = filter even xs
   in (ys1, ys2)
\end{haskell}

The size constraints for this program, already in solved form, are as follows:
$$
\begin{array}{ll}
       & xs : k_{xs},~ ys_1 : k_{ys1},~ \exists k_1,~ ys_2 : k_{ys2},~ k_2
\\
\vdash &          k_{ys_1} = k_1
        ~\wedge~  k_{ys_2} = k_2
        ~\wedge~  k_{xs}   = k_2
\end{array}
$$

As variable $k_1$ is marked as rigid, we introduce an existential quantifier for it, producing the size scheme stated earlier:

$$
@filterLeft@ :\sb{s}\,\forall k\sb{2}.\ \exists k\sb{1}.\ (xs\,:\,k\sb{2})\;\to\;(ys\sb{1}\,:\,k\sb{2},\,ys\sb{2}\,:\,k\sb{1})
$$

Note that, although rule~(SProgram) from \cref{clustering:f:ConstraintSolving} performs a \emph{generalization} process, there is no corresponding instantiation rule. The size inference process works on the entire graph at a time, and there is no mechanism for one operator to invoke another. To say this another way, all subgraphs are fully inlined. Recall from \cref{clustering:s:CombinatorNormalForm}, that we assume our operator graphs are embedded in a larger host program. We use size information to guide the clustering process, and although the host program can certainly call the operator graph, static size information does not flow across this boundary.

When producing size schemes, we do not permit the arguments of an operator graph to have existentially quantified sizes.
This restriction is necessary to reject programs that we cannot statically guarantee will be well-sized.
For example:
\begin{haskell}
bad1 xs
 = let flt = filter p xs
       ys  = map2   f flt xs
   in  ys
\end{haskell}

The above program filters its input array, and then applies \Hs@map2@ to the filtered version as well as the original array.
As the \Hs@map2@ operator requires both of its arguments to have the same size, \Hs@bad1@ would only be valid when the predicate \Hs@p@ is always true.
The size constraints are as follows:
$$
\begin{array}{ll}
       & xs : k_{xs},~ flt : k_{flt},~ \exists k_1,~ ys : k_{ys},~ k_2
\\
\vdash &          k_{flt}  = k_1
        ~\wedge~  k_{flt}  = k_2
        ~\wedge~  k_{xs}   = k_2
        ~\wedge~  k_{ys}   = k_2
\end{array}
$$

\noindent
Solving this then yields:
$$
\begin{array}{ll}
       & xs : k_{xs},~ flt : k_{flt},~ \exists k_1,~ ys : k_{ys},~ k_1
\\
\vdash &          k_{flt}  = k_1
        ~\wedge~  k_{xs}   = k_1
        ~\wedge~  k_{ys}   = k_1
\end{array}
$$

In this case, rule~(SProgram) does not apply, because the parameter variable $xs$ has size $k_1$, but $k_1$ is marked as rigid in the environment (with $\exists k_1$). 

As a final example, the following program is ill-sized because the two filter operators are not guaranteed to produce the same number of elements:
\begin{haskell}
bad2 xs
 = let flt1 = filter p1 xs
       flt2 = filter p2 xs
       ys   = map2   f  flt1 flt2
   in  ys
\end{haskell}

The initial size constraints for this program are:
\newcommand\flt{\textit{flt}}
$$
\begin{array}{ll}
       & xs : k_{xs},~ \flt1 : k_{\flt1},~ \exists k_1,~ \flt2 : k_{\flt2},~ \exists k_2,~ ys : k_{ys},~ k_3
\\
\vdash &          k_{\flt1}   = k_1
        ~\wedge~  k_{\flt2}   = k_2
        ~\wedge~  k_{\flt1}   = k_3
        ~\wedge~  k_{\flt2}   = k_3
        ~\wedge~  k_{ys}   = k_3
\end{array}
$$

To solve these, we note that $k_{\flt1}$ is used twice on the left of an atomic equality constraint, so we substitute $k_1$ for $k_3$:
$$
\begin{array}{ll}
       & xs : k_{xs},~ \flt1 : k_{\flt1},~ \exists k_1,~ \flt2 : k_{\flt2},~ \exists k_2,~ ys : k_{ys},~ k_1
\\
\vdash &          k_{\flt1}   = k_1
        ~\wedge~  k_{\flt2}   = k_2
        ~\wedge~  k_{\flt2}   = k_1
        ~\wedge~  k_{ys}   = k_1
\end{array}
$$

At this stage we are stuck, because the constraints are not yet in solved form, and we cannot simplify them further.
Both $k_1$ and $k_2$ are marked as rigid, so we cannot substitute one for the other and produce a single atomic constraint for $k_{\flt2}$.
The $\textrm{SOLVE}$ function fails to return a solution, and rule (SProgram) cannot apply.


% \newcommand{\eqclasses}[1]{
%     \begin{tabbing}
%         MM \= M \= \kill
%         #1
%     \end{tabbing}}
% 
% \newcommand{\eqclass}[2]{$#1$ \> $\in$ \> $\{#2\}$ \\}

% The next example involves two filters using the same predicate.
% Despite using the same predicate and input data, we produce different output sizes for each filter.
% \begin{tabbing}
% @MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM@  \= \kill
% @diff xs@                           \> $\exists k_{xs}.$ \\
% @ = let ys1 = filter p xs@          \> $\forall k_{ys1}.$       \\
% @       ys2 = filter p xs@          \> $\forall k_{ys2}.$       \\
% @   in (ys1, ys2)@                  \> $true$                   \\
% \end{tabbing}
% 
% This constraint is valid, with the equivalence classes being:
% \eqclasses{
%     \eqclass{k_{xs}}    {k_{xs}}
%     \eqclass{k_{ys1}}   {k_{ys1}}
%     \eqclass{k_{ys2}}   {k_{ys2}}
% }


% -----------------------------------------------------------------------------
\subsection{Iteration size}
After inferring the size of each array variable, each operator is assigned an \emph{iteration size}, which is the number of iterations needed in the loop which evaluates that operator.
For \Hs@filter@ and other size-changing operators, the iteration and result sizes are in general different.
For such an operator, we say that the result size is a \emph{descendant} of the iteration size.
Conversely, the iteration size is a \emph{parent} of the result size. 

This descendant--parent size relation is transitive, so if we filter an array, then filter the resulting array, the size of the result is a descendant of the iteration size of the initial filter.
This relation arises naturally from the fact that we compile individual clusters into a loop using process fusion (\cref{chapter:process:processes}).
With process fusion, such an operation would be compiled into a single loop --- with an iteration size identical to the size of the input array, and containing two nested if-expressions to perform the two layers of filtering.

Iteration sizes are used to decide which operators can be fused with each other.
As in prior work, operators with the same iteration size can be fused.
However, in our system we also allow operators of different iteration sizes to be fused, provided those sizes are descendants of the same parent size.

We use $T$ to range over iteration sizes, and write $\bot$ for the case where the iteration size is unknown. The $\bot$ size is needed to handle the \Hs@external@ operator, as we cannot statically infer its true iteration size, and it cannot be fused with any other operator.

\begin{tabbing}
MMMMMMMM \= MM       \= MM \= MMMM \= \kill
\textbf{Iteration Size}
 \> $T$         \> @::=@  \> $\tau$        \> (known size) \\
 \>             \> $~|$   \> $\bot$     \> (unknown size) \\
\end{tabbing}

Once the size constraints have been solved, we can use the $\iiter$ function in \cref{fig:clustering:iter} to compute the iteration size of each binding.
In the definition, we use the syntax $\Gamma(xs)$ to find the ($xs : k$) element in the environment $\Gamma$ and return the associated size $k$.
Similarly, we use the syntax $C(k)$ to find the corresponding ($k = \tau$) constraint in $C$ and return the associated size type $\tau$.


\begin{figure}
\begin{tabbing}
MMMMM \= M \= MMMMMMMMMM \= MM \= \kill
$\iiter_{\Gamma,C}$  
        \>$:$\> $bind \rightarrow T$ 
\\[1ex]
$\iiter_{\Gamma,C}$
        \> $|$  \> $(z~ = \Hs@fold@~ f~xs)$     
                \> $=$ \> $C(\Gamma(xs))$ 
\\
        \> $|$  \> $(ys = \Hs@map@_n~f~\overline{xs})$
                \> $=$ \> $C(\Gamma(ys))$ 
\\
        \> $|$  \> $(ys = \Hs@filter@~f~xs)$    
                \> $=$ \> $C(\Gamma(xs))$ 
\\
        \> $|$  \> $(ys = \Hs@generate@~s~f)$  
                \> $=$ \> $C(\Gamma(ys))$ 
\\
        \> $|$  \> $(ys = \Hs@gather@~is~xs)$    
                \> $=$ \> $C(\Gamma(is))$ 
\\
        \> $|$  \> $(ys = \Hs@cross@~as~bs)$     
                \> $=$ \> $C(\Gamma(as)) \times C(\Gamma(bs))$ 
\\
        \> $|$  \> $(ys = \Hs@external@~\overline{xs})$  
                \> $=$ \> $\bot$ 
\end{tabbing}
\caption{Computing the iteration size of a binding}
\label{fig:clustering:iter}
\end{figure}


% Once the constraints are solved, known to be valid, and sorted into equivalence classes, each combinator is assigned a size. Note that for a filter, the size of the output array $k_o$ is some existential that is less than or equal to $k_n$, but the actual loop size of the \emph{combinator} is equal to $k_n$. This is because, in order to produce the filtered output, all elements of the input $n$ must be considered.

% After the generated equality constraints are solved, and sizes are grouped into equivalence classes, combinators with iteration sizes in the same equivalence class may be fused together if there are no fusion-preventing dependencies between them.


% -----------------------------------------------------------------------------
\subsection{Transducers}
%     we've said that several times before
% Unlike previous work, we do allow combinators with different iteration sizes to be fused together. For example, an operation on filtered data may be fused with the filter operation that generates the data, even though the iteration sizes are different.

We define the concept of \emph{transducers} as combinators having a different output size to their iteration size.
As with any other combinator, a transducer may fuse with other nodes of the same iteration size, but transducers may also fuse with nodes having iteration size the same as the transducer's output size.
For our set of combinators, the only transducer is \Hs@filter@.

Looking back at the \Hs@normalize2@ example, the iteration sizes of the combinators \Hs@gts@ (bound to a \Hs/filter/ over input \Hs/xs/) and \Hs@sum1@ (a \Hs/fold/ over \Hs/xs/) are both $k_{xs}$.
%The iteration size of \Hs@sum2@ is $k_{gts}$, and \Hs@gts@ is a transducer from $k_{xs}$ to $k_{gts}$. 
%\gabi{gts is no combinator, so can't be a transducer
The iteration size of combinator \Hs@sum2@ (a \Hs/fold/ over \Hs/gts/) is $k_{gts}$, and the filter combinator which produces \Hs@gts@ is a transducer from $k_{xs}$ to $k_{gts}$. 
Even though $k_{gts}$ is distinct from $k_{xs}$, the three nodes \Hs@gts@, \Hs@sum1@ and \Hs@sum2@ can all be fused together.
% The two nodes \Hs@sum2@ and \Hs@gts@ can be fused together, since the output size of \Hs@gts@ is the iteration size of \Hs@sum2@.
% Similarly, \Hs@sum1@ and \Hs@gts@ can be fused together, as they have the same iteration size.

\Cref{fig:clustering:trans} defines a function $trans$, to find the parent transducer of a combinator application.
Since each name is bound to at most one combinator, we abuse terminology here slightly and write \emph{combinator $n$} when refering to the combinator occuring in the binding of the name $n$.
The parent transducer $trans(bs, n)$ of a combinator $n$ has the same output size as $n$'s iteration size, but the two have different iteration sizes.

\begin{figure}
\begin{tabbing}
MMMM \= MM \= MMMMMMMMM \= MMMM \= MM \= \kill
$trans$  \>$:$\> $binds \rightarrow name \rightarrow \{name\}$ \\
$trans(bs,o)$    \\
            \> $|$ \> $o = \Hs@filter@~f~n$    \> $\in bs$ \> $=$ \> $trans'(bs,n)$ \\
            \> $|$ \> otherwise             \>          \> $=$ \> $trans'(bs,o)$ \\
\\
$trans'(bs,o)$    \\
            \> $|$ \> $o = \Hs@fold@~f~n$      \> $\in bs$ \> $=$ \> $\emptyset$ \\
            \> $|$ \> $o = \Hs@map@_n~f~ns$    \> $\in bs$ \> $=$ \> $\bigcup_{x \in ns} trans(bs, x)$ \\
            \> $|$ \> $o = \Hs@filter@~f~n$    \> $\in bs$ \> $=$ \> $\{o\}$       \\
            \> $|$ \> $o = \Hs@generate@~s~f$  \> $\in bs$ \> $=$ \> $\emptyset$ \\
            \> $|$ \> $o = \Hs@gather@~i~d$    \> $\in bs$ \> $=$ \> $trans(bs,i)$ \\
            \> $|$ \> $o = \Hs@cross@~a~b$     \> $\in bs$ \> $=$ \> $\emptyset$ \\
            \> $|$ \> $o = \Hs@external@~ins$  \> $\in bs$ \> $=$ \> $\emptyset$ \\
\end{tabbing}
\caption{Finding the parent transducers of a combinator}
\label{fig:clustering:trans}
\end{figure}

With the $trans$ function, we can express the restriction on programs we view as valid for our transformation more formally:

\textbf{Lemma: sole transducers}.
If a program $p$ is \emph{valid}, then its bindings will have at most one transducer:
\[
\forall p, \sigma, n.\ p :_s \sigma \implies |trans(binds(p), n)| \le 1
\]

Informally, only the $\Hs/map/_n$ clause in $trans$ can introduce multiple elements in the result.
Since the constraint generation for $\Hs/map/_n$ requires all inputs to have the same size, the inputs should also have the same transducer.
If the inputs had different transducers, then their size would be generated by different filters, and each would have its own, separate existential variable as a size type.

% \textbf{Proof:} by induction on $bs$. If $n = \Hs@map@_n~f~ns$,
% then $trans(bs,n) = \bigcup_{x \in ns} trans(bs,x)$.
% As \Hs@map@ requires its arguments to have the same size, and \Hs@filter@ introduces a fresh size for its output,
% if any of the $trans(bs,x)$ are non-empty, they will refer to the same \Hs@filter@.
% The other cases are trivial.

% this is only useful for proving sole parents
% \textbf{Lemma: transducers change types}.
% For some bindings $bs$, if $n$ has a transducer, the iteration size is not the same.
% \[
% \forall bs, n, m.\ trans(bs, n) = \{m\} \implies \tau(n) \not= \tau(m)
% \]
 
% \textbf{Lemma: sole parents}.
% For some program $p$ with valid constraints, each pair of names $a$ and $b$ will have at most one pair of parents $parents(a,b)$.
% \[
% \forall p, \sigma, a, b.\ p :_s \sigma \implies |parents(binds(p), a, b)| \le 1
% \]
% 
% These two lemmas are used in the integer linear programming formulation, when generating the constraints.
% When fusing two nodes of different iteration size, at most one pair of parents will need to be checked.


\begin{figure}
% \begin{tabbing}
% MMMM \= M \= M \= M \= \kill
% $parents$ \> $:$ \> $binds \to name \to name \to \{name \times name\}$ \\
% $parents(bs, a, b)$ \\
%         \> $|$ \> $\iiter_{\Gamma,C}(bs(a)) == \iiter_{\Gamma,C}(bs(b))$ \\
%         \>     \>                      \> $=$ \> $\{(a, b)\}$ \\
%         \> $|$ \> otherwise            \\
%         \>     \>                      \> $=$    \> $\{ parents(bs, a', b) ~|~ a' \in trans(bs, a) \} $      \\
%         \>     \>                      \> $\cup$ \> $\{ parents(bs, a, b') ~|~ b' \in trans(bs, b) \} $  \\
% \end{tabbing}
% changed to return optional single closest parents:
%
\begin{tabbing}
MMMM \= M \= M \= M \= MMMM \= M \= \kill
$parents$ \> $:$ \> $binds \to name \to name \to (name \times name)_\bot$ \\
$parents(bs, a, b)$ \\
        \> $|$ \> $(p_a,p_b,d) \in parents'(bs,a,b)$ \\
        \>     \>                      \> $=$ \> $\{(a, b)\}$ \\
        \> $|$ \> otherwise \\
        \>     \>                      \> $=$    \> $\bot$ \\
\\
$parents'$ \> $:$ \> $binds \to name \to name \to (name \times name \times \mathbb{N})_\bot$ \\
$parents'(bs, a, b)$ \\
        \> $|$ \> $\iiter_{\Gamma,C}(bs(a)) == \iiter_{\Gamma,C}(bs(b))$ \\
        \>     \>                      \> $=$ \> $\{(a, b, 0)\}$ \\
        \> $|$ \> $a' \in trans(bs,a),~ p_a \in parents'(bs,a',b)$ \\
        \> $,$ \> $b' \in trans(bs,b),~ p_b \in parents'(bs,a,b')$ \\
        \>     \>                      \> $=$ \> $increment(closest(p_a, p_b))$ \\
        \> $|$ \> $a' \in trans(bs,a),~ p_a \in parents'(bs,a',b)$ \\
        \>     \>                      \> $=$ \> $increment(p_a)$ \\
        \> $|$ \> $b' \in trans(bs,b),~ p_b \in parents'(bs,a,b')$ \\
        \>     \>                      \> $=$ \> $increment(p_b)$ \\
        \> $|$ \> otherwise            \\
        \>     \>                      \> $=$    \> $\bot$
\\
\\
$closest$ \> $:$ \> $(name \times name \times \mathbb{N}) \to (name \times name \times \mathbb{N}) \to (name \times name \times \mathbb{N})$ \\
$closest((l_a,l_b,l_d), (r_a,r_b,r_d))$ \\
        \> $|$ \> $l_d \le r_d$ \> \> \> $=$ \> $(l_a,l_b,l_d)$ \\
        \> $|$ \> otherwise     \> \> \> $=$ \> $(r_a,r_b,r_d)$ \\
\\
$increment ~ :$ \> \> $(name \times name \times \mathbb{N}) \to (name \times name \times \mathbb{N})$ \\
$increment((a,b,d)) = (a,b,d+1)$ \\
\end{tabbing}

\caption{Finding the closest ancestor transducers with same iteration size}
\label{fig:clustering:parents}
\end{figure}

To determine whether two combinators of different iteration sizes may be fused together, \cref{fig:clustering:parents} defines the $parents$ function, which finds the closest pair of parent or ancestor transducers, where both ancestors are the same size.
We use the syntax $(name \times name)_\bot$ to denote an optional pair of names.
Two combinators $a$ and $b$ of different size may be fused together only if they have parents $(c, d) \in parents(a,b)$, and the combinators and their parents are also fused together.
That is, in order for $a$ and $b$ to be fused together, $c$ and $d$ must be fused, $a$ and $c$ must be fused, and $d$ and $b$ must be fused.
Although we use the terms ``ancestors'' and ``parents'', if the two inputs have the same iteration size, the two parents will be the inputs themselves and the above fusion requirements for different size inputs can be simplified away.

There may be, in general, multiple pairs of ancestor transducers with compatible iteration sizes.
To fuse a combinator with its ancestor, we must also fuse the combinator with all the nodes in the path between the combinator and its ancestor.
In the definition of $parents$, we find the closest ancestors because it allows the most fusion; finding the furthest ancestors would be more restrictive, as it would also require fusing the inputs with any other ancestor transducers between the input combinators and the furthest ancestors.
% In the original publication of this work \citep{robinson2014fusing}, we only allowed fusion when there was a single pair of ancestors.

In the previous \Hs/normalize2/ example, \Hs@sum1@ consumes the input \Hs/xs/, while \Hs@sum2@ consumes the output of the filter \Hs/gts/, which in turn consumes the input \Hs/xs/.
The two folds have different iteration sizes, and their parents are $parents(\Hs@sum1@, \Hs@sum2@) = (\Hs@sum1@, \Hs@gts@)$.
The parents \Hs/sum1/ and \Hs/gts/ both consume the input \Hs/xs/ and have the same iteration size.
In order for \Hs@sum1@ and \Hs@sum2@ to be fused together, we require that: \Hs@sum1@ and \Hs@gts@ are fused together; \Hs@sum1@ and \Hs@sum1@ are fused together, which is trivial as fusion is reflexive;  and \Hs@gts@ and \Hs@sum2@ are fused together.
