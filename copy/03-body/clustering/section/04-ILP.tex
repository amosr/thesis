%!TEX root = ../Main.tex

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{copy/03-body/clustering/figures/ex2-normalizeInc.pdf}
\end{center}
\caption{Two clusterings for \Hs/normalizeInc/}
\label{clustering:f:normalizeInc}
\end{figure}


% -----------------------------------------------------------------------------
\section{Integer linear programming}
\label{clustering:s:ILP}
It is usually possible to cluster a program graph in multiple ways.
For example, consider the following simple function:
\begin{haskell}
normalizeInc :: Array Int -> Array Int
normalizeInc xs
 = let incs = map  (+1)     us
       sum1 = fold (+) 0    us
       ys   = map  (/ sum1) incs
   in  ys
\end{haskell}

Two possible clusterings are shown in \cref{clustering:f:normalizeInc}.
One option is to compute \Hs@sum1@ first and fuse the computation of \Hs@incs@ and \Hs@ys@.
Another option is to fuse the computation of \Hs@incs@ and \Hs@sum1@ into a single loop, then compute \Hs@ys@ separately.
A third option (not shown) is to compute all results separately, and not perform any fusion. 

Which option is better?
On current hardware, we generally expect the cost of memory access to dominate runtime.
The first clustering in \cref{clustering:f:normalizeInc} requires two reads from array \Hs@xs@ and one write to array \Hs@ys@.
The second clustering requires a single fused read from \Hs@xs@, one write to \Hs@incs@, a read back from \Hs@incs@ and a final write to \Hs@ys@.
From the size constraints of the program, we know that all intermediate arrays have the same size, so we expect the first clustering will peform better as it only needs three array accesses per element in the input array, instead of four. 

For small programs such as \Hs@normalizeInc@, it is possible to naively enumerate all possible clusterings, select just those that are \emph{valid} with respect to fusion-preventing edges, and choose the one that maximises a cost metric such as the number of array accesses needed.
However, as the program size increases the number of possible clusterings becomes too large to naively enumerate.
For example, \citet{pouchet2010combined,pouchet2011polyhedral} present a fusion system using the polyhedral model and report that some simple numeric programs have over 40,000 possible clusterings, with one particular example having $10^{12}$ clusterings.

To deal with the combinatorial explosion in the number of potential clusterings, we instead use an integer linear programming (ILP) formulation.
ILP problems are defined as a set of variables, an objective linear function and a set of linear constraints.
The integer linear solver finds an assignment to the variables that minimises the objective function, while satisfying all constraints.
For the clustering problem, we express our constraints regarding fusion-preventing edges as linear constraints on the ILP variables, then use the objective function to encode our cost metric.
This general approach was first fully described by \citet{megiddo1998optimal}; our main contribution is to extend their formulation to work with size-changing operators such as \Hs@filter@. 


% -----------------------------------------------------------------------------
\subsection{Dependency graphs}
A dependency graph represents the data dependencies of the program to be fused, and we use it as an intermediate stage when producing linear constraints for the ILP problem.
The dependency graph contains enough information to determine the possible clusterings of the input program, while abstracting away from the exact operators used to compute each intermediate array.
The rules for producing dependency graphs are shown in \cref{clustering:f:DependencyGraph}.

\input{copy/03-body/clustering/figures/ILP-dependency-graph.tex}

Each binding in the source program becomes a node ($V$) in the dependency graph.
For each intermediate variable, we add a directed edge ($E$) from the binding that produces a value to all bindings that consume it.
Each edge is also marked as either \emph{fusible} or \emph{fusion-preventing}.
Fusion-preventing edges are used when the producer must finish its execution before the consumer node can start.
For example, a \Hs@fold@ operation must complete execution before it can produce the scalar value needed by its consumers.
Conversely, the \Hs@map@ operation produces an output value for each value it consumes; as the input array is processed from start to end and values are produced incrementally, its edge is marked as fusible. 

The \Hs@gather@ operation is a hybrid: it takes an indices array and an elements array, and for each element in the indices array returns the corresponding data element.
This means that \Hs/gather/ can be fused with the operation that produces its indices, but not the operation that produces its elements --- because those are accessed in a random-access manner. 

The \Hs/cross/ operation computes the cartesian product by looping over its second array for every element in the first array, so it can fuse with the operation that produces its first input, but requires the second input to be a manifest array.

% Each node may simply be the name of its output binding (or bindings, in the case of \Hs@external@); as we require names to only be bound once, this is assured to be unique. Creating edges between these nodes is simply when one binding references an earlier one. The only complication is designating edges as \emph{fusible} or \emph{fusion-preventing}.



% -----------------------------------------------------------------------------
\subsection{Integer linear program variables}
After generating the dependency graph, the next step is to produce a set of linear constraints from this graph.
The variables involved in these constraints are split into three groups, shown in \cref{fig:clustering:ilp-vars}.

\begin{figure}
\centering
\parbox{0cm}{
\begin{tabbing}
M   \= M \= MMMMMM \= MM \= \kill
$x$   \> @:@  \> $V \times V$ \> $\to$ \> $\mathbb{B}$ \\
$\pi$ \> @:@  \> $V$             \> $\to$ \> $\mathbb{R}$ \\
$c$   \> @:@  \> $V$             \> $\to$ \> $\mathbb{B}$
\end{tabbing}
}
\caption{Definition of variables in the integer linear program}
\label{fig:clustering:ilp-vars}
\end{figure}

% \subsubsection{Fused together}

The first group of variables, $x$, is parameterised by a pair of node indices from the dependency graph.
For each pair of nodes with indices $i$ and $j$, we use a boolean variable $x_{i,j}$, which indicates whether those two nodes are fused.
We use $x_{i,j} = 0$ when the nodes are fused and $x_{i,j} = 1$ when they are not.
Using $0$ for the fused case means that the objective function can be a weighted function of the $x_{i,j}$ variables, and minimizing it tends to increase the number of nodes that are fused.
The values of these variables are used to construct the final clustering, such that $\forall i,j.\ x_{i,j} = 0 \iff \mathit{cluster}(i) = \mathit{cluster}(j)$.
% This group of variables is equivalent to $x$ in \citet{megiddo1998optimal}.

% \subsubsection{Topological ordering}
The second group of variables in \cref{fig:clustering:ilp-vars}, $\pi$, is parameterised by a single node index.
This group of variables is used to ensure that the clustering is acyclic.
An acyclic clustering is necessary to be able to execute the resulting clustering: we need to ensure that for each node in the graph, the dependencies of that node can be executed before the node itself.
For each node $i$, we associate a real number $\pi_i$, such that for every node $j$ that depends on $i$ and is not fused with $i$, we have $\pi_j > \pi_i$.
Our linear constraints will ensure that if two nodes are fused into the same cluster, then their $\pi$ values will be identical --- though nodes in different clusters can also have the same $\pi$ value.

\begin{figure}
\begin{minipage}{0.5\textwidth}
\begin{haskell}
mapFoldMap xs 
 = let ys  = map  (+1)   xs
       sum = fold (+) 0  ys
       zs  = map  (+sum) ys
   in  zs
\end{haskell}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{center}
\includegraphics[scale=0.7]{copy/03-body/clustering/figures/ex3-cyclic.pdf}
\end{center}
\end{minipage}
\caption{Program \Hs/mapFoldMap/, with an invalid clustering that contains a cycle}
\label{clustering:f:cyclic}
\end{figure}


The left of \Cref{clustering:f:cyclic} shows an example program, \Hs/mapFoldMap/, with an invalid clustering shown on the right.
In this program, there is no fusion-preventing edge directly between the \Hs@ys@ and \Hs@zs@ bindings, but there is a fusion-preventing edge between \Hs@sum@ and \Hs@zs@.
The cluster diagram shows the \Hs@ys@ and \Hs@zs@ bindings in the same cluster, while \Hs@sum@ is in a different cluster.
This clustering contains a dependency cycle between the two clusters, and neither can be executed before the other.
We constrain the $\pi$ variables to reject this clustering, by requiring that $\pi_{\Hs@ys@} \le \pi_{\Hs@sum@} < \pi_{\Hs@zs@}$.
Since $\pi_{\Hs@ys@} < \pi_{\Hs@zs@}$, the two cannot be in the same cluster.

% The $\pi$ group is equivalent to $\pi$ in \citet{megiddo1998optimal}, and $\rho$ in \citet{darte2002contraction}.

% \subsubsection{Array contraction}
The final group of variables in \cref{fig:clustering:ilp-vars}, $c$, is parameterised by a single node index.
This group of variables is used to help define the cost model encoded by the objective function.
Each node is assigned a variable $c_i$ that indicates whether the array produced by the associated binding is \emph{fully contracted}.
When an array is fully contracted, it means that all consumers of that array are fused into the same cluster, so we have $c_i = 0 \iff (\forall (i',j) \in E.\ i = i' \implies x_{i,j} = 0)$.
In the final program, each successive element of a fully contracted array can be stored in a scalar register, rather than requiring an array register or memory storage. 

% The $c$ group is equivalent to the $k$ variable in \citet{darte2002contraction}.
% The cost model in \citet{darte2002contraction} optimises for fully contracted
% To implement our desired cost model, we use a mixture of variable groups $x$ and $\pi$ from \citet{megiddo1998optimal}, and the variable groups $\pi$ and $c$ from \citet{darte2002contraction}.
% We describe the cost model in \cref{clustering:s:ObjectiveFunction}.
The names of the first two variable groups are standard; we propose the rather strained mnemonics $x_{i,j}$ denotes an e{\bf x}tra loop between $i$ and $j$; $\pi_i$ denotes $i$'s {\bf p}osition in the topological ordering; and $c_i$ denotes that $i$'s output array is fully {\bf c}ontracted.
These three variable groups are a mixture of the variables from previous work.
\citet{megiddo1998optimal}'s formulation uses the $x$ and $\pi$ groups, but does not include the $c$ group; their cost model does not take into account fully contracted arrays.
\citet{darte2002contraction}'s formulation uses both $c$ and $\pi$ groups, where they are called $k$ and $\rho$ respectively, but does not include the $x$ group, which is necessary for our formulation of size-changing operations.


% -----------------------------------------------------------------------------
\subsection{Linear constraints}
\label{clustering:s:LinearConstraints}
We place linear constraints on the integer linear program variables, and split the constraints into four groups: constraints that ensure the clustering is acyclic; constraints that encode fusion-preventing edges; constraints describing when nodes with different iteration sizes can be fused together; and constraints involving array contraction. 

% Before showing the optimised version with certain constraints removed (\cref{clustering:s:OptimisedConstraints}), this simpler, unoptimised version is shown. The only difference is that fewer constraints and variables are required in the optimised version, but both versions give the same clustering. This unoptimised version generates clustering constraints and variables for every pair of nodes, regardless of whether they may, in fact, be fused together. Later, we show that certain constraints and variables can be removed when there is a fusion-preventing edge between two nodes.


% -------------------------------------
\subsubsection{Acyclic and precedence-preserving constraints}

The first group of constraints ensures that the clustering is acyclic, using the $\pi$ variable group described earlier:
\begin{tabbing}
MM  \= MMMx \= M \= MMM \= M \= MMMM \= \kill
    \> ~~~~~~~ $x_{i,j}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N \cdot x_{i,j}$ 
    \>             (with an edge from $i$ to $j$)            \\
    \> $-N \cdot  x_{i,j}$  \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N \cdot x_{i,j}$ 
    \>             (with no edge from $i$ to $j$)
\end{tabbing}

As per \citet{megiddo1998optimal}, the form of these constraints is determined by whether there is a dependency between nodes $i$ and $j$.
The $N$ value is set to the total number of nodes in the graph.

If there is an edge from node $i$ to node $j$, we use the first constraint form shown above.
If the two nodes are fused into the same cluster then we have $x_{i,j} = 0$.
In this case, the constraint simplifies to $0 \le \pi_j - \pi_i \le 0$, which forces $\pi_i = \pi_j$.
If the two nodes are in \emph{different} clusters then the constraint instead simplifies to $1 \le \pi_j - \pi_i \le N$.
This means that the difference between the two $\pi$ variables must be at least 1, and less than or equal to $N$.
Since there are $N$ nodes, the maximum number of clusters, when each node is assigned to its own separate cluster, is $N$ clusters.
For this clustering the difference between any two $\pi$ variables would be at most $N$, so the upper bound of $N$ is large enough to be safely ignored.
Ignoring the upper bound, the constraint can roughly be translated to $\pi_i < \pi_j$, which enforces the acyclicity constraint.

If instead there is no edge from node $i$ to node $j$, then we use the second constraint form above.
As before, if the two nodes are fused into the same cluster then we have $x_{i,j} = 0$, which forces $\pi_i = \pi_j$.
If the nodes are in different clusters then the constraint simplifies to $-N \le \pi_j - \pi_i \le N$, which effectively puts no constraint on the $\pi$ values.


% ------------------------------------
\subsubsection{Fusion-preventing edges}
As per \citet{megiddo1998optimal}, if there is a fusion-preventing edge between two nodes, we add a constraint to ensure that the nodes will be placed in different clusters.
\begin{tabbing}
MMM     \= MMM \= M  \= MMM \= M \= MMM \= \kill
        \> $x_{i,j}$ \> $=$ \> $1$ \>   \> \\
        \> (for fusion-preventing edges from $i$ to $j$) 
\end{tabbing}

When combined with the precedence-preserving constraints shown earlier, setting $x_{i,j} = 1$ also forces $\pi_i < \pi_j$. 


% -------------------------------------
\subsubsection{Fusion between different iteration sizes}
This group of constraints restricts which nodes can be placed in the same cluster, based on their iteration size.
The group has three parts. 
Firstly, if either of the two nodes connected by an edge have an unknown ($\bot$) iteration size, as with \Hs/external/ operators, then they cannot be fused and we set $x_{i,j} = 1$:
\begin{tabbing}
MMM     \= MMM \= M \= MMM \= M \= MMM \= \kill
        \> $x_{i,j}$   \> $=$   \> $1$          \>       \>     \\
        \> (if $\iiter_{\Gamma,C}(i) = \bot 
                ~\vee~ \iiter_{\Gamma,C}(j) = \bot$)
\end{tabbing}

Secondly, if the two nodes have different iteration sizes and no common ancestors with compatible iteration sizes, then they also cannot be fused and we set $x_{i,j} = 1$:
\begin{tabbing}
MMM     \= MMM \= M \= MMM \= M \= MMM \= \kill
        \> $x_{i,j}$   \> $=$   \> $1$          \>       \>     \\
        \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) 
                ~\wedge~ \concestors(i,j) = \bot$)
\end{tabbing}

Finally, if the two nodes have different iteration sizes but \emph{do} have compatible common ancestors, then the two nodes can be fused together if they are fused with their respective concestors, and the concestors themselves are fused together:
\begin{tabbing}
MMM     \= MMM \= M \= MMM \= M \= MMM \= \kill
        \> $x_{a,A}$   \> $\le$ \> $x_{a,b}$    \>       \>     \\
        \> $x_{b,B}$   \> $\le$ \> $x_{a,b}$    \>       \>     \\
        \> $x_{A,B}$   \> $\le$ \> $x_{a,b}$    \>       \>     \\
        \> (if $\iiter_{\Gamma,C}(a) \not= \iiter_{\Gamma,C}(b) 
                ~\wedge~ (A,B) \in \concestors(a,b)$)
\end{tabbing}

This last part is the main difference to existing ILP solutions: we allow nodes with different iteration sizes to be fused when their common ancestor transducers are fused.
The actual constraints encode a ``no more fused than'' relationship.
For example, $x_{a,A} \le x_{a,b}$ means that nodes $a$ and $b$ can be no more fused than nodes $a$ and $A$. 

As a simple example, consider fusing an operation on filtered data with its generating filter, as in the folds from \Hs/normalize2/:
\begin{haskell}
    sum1 = fold (+) 0  xs
    gts  = filter (>0) xs
    sum2 = fold (+) 0  gts
\end{haskell}

Here, \Hs/sum1/ and \Hs/sum2/ have different iteration sizes, and their compatible common ancestor transducers are computed to be $\concestors(\Hs/sum1/, \Hs/sum2/) = (\Hs/sum1/, \Hs/gts/)$.
With the above constraints, \Hs/sum1/ and \Hs/sum2/ may only be fused together if three requirements are satisfied: \Hs/sum1/ is fused with \Hs/sum1/ (trivial), \Hs/sum2/ is fused with \Hs/gts/, and \Hs/sum1/ is fused with \Hs/gts/.

% Here, $\concestors(gts,sum2) = \{gts, gts\}$. This generates spurious, but still valid, constraints that $gts$ must be fused with $gts$ and $gts$ must be fused with $sum2$, in order for $gts$ and $sum2$ to be fused together. While these constraints are unnecessary in this case, they are harmless.

% While it may seem like these constraints should be implied by transitivity of clustering, all clustering variables are used for the objective function.


% -------------------------------------
\subsubsection{Array contraction}
The final group of constraints gives meaning to the $c$ variables, which represent whether an array is fully contracted:
\begin{tabbing}
MMM     \= MMM \= M \= MMM \= M \= MMM \= \kill
        \> $x_{i,j}$    \> $\le$ \> $c_i$  \> \> \\
        \> (for all edges from i)
\end{tabbing}

An array is fully contracted when all of the consumers are fused with the node that produces it, which means that the array does not need to be fully materialized in memory.
As per \citet{darte2002contraction}'s work on array contraction, we define a variable $c_i$ for each array, and the constraint above ensures that $c_i = 0$ only if $\forall (i',j) \in E.\ i = i' \implies x_{i,j} = 0$.
By minimizing $c_i$ in the objective function, we favour solutions that reduce the number of intermediate arrays.


% -----------------------------------------------------------------------------
\subsection{Objective function}
\label{clustering:s:ObjectiveFunction}
The objective function defines the cost model of the program, and the ILP solver will find the clustering that minimizes this function while satisfying all the constraints defined in the previous section.
Our cost model has three components:
\begin{itemize}
\item
the number of array reads and writes --- an abstraction of the amount of memory bandwidth needed by the program; 
\item
the number of intermediate arrays --- an abstraction of the amount of intermediate memory needed; 
\item
the number of distinct clusters --- an abstraction of the cost of loop management instructions, which maintain loop counters and the like.
\end{itemize}

The three components of the cost model are a heuristic abstraction of the true cost of executing the program on current hardware.
They are ranked in order of importance --- so we prefer to minimize the number of array reads and writes over the number of intermediate arrays, and to minimize the number of intermediate arrays over the number of clusters.
However, minimizing one component does not necessarily minimize any other.
For example, as the fused program executes multiple array operations at the same time, in some cases the clustering that requires the least number of array reads and writes uses more intermediate arrays than strictly necessary.

As this cost model simply counts the number of intermediate arrays, the cost model assumes that the arrays are close enough in size that the difference is not significant.
One possible extension would be to add size hints to each array, and scale each array's cost by its expected size.
We do not assign any cost to the worker functions, because each worker function will be called the same number of times regardless of the clustering.
This simplification ignores the locality benefits of fusion, as two worker functions performed in the same cluster will often operate over the same input value, or share work in some way.
For datasets sourced from network or disk, the cost of reading the data can be several orders of magnitudes larger than any locality benefits.


We encode the ordering of the components of the cost model as different weights in the objective function.
First, note that if the program graph contains $N$ combinators (nodes) then there are at most $N$ opportunities for fusion, and at most $N$ intermediate arrays.
We then encode the relative cost of loop overhead as weight $1$, the cost of an intermediate array as weight $N$, and the cost of an array read or write as weight $N^2$.
These coefficients ensure that no amount of loop overhead reduction can outweigh the benefit of removing an intermediate array, and likewise no number of removed intermediate arrays can outweigh a reduction in the number of array reads or writes.
The integer linear program including the objective function is shown in \cref{fig:clustering:ilp-obj}.

\begin{figure}
\begin{tabbing}
MMMMM   \= MMMMM \= M \= \kill
Minimise   \>     $\Sigma_{(i,j) \in E} W_{i,j} \cdot x_{i,j}$   
                        ~~ (memory traffic and loop overhead)
\\ ~~~~~~~~~~~~~ $+$ \> $\Sigma_{i \in V} N \cdot c_i$
                        ~~~~~~~~~~ (removing intermediate arrays)
\\[1ex]
   Subject to  \> \ldots ~ constraints from \cref{clustering:s:LinearConstraints} ~ \ldots 
\\ Where   \> $W_{i,j} = N^2$ \> $~|$ \> $(i,j) \in E $         
\\         \> \> \> (fusing $i$ and $j$ will reduce memory traffic)         
\\         \> $W_{i,j} = N^2$ \> $~|$ \> $\exists k. (k,i) \in E \wedge (k,j) \in E $     
\\         \> \> \> ($i$ and $j$ share an input array)
\\         \> $W_{i,j} = 1$   \> $~|$ \> $\Hs@otherwise@$
\\         \> \> \> (the only benefit is loop overhead)
\\
\\
           \> $N = |V|$
           \> \> (the number of nodes in the graph)
\end{tabbing}
\caption{Integer linear program with objective function}
\label{fig:clustering:ilp-obj}
\end{figure}

% However, these benefits cannot be considered in isolation; for example, fusing two loops to reduce loop overhead may remove potential fusion opportunities that reduce memory traffic. When operating on large arrays that do not fit in cache, memory traffic dominates execution time. An excessive number of intermediate arrays can also cause issues if all are live in memory at once, potentially leading to \emph{thrashing}. The benefits of removing loop overhead are least of all; it should be performed if possible, but must never remove opportunities for other fusion.



% -----------------------------------------------------------------------------
\subsection{Fusion-preventing path optimisation}
\label{clustering:s:OptimisedConstraints}
The integer linear program defined in the previous section includes more constraints than strictly necessary to define the valid clusterings.
If two nodes have a path between them that includes a fusion-preventing edge, then we know upfront that they must be placed in different clusters.
\Cref{fig:clustering:possible} defines the function $\possible(a,b)$, which determines whether there is any possibility that the two nodes $a$ and $b$ can be fused.
Similarly, the function $\possible'(a, b)$ checks whether there is any possibility that the compatible common ancestors of $a$ and $b$ may be fused.

\begin{figure}
\begin{tabbing}
MMMM \= M \=     \= MMMMMMMMMMMM    \=  \kill
$\possible : \name \times \name \to \mathbb{B}$ \\
$\possible(a,b) = \forall p \in path(a,b) \cup path(b,a).\ \fusionpreventing \not\in p$
\\[1ex]
$\possible' : \name \times \name \to \mathbb{B}$ \\
MMMMMMx        \= M    \= \kill
$\possible'(a,b)$ 
        \> $=$   \>$\exists A, B.\ (A,B) \in \concestors(a,b) \wedge \possible(a,b)$ \\
        \> $\wedge$ \> $\possible(A,a) \wedge \possible(B,b) \wedge \possible(A,B)$
\end{tabbing}
\caption{Definition of $\possible$ function for checking fusion-preventing paths}
\label{fig:clustering:possible}
\end{figure}

\begin{figure}
\begin{tabbing}
MMMMM   \= MMMM \= M \= MMMM \= M \= MMMM \= \kill
Minimise   \> $\Sigma_{(i,j) \in E} W_{i,j} \cdot x_{i,j} + \Sigma_{i \in V} N \cdot c_i$  \\
           \> (if $\possible(i,j)$)         
\\[0.5ex]
Subject to \> $-N \cdot x_{i,j}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N \cdot x_{i,j}$ \\
           \> (if $\possible(i,j) \wedge (i,j) \not\in E \wedge (j,i) \not\in E$)            
\\[0.5ex]
           \>    $x_{i,j}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N \cdot x_{i,j}$ \\
           \> (if $\possible(i,j) \wedge (i,j,\fusible) \in E$)     
\\[0.5ex]
           \>             \>       \> $\pi_i < \pi_j$ \>       \>            \\
           \> (if $(i,j,\fusionpreventing) \in E$)    
\\[0.5ex]
           \> $x_{i,j}$    \> $\le$ \> $c_i$           \>       \>            \\
           \> (if $(i,j, \fusible) \in E$) \\
           \> $c_{i }$    \> $ = $ \> $ 1 $           \>       \>            \\
           \> (if $(i,j, \fusionpreventing) \in E$)
\\[0.5ex]
           \> $x_{i,j}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\bot \in \{\iiter_{\Gamma,C}(i), \iiter_{\Gamma,C}(j)\}$)  
\\[0.5ex]
           \> $x_{i',i}$   \> $\le$ \> $x_{i,j}$        \>       \>            \\
           \> $x_{j',j}$   \> $\le$ \> $x_{i,j}$        \>       \>            \\
           \> $x_{i',j'}$   \> $\le$ \> $x_{i,j}$        \>       \>            \\
           \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) \wedge \possible'(i,j)$ \\
           \> \> $\wedge~\concestors(i,j) = \{(i',j')\}$) 
\\[0.5ex]
           \> $x_{i,j}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) \wedge \neg \possible'(i,j)$) 
\\[0.5ex]
MMMMM   \= MMMM \= M \= \kill
Where      \> $W_{ij} = N^2$ \> $~|$ \> $(i,j) \in E $         \\
           \> \> \> (fusing $i$ and $j$ will reduce memory traffic)         \\
           \> $W_{ij} = N^2$ \> $~|$ \> $\exists k. (k,i) \in E \wedge (k,j) \in E $     \\
           \> \> \> ($i$ and $j$ share an input array)                                         \\
           \> $W_{ij} = 1$   \> $~|$ \> $\Hs@otherwise@$                                                  \\
           \> \> \> (the only benefit is loop overhead)                                        
\\
\\
           \> $N = |V|$
           \> \> (the number of nodes in the graph)
\end{tabbing}
\caption{Integer linear program with fusion-preventing path optimisation}
\label{fig:clustering:ilp-fpo}
\end{figure}

With $\possible$ and $\possible'$ defined, we refine our formulation to only generate constraints between two nodes if there is a chance they may be fused together.
The final formulation of the integer linear program is shown in \cref{fig:clustering:ilp-fpo}.
This refined version generates fewer constraints, and makes the job of the ILP solver easier.

% \input{copy/03-body/clustering/section/04-ILP-Proof.tex}
