%!TEX root = ../Main.tex
\section{Benchmarks}
\label{clustering:s:Benchmarks}

\begin{table}
$$\begin{array}{c}

\begin{tabular}{lrrrrrrrr}
                & \multicolumn{2}{c}{Unfused}         & \multicolumn{2}{c}{Stream}
                & \multicolumn{2}{c}{Megiddo} &\multicolumn{2}{c}{\textbf{Ours}} \\
                & Time & Loops   & Time & Loops      & Time & Loops & Time & Loops   \\
\hline
Normalize2      & 0.37s & 5      & 0.31s & 4          & 0.34s & 3  & \textbf{0.28s} & \textbf{2}\\
Closest points  & 7.34s & 7      & 6.86s & 6          & 6.33s & 4  & \textbf{6.33s} & \textbf{4}\\
Quadtree        & 0.25s & 8      & 0.25s & 8          & 0.11s & 2  & \textbf{0.11s} & \textbf{2}\\
%                   25ms                                                   11ms
Quickhull       & 0.43s & 4      & 0.39s & 3          & 0.28s & 2  & \textbf{0.21s} & \textbf{1}\\
\end{tabular}

\end{array}$$
\caption{Benchmark results}
\label{clustering:f:BenchResults}
\end{table}

This section discusses four representative benchmarks, and gives the full ILP program of the first benchmark.
These benchmarks highlight the main differences between our fusion mechanism and related work.
The runtimes of each benchmark are summarized in \cref{clustering:f:BenchResults}.
We report times and the number of clusters for: the unfused case, where each operator is assigned to its own cluster; the clustering implied by pull-based stream fusion~\cite{coutts2007stream}; the clustering chosen by \citet{megiddo1998optimal}; and the clustering chosen by our system. 

For each benchmark, we compute the different clusterings, and compile each cluster using the process fusion implementation described in \cref{s:Benchmarks}.
Using the same fusion algorithm isolates the true cost of the various clusterings from low-level differences in code generation.
% report the runtimes of hand-fused C code based on the clustering determined by each algorithm.
% Although in a production implementation of clustering we would use the process fusion from \cref{chapter:process:processes} to fuse each cluster, we report on hand-fused C code to provide a fair comparison to related work.
% As mentioned in~\citet{lippmeier2013data}, the current Haskell stream fusion mechanism introduces overhead in terms of a large number of duplicate loop counters, which increases register pressure unnecessarily.
% Hand-fusing all code and compiling it with the same compiler (GCC) isolates the true cost of the various clusterings from low-level differences in code generation.

We have used both GLPK and CPLEX as external ILP solvers.
For small programs such as \Hs@normalizeInc@, both solvers produce solutions in under 100ms.
For a larger randomly generated example with twenty-five combinators, GLPK took over twenty minutes to produce a solution, while the commercial CPLEX solver was able to produce a solution in under one second --- which is still quite usable.
We will investigate the reason for this wide range in performance in future work.

% We use GLPK as an external solver, which is not particularly fast, but has readily-available Haskell bindings. For these small programs with few combinators, it produces results in under one hundred milliseconds.
% For larger programs with twenty or more combinators to be clustered, GLPK is inadequate.
% One randomly generated example with twenty-five combinators took over twenty minutes in GLPK, while the commercial solver CPLEX was able to solve the same example in a second, and another open source solver CBC took one hundred seconds.
% For the same example, the unoptimised program with all constraints took over twenty minutes in CBC, while CPLEX still only took one and a half seconds.

The hand-fused implementations of the benchmark programs are available at \url{https://github.com/amosr/papers/tree/master/2014betterfusionforfilters/benches}.

% The three benchmarks are the \Hs@normalize2@ example, finding the closest pair of points, and quadtree. The benchmark programs are all hand-written, hand-fused C code based on the clustering. Each program was run with the same input five times, and the minimum runtime was used. Runtimes and the number of loops for each clustering are shown in \cref{clustering:f:BenchResults}. In all cases, our clustering performs better than or as good as Megiddo's, and better than stream fusion and unfused. Interestingly, stream fusion's clustering for \Hs@normalize2@ performs better than Megiddo's, despite having more loops, as stream fusion is able to remove the intermediate array.


% -----------------------------------------------------------------------------
\subsection{Normalize2}
To demonstrate the ILP formulation, we will use the \Hs@normalize2@ example from \cref{clustering:s:Introduction}, repeated here:
\begin{haskell}
normalize2 :: Array Int -> (Array Int, Array Int)
normalize2 xs
 = let sum1 = fold   (+)  0   xs
       gts  = filter (>   0)  xs
       sum2 = fold   (+)  0   gts
       ys1  = map    (/ sum1) xs
       ys2  = map    (/ sum2) xs
   in (ys1, ys2)
\end{haskell}

We use the ILP formulation with fusion-preventing path optimisation from \cref{clustering:s:OptimisedConstraints}.
First, we calculate the $\possible$ function to find the nodes which have no fusion-preventing path between them.
The sets of nodes which can potentially be fused together are as follows:
\[ \{ \{sum1, gts, sum2\}
 , \{sum1, ys2\}
 , \{gts, sum2, ys1\}
 , \{ys1, ys2\} \} \]

The complete ILP program is shown in \cref{fig:clustering:normalize2-ilp}.
In the objective function the weights for $x_{sum1, sum2}$ and $x_{sum2, ys1}$ are both only 1, because the respective combinators do not share any input arrays.

\begin{figure}
\begin{tabbing}
MMMMM   \= MMMMMMM \= M \= MMMMMMM \= M \= MMMMMMM \= \kill
Minimise   \> $25 \cdot x_{sum1, gts} + 1  \cdot x_{sum1,sum2} + 25 \cdot x_{sum1, ys2} +$ \\
           \> $25 \cdot x_{gts, sum2} + 25 \cdot x_{gts, ys1} + 1 \cdot x_{sum2, ys1} +$ \\
           \> $25 \cdot x_{ys1, ys2}  + 5  \cdot c_{gts} + 5 \cdot c_{ys1} + 5 \cdot c_{ys2} $
\\[0.5ex]
Subject to 
    \> $-5 \cdot x_{sum1, gts}$  \> $\le$ \> $\pi_{gts} - \pi_{sum1}$  \> $\le$ \> $5 \cdot x_{sum1, gts}$  \\
    \> $-5 \cdot x_{sum1, sum2}$ \> $\le$ \> $\pi_{sum2} - \pi_{sum1}$ \> $\le$ \> $5 \cdot x_{sum1, sum2}$ \\
    \> $-5 \cdot x_{sum1, ys2 }$ \> $\le$ \> $\pi_{ys2 } - \pi_{sum1}$ \> $\le$ \> $5 \cdot x_{sum1, ys2 }$ \\
    \> $-5 \cdot x_{gts,  ys1 }$ \> $\le$ \> $\pi_{ys1 } - \pi_{gts }$ \> $\le$ \> $5 \cdot x_{gts, ys1  }$ \\
    \> $-5 \cdot x_{sum2, ys1 }$ \> $\le$ \> $\pi_{ys1 } - \pi_{sum2}$ \> $\le$ \> $5 \cdot x_{sum2, ys1 }$ \\
    \> $-5 \cdot x_{ys1, ys2  }$ \> $\le$ \> $\pi_{ys2 } - \pi_{ys1 }$ \> $\le$ \> $5 \cdot x_{ys1, ys2  }$ 
\\[0.5ex]
    \> $   x_{gts, sum2 }$ \> $\le$ \> $\pi_{sum2} - \pi_{gts }$ \> $\le$ \> $5 \cdot x_{gts, sum2 }$ 
\\[0.5ex]
    \>                     \>       \> $\pi_{sum1} < \pi_{ys1}$ \\
    \>                     \>       \> $\pi_{sum2} < \pi_{ys2}$
\\[0.5ex]
    \> $ x_{gts,sum2} $    \> $\le$ \> $c_{gts}$
\\[0.5ex]
    \> $x_{gts, sum2}$     \> $\le$ \> $x_{sum1, sum2}$ \\
    \> $x_{sum1,sum1}$     \> $\le$ \> $x_{sum1, sum2}$ \\
    \> $x_{sum1, gts}$     \> $\le$ \> $x_{sum1, sum2}$
\end{tabbing}
\caption{Complete integer linear program for \Hs/normalize2/}
\label{fig:clustering:normalize2-ilp}
\end{figure}

\begin{figure}
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$x_{sum1, gts},~ x_{sum1, sum1},~ x_{sum1, sum2},~ x_{gts, sum2},~ x_{ys1,  ys2}$
    \> $=$ \> $0$ \\
$x_{sum1, ys2},~ x_{gts, ys1},~   x_{sum2, ys1}$
    \> $=$ \> $1$ 
\\[1ex]
$\pi_{sum1},~ \pi_{gts },~ \pi_{sum2}$
    \> $=$ \> $0$ \\
$\pi_{ys1 },~ \pi_{ys2 }$
    \> $=$ \> $1$ 
\\[1ex]
$c_{gts},~ c_{ys1},~ c_{ys2}$           
    \> $=$ \> $0$
\end{tabbing}
\caption{A minimal solution to the integer linear program for \Hs/normalize2/}
\label{fig:clustering:normalize2-ilp-sol}
\end{figure}

One minimal solution to the integer linear program for \Hs/normalize2/ is given in \cref{fig:clustering:normalize2-ilp-sol}.
This minimal solution is not unique, though in this case the only other minimal solutions use different $\pi$ values, and denote the same clustering.
Looking at just the non-zero variables in the objective function, the value is $25 \cdot x_{sum1,ys2} + 25 \cdot x_{gts,ys1} + 1 \cdot x_{sum2, ys1} = 51$.
For illustrative purposes, note that the objective function could be reduced by setting $x_{sum1,ys2} = 0$ (fusing $sum1$ and $ys1$), but this conflicts with the other constraints.
Since $x_{sum1, sum2} = 0$, we require that $\pi_{sum1} = \pi_{sum2}$, as well as \mbox{$\pi_{sum2} < \pi_{ys2}$}.
These constraints cannot be satisfied, so a clustering that fused $sum1$ and $ys2$ would not also permit $sum1$ and $sum2$ to be fused.

We will now compare the clustering produced by our system with the one implied by pull-based stream fusion.
As we saw in \cref{taxonomy/pull}, pull streams do not support distributing an input stream among multiple consumers; likewise, stream fusion does not support fusing an input with multiple consumers into a single loop.
% , or fuse operators that are not in a producer-consumer relationship.
The corresponding values of the $x_{ij}$ variables are:

\begin{figure}[h!]
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$x_{gts, sum2}$
    \> $=$ \> $0$ \\
$x_{sum1, gts}, x_{sum1, sum2}, x_{ys1,  ys2}, x_{sum1, ys2}, x_{gts, ys1 }, x_{sum2, ys1}$
    \> $=$ \> $1$
\end{tabbing}
\end{figure}

We can force this clustering to be applied in our integer linear program by adding the above equations as new constraints.
Solving the resulting program then yields:

\begin{figure}[h!]
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$\pi_{sum1}, \pi_{gts }, \pi_{sum2}$
    \> $=$ \> $0$ \\
$\pi_{ys1 }, \pi_{ys2 }$
    \> $=$ \> $1$ \\
$c_{gts}, c_{ys1}, c_{ys2}$           
    \> $=$ \> $0$
\end{tabbing}
\end{figure}
Note that although nodes $sum1$ and $sum2$ have equal $\pi$ values, they are not fused because their $x$ values are non-zero.
Conversely, if two nodes have different $\pi$ values, they are never fused. 

For the stream fusion clustering, the corresponding value of the objective function is: \\
$25 \cdot x_{sum1, gts} + 1 \cdot x_{sum1,sum2} + 25 \cdot x_{sum1, ys2} + 25 \cdot x_{gts, ys1} + 1 \cdot x_{sum2, ys1} + 25 \cdot x_{ys1, ys2} = 102$. 


% -----------------------------------------------------------------------------
\subsection{Closest points}
The closest points benchmark is a divide-and-conquer algorithm that finds the distance between the closest pair of two-dimensional points in an array.
We first find the midpoint along the Y-axis, and filter the remaining points to those above and below the midpoint.
We then recursively find the closest pair of points in the two halves, and merge the results.

\begin{haskell}[float,caption=Closest points benchmark,label=figs:clustering:bench:closest-points]
closestPoints :: Array Point -> Double
closestPoints pts
 | length pts < 100
 -- Naive $O(n^2)$ implementation for small arrays
 = closestPointsNaive pts
 | otherwise
 = let -- (external) Midpoint
       midy    = fold (\s (x,y) -> s + y) 0 pts / length pts
       -- (cluster 1) Filter above and below
       aboves  = filter (above midy) pts
       belows  = filter (below midy) pts
       -- (external) Recursive `divide' step
       above'  = closestPoints aboves
       below'  = closestPoints belows
       border  = min above' below'
       -- (cluster 2) Find points near the border to compare against each other
       aboveB  = filter (\p -> below (midy - border) p && above border p) pts
       belowB  = filter (\p -> above (midy + border) p && below border p) pts
       -- (cluster 3) Merge results for `conquer' step
       merged  = cross aboveB belowB
       dists   = map distance merged
       mins    = fold min border dists
   in  mins
\end{haskell}

The closest points implementation is shown in \cref{figs:clustering:bench:closest-points}, with our clustering described in the comments.
To compute the clustering of this program, we ignore the base case for small arrays and only look at the recursive case.
Our formulation does not directly support the \Hs/length/ operator; we encode the operation to compute the \Hs/midy/ midpoint as an \Hs/external/ combinator when performing clustering.
The two recursive calls are also encoded as \Hs/external/ combinators.
As the filtered points in \Hs/aboves/ and \Hs/belows/ are passed directly to the recursive call, there is no further opportunity to fuse them, and our clustering is the same as returned by Megiddo's algorithm.
However, unlike stream fusion, our clustering fuses the filter combinators for arrays \Hs/aboves/ and \Hs/belows/ into a single loop, as well as fusing the filter combinators for arrays \Hs/aboveB/ and \Hs/belowB/.


% -----------------------------------------------------------------------------
\subsection{Quadtree}
The Quadtree benchmark recursively builds a two-dimensional space partitioning tree from an array of points.
We first find the initial bounding-box that all the points fit into, by computing the minimum and maximum of both X and Y axes.
Then, at each recursive step, the bounding-box is split into four quadrants, and each point in the array is placed in the array for its corresponding quadrant.

\begin{haskell}[float,caption=Quadtree benchmark,label=figs:clustering:bench:quadtree]
quadtree :: Array Point -> Quadtree
quadtree pts = go pts initialBounds
 where
  initialBounds
   = let -- (cluster 1.1) Compute initial bounds
         xs = map fst pts
         ys = map snd pts
         x1 = minimum xs
         x2 = maximum xs
         y1 = minimum ys
         y2 = maximum ys
     in (x1, y1, x2, y2)

  go ins box
   -- Non-recursive base cases for empty and small input arrays
   | length ins == 0
   = Nil
   -- Check if bounding-box contains a single point
   | smallbox box
   = LeafArray ins
   | otherwise
   = let -- (external) Split input box into four quadrants
         (b1,b2,b3,b4)    = splitbox box
         -- (cluster 2.1) Partition input points into above quadrants
         p1               = filter (inbox b1) ins
         p2               = filter (inbox b2) ins
         p3               = filter (inbox b3) ins
         p4               = filter (inbox b4) ins
         -- Recurse into each partitioned quadrant separately
     in  Tree (go p1 b1) (go p2 b2) (go p3 b3) (go p4 b4)
\end{haskell}

The Quadtree implementation is shown in \cref{figs:clustering:bench:quadtree}.
The \Hs/initialBounds/ definition deconstructs the input points into two arrays containing the X and Y axes using two \Hs/map/ operations.
The \Hs/minimum/ and \Hs/maximum/ operations are implemented as folds.
Our clustering for \Hs/initialBounds/ fuses all six operations into a single loop, as does Megiddo's clustering.
Stream fusion requires a separate loop for each fold, and would require a separate loop for each \Hs/map/ operation, as each result is used twice.
The Vector library, which implements stream fusion, supports constant-time \Hs/unzip/ by using a struct-of-arrays representation for unboxed arrays of pairs.
To provide a fair comparison, we replace the \Hs/map/ operations with constant-time \Hs/unzip/ in the stream fusion clustering for \Hs/initialBounds/, leading to four loops in total.

The \Hs/go/ function performs the recursive loop over the points array.
First, it checks whether the input array is empty or contains a single unique point, and returns a leaf node if so.
Otherwise, the bounding-box is split into its four quadrants, and the points are partitioned into an array for each quadrant.
Our clustering for \Hs/go/ fuses all four filters into a single loop, as does Megiddo's clustering.
Stream fusion requires a separate loop for each filter, with four loops in total.


% -----------------------------------------------------------------------------
\subsection{Quickhull}
We previously benchmarked the Quickhull algorithm in the process fusion benchmarks, in \cref{s:Benchmarks}.
\Cref{figs:clustering:bench:filterMax} shows the implementation of \Hs/filterMax/, which forms the core of the Quickhull algorithm.
In our previous benchmark, we saw that stream fusion was unable to fuse \Hs/filterMax/ into a single loop because the \Hs/ptsAnn/ array is used twice.
Stream fusion only fuses the combinators for \Hs/aboveAnn/ and \Hs/above/ together, requiring three loops in total.
% We previously compared two Vector versions of \Hs/filterMax/: one that used a manifest array for \Hs/ptsAnn/ and shared the array, and another that recomputed the distances for both consumers.
% For this benchmark, we compare against the shared array clustering for stream fusion, as it is the clustering for stream fusion.

For \Hs/filterMax/, our clustering algorithm produces a single cluster with all combinators fused together.
% In the previous benchmarks, we saw that this clustering, when executed as a fused process network, was faster than both Vector implementations.
Megiddo's clustering for \Hs/filterMax/ requires a separate loop for the \Hs/map/ operation that produces the \Hs/above/ array, as it is looping over the result of a filter.
% We have not implemented Megiddo's clustering and so do not include the runtime for Quickhull in the results in \cref{clustering:f:BenchResults}; based on our cost model, we expect the runtime to be between the other two clusterings presented in the previous benchmark.



\begin{haskell}[float,caption=Quickhull core (\Hs/filterMax/) implementation,label=figs:clustering:bench:filterMax]
filterMax :: Line -> Array Point -> (Point, Array Point)
filterMax l pts
 = let -- (cluster 1) Compute filter and maximum 
       ptsAnn   = map (\p -> (p, distance p l)) pts
       maximAnn = maximumBy (compare `on` snd)  ptsAnn
       aboveAnn = filter ((>0) . snd)           ptsAnn
       above    = map fst                       aboveAnn
   in (fst maximAnn, above)
\end{haskell}
% hull :: (Point,Point) -> Array Point -> Array Point
% hull line@(l,r) pts
%  = let pts' = filter (above   line) pts
%        ma   = fold   (maxFrom line) pts'
%    in (hull (l, ma) pts') ++ (hull (ma, r) pts')
% \end{haskell}

% Stream fusion cannot fuse the \Hs@pts'@ and \Hs@ma@ bindings because \Hs@pts'@ is referred to multiple times and thus cannot be inlined.
% Megiddo's algorithm also cannot fuse the two bindings because their iteration sizes are different.
% If the \Hs@ma@ binding was rewritten to operate over the \Hs@pts@ array instead of \Hs@pts'@, Megiddo's formulation would be able to fuse the two, and the overall program would give the same result.
% However, this performance behavior is counter intuitive because \Hs@pts'@ is likely to be smaller than \Hs@pts@, so in an unfused program the original version would be faster.
% Our system fuses both versions.

