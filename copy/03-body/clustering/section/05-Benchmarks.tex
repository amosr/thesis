%!TEX root = ../Main.tex
\section{Benchmarks}
\label{clustering:s:Benchmarks}

\begin{table}
$$\begin{array}{c}

\begin{tabular}{lrrrrrrrr}
                & \multicolumn{2}{c}{Unfused}         & \multicolumn{2}{c}{Stream}
                & \multicolumn{2}{c}{Megiddo} &\multicolumn{2}{c}{\textbf{Ours}} \\
                & Time & Loops   & Time & Loops      & Time & Loops & Time & Loops   \\
\hline
Normalize2      & 1.88s & 5      & 1.64s & 4          & 1.82s & 3  & \textbf{1.59s} & \textbf{2}\\
Closest points  & 3.83s & 6      & 3.33s & 5          & 2.92s & 3  & \textbf{2.92s} & \textbf{3}\\
QuadTree        & 5.22s & 8      & 5.22s & 8          & 4.72s & 2  & \textbf{4.72s} & \textbf{2}\\
\end{tabular}

\end{array}$$
\caption{Benchmark results}
\label{clustering:f:BenchResults}
\end{table}

This section discusses three representative benchmarks, and gives the full ILP program of the first benchmark.
These benchmarks highlight the main differences between our fusion mechanism and related work.
The runtimes of each benchmark are summarized in \cref{clustering:f:BenchResults}.
We report times for: the unfused case, where each operator is assigned to its own cluster; the clustering implied by pull-based stream fusion~\cite{coutts2007stream}; the clustering chosen by Megiddo~\cite{megiddo1998optimal}; and the clustering chosen by our system. 

For each benchmark, we report the runtimes of hand-fused C code based on the clustering determined by each algorithm.
Although in a production implementation of clustering we would use the process fusion from \cref{chapter:process:processes} to fuse each cluster, we report on hand-fused C code to provide a fair comparison to related work.
As mentioned in~\citet{lippmeier2013data}, the current Haskell stream fusion mechanism introduces overhead in terms of a large number of duplicate loop counters, which increases register pressure unnecessarily.
Hand-fusing all code and compiling it with the same compiler (GCC) isolates the true cost of the various clusterings from low-level differences in code generation.

We have used both GLPK and CPLEX as external ILP solvers.
For small programs such as \Hs@normalizeInc@, both solvers produce solutions in under 100ms.
For a larger randomly generated example with twenty-five combinators, GLPK took over twenty minutes to produce a solution, while the commercial CPLEX solver was able to produce a solution in under one second --- which is still quite usable.
We will investigate the reason for this wide range in performance in future work.

% We use GLPK as an external solver, which is not particularly fast, but has readily-available Haskell bindings. For these small programs with few combinators, it produces results in under one hundred milliseconds.
% For larger programs with twenty or more combinators to be clustered, GLPK is inadequate.
% One randomly generated example with twenty-five combinators took over twenty minutes in GLPK, while the commercial solver CPLEX was able to solve the same example in a second, and another open source solver CBC took one hundred seconds.
% For the same example, the unoptimised program with all constraints took over twenty minutes in CBC, while CPLEX still only took one and a half seconds.

The hand-fused implementations of the benchmark programs are available at \url{https://github.com/amosr/papers/tree/master/2014betterfusionforfilters/benches}.

% The three benchmarks are the \Hs@normalize2@ example, finding the closest pair of points, and quadtree. The benchmark programs are all hand-written, hand-fused C code based on the clustering. Each program was run with the same input five times, and the minimum runtime was used. Runtimes and the number of loops for each clustering are shown in \cref{clustering:f:BenchResults}. In all cases, our clustering performs better than or as good as Megiddo's, and better than stream fusion and unfused. Interestingly, stream fusion's clustering for \Hs@normalize2@ performs better than Megiddo's, despite having more loops, as stream fusion is able to remove the intermediate array.


% -----------------------------------------------------------------------------
\pagebreak
\subsection{Normalize2}
To demonstrate the ILP formulation, we will use the \Hs@normalize2@ example from \cref{clustering:s:Introduction}, repeated here:
\begin{haskell}
normalize2 :: Array Int -> Array Int
normalize2 xs
 = let sum1 = fold   (+)  0   xs
       gts  = filter (>   0)  xs
       sum2 = fold   (+)  0   gts
       ys1  = map    (/ sum1) xs
       ys2  = map    (/ sum2) xs
   in (ys1, ys2)
\end{haskell}

We use the ILP formulation with fusion-preventing path optimisation from \cref{clustering:s:OptimisedConstraints}.
First, we calculate $\possible$ to find the nodes which have no fusion-preventing path between them.
The sets of nodes which can potentially be fused together are as follows:
\[ \{ \{sum1, gts, sum2\}
 , \{sum1, ys2\}
 , \{gts, sum2, ys1\}
 , \{ys1, ys2\} \} \]

The complete ILP program is shown in \cref{fig:clustering:normalize2-ilp}.
In the objective function the weights for $x_{sum1, sum2}$ and $x_{sum2, ys1}$ are both only 1, because they do not share any input arrays.

\begin{figure}
\begin{tabbing}
MMMMM   \= MMMMMMM \= M \= MMMMMMM \= M \= MMMMMMM \= \kill
Minimise   \> $25 \cdot x_{sum1, gts} + 1  \cdot x_{sum1,sum2} + 25 \cdot x_{sum1, ys2} +$ \\
           \> $25 \cdot x_{gts, sum2} + 25 \cdot x_{gts, ys1} + 1 \cdot x_{sum2, ys1} +$ \\
           \> $25 \cdot x_{ys1, ys2}  + 5  \cdot c_{gts} + 5 \cdot c_{ys1} + 5 \cdot c_{ys2} $
\\[0.5ex]
Subject to 
    \> $-5 \cdot x_{sum1, gts}$  \> $\le$ \> $\pi_{gts} - \pi_{sum1}$  \> $\le$ \> $5 \cdot x_{sum1, gts}$  \\
    \> $-5 \cdot x_{sum1, sum2}$ \> $\le$ \> $\pi_{sum2} - \pi_{sum1}$ \> $\le$ \> $5 \cdot x_{sum1, sum2}$ \\
    \> $-5 \cdot x_{sum1, ys2 }$ \> $\le$ \> $\pi_{ys2 } - \pi_{sum1}$ \> $\le$ \> $5 \cdot x_{sum1, ys2 }$ \\
    \> $-5 \cdot x_{gts,  ys1 }$ \> $\le$ \> $\pi_{ys1 } - \pi_{gts }$ \> $\le$ \> $5 \cdot x_{gts, ys1  }$ \\
    \> $-5 \cdot x_{sum2, ys1 }$ \> $\le$ \> $\pi_{ys1 } - \pi_{sum2}$ \> $\le$ \> $5 \cdot x_{sum2, ys1 }$ \\
    \> $-5 \cdot x_{ys1, ys2  }$ \> $\le$ \> $\pi_{ys2 } - \pi_{ys1 }$ \> $\le$ \> $5 \cdot x_{ys1, ys2  }$ 
\\[0.5ex]
    \> $   x_{gts, sum2 }$ \> $\le$ \> $\pi_{sum2} - \pi_{gts }$ \> $\le$ \> $5 \cdot x_{gts, sum2 }$ 
\\[0.5ex]
    \>                     \>       \> $\pi_{sum1} < \pi_{ys1}$ \\
    \>                     \>       \> $\pi_{sum2} < \pi_{ys2}$
\\[0.5ex]
    \> $ x_{gts,sum2} $    \> $\le$ \> $c_{gts}$
\\[0.5ex]
    \> $x_{gts, sum2}$     \> $\le$ \> $x_{sum1, sum2}$ \\
    \> $x_{sum1,sum1}$     \> $\le$ \> $x_{sum1, sum2}$ \\
    \> $x_{sum1, gts}$     \> $\le$ \> $x_{sum1, sum2}$
\end{tabbing}
\caption{Complete integer linear program for \Hs/normalize2/}
\label{fig:clustering:normalize2-ilp}
\end{figure}

\begin{figure}
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$x_{sum1, gts},~ x_{sum1, sum1},~ x_{sum1, sum2},~ x_{gts, sum2},~ x_{ys1,  ys2}$
    \> $=$ \> $0$ \\
$x_{sum1, ys2},~ x_{gts, ys1},~   x_{sum2, ys1}$
    \> $=$ \> $1$ 
\\[1ex]
$\pi_{sum1},~ \pi_{gts },~ \pi_{sum2}$
    \> $=$ \> $0$ \\
$\pi_{ys1 },~ \pi_{ys2 }$
    \> $=$ \> $1$ 
\\[1ex]
$c_{gts},~ c_{ys1},~ c_{ys2}$           
    \> $=$ \> $0$
\end{tabbing}
\caption{A solution to the integer linear program for \Hs/normalize2/}
\label{fig:clustering:normalize2-ilp-sol}
\end{figure}

One minimal solution to the integer linear program for \Hs/normalize2/ is given in \cref{fig:clustering:normalize2-ilp-sol}.
This minimal solution is not unique, though in this case the only other minimal solutions use different $\pi$ values, and denote the same clustering.
Looking at just the non-zero variables in the objective function, the value is $25 \cdot x_{sum1,ys2} + 25 \cdot x_{gts,ys1} + 1 \cdot x_{sum2, ys1} = 51$.
For illustrative purposes, note that objective function could be reduced by setting $x_{sum1,ys2} = 0$ (fusing $sum1$ and $ys1$), but this conflicts with the other constraints.
Since $x_{sum1, sum2} = 0$, we require that $\pi_{sum1} = \pi_{sum2}$, as well as \mbox{$\pi_{sum2} < \pi_{ys2}$}.
These constraints cannot be satisfied, so a clustering that fused $sum1$ and $ys2$ would not also permit $sum1$ and $sum2$ to be fused.

We will now compare the clustering produced by our system, with the one implied by pull-based stream fusion.
As we saw in \cref{taxonomy/pull}, pull streams do not support distributing an input stream among multiple consumers; likewise, stream fusion does not support fusing an input with multiple consumers into a single loop.
% , or fuse operators that are not in a producer-consumer relationship.
The corresponding values of the $x_{ij}$ variables are:

\begin{figure}[h!]
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$x_{gts, sum2}$
    \> $=$ \> $0$ \\
$x_{sum1, gts}, x_{sum1, sum2}, x_{ys1,  ys2}, x_{sum1, ys2}, x_{gts, ys1 }, x_{sum2, ys1}$
    \> $=$ \> $1$
\end{tabbing}
\end{figure}

We can force this clustering to be applied in our integer linear program by adding the above equations as new constraints.
Solving the resulting program then yields:

\begin{figure}[h!]
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$\pi_{sum1}, \pi_{gts }, \pi_{sum2}$
    \> $=$ \> $0$ \\
$\pi_{ys1 }, \pi_{ys2 }$
    \> $=$ \> $1$ \\
$c_{gts}, c_{ys1}, c_{ys2}$           
    \> $=$ \> $0$
\end{tabbing}
\end{figure}
Note that although nodes $sum1$ and $sum2$ have equal $\pi$ values, they are not fused because their $x$ values are non-zero.
Conversely, if two nodes have different $\pi$ values, they are never fused. 

For the stream fusion clustering, the corresponding value of the objective function is: \\
$25 \cdot x_{sum1, gts} + 1 \cdot x_{sum1,sum2} + 25 \cdot x_{sum1, ys2} + 25 \cdot x_{gts, ys1} + 1 \cdot x_{sum2, ys1} + 25 \cdot x_{ys1, ys2} = 102$. 


% -----------------------------------------------------------------------------
\subsection{Closest Points}
The closest points benchmark is a divide-and-conquer algorithm that finds the closest pair of 2-dimensional points in an array. We first find the midpoint along the Y-axis, and filter the remaining points to those above and below the midpoint. We then recursively find the closest pair of points in the two halves, and merge the results. As the filtered points are passed directly to the recursive call, there is no further opportunity to fuse them, and our clustering is the same as returned by Megiddo's algorithm. However, our clustering generates both filtered arrays in a single loop, unlike stream fusion that requires a separate loop for each.


% -----------------------------------------------------------------------------
\subsection{QuadTree}
The QuadTree benchmark recursively builds a 2-dimensional space partitioning tree from an array of points. At each step the array of points is filtered into four 2-dimensional boxes. As with the closest points algorithm, there are no further opportunities for fusing the filtered results, and our clustering is the same as Megiddo's. However, our clustering produces all four filtered results in a single loop, whereas stream fusion requires four loops.


% -----------------------------------------------------------------------------
\subsection{QuickHull}
The core of the QuickHull algorithm is shown below: given a line and an array of points, we filter the points to those above the line, and also find the point farthest from that line.

\begin{haskell}
hull :: (Point,Point) -> Array Point -> Array Point
hull line@(l,r) pts
 = let pts' = filter (above   line) pts
       ma   = fold   (maxFrom line) pts'
   in (hull (l, ma) pts') ++ (hull (ma, r) pts')
\end{haskell}

Stream fusion cannot fuse the \Hs@pts'@ and \Hs@ma@ bindings because \Hs@pts'@ is referred to multiple times and thus cannot be inlined. Megiddo's algorithm also cannot fuse the two bindings because their iteration sizes are different. If the \Hs@ma@ binding was rewritten to operate over the \Hs@pts@ array instead of \Hs@pts'@, Megiddo's formulation would be able to fuse the two, and the overall program would give the same result. However, this performance behavior is counter intuitive because \Hs@pts'@ is likely to be smaller than \Hs@pts@, so in an unfused program the original version would be faster. Our system fuses both versions.

